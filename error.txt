PS C:\Users\TU\Desktop\new project\N1V1> pytest -q
......................................................................................................... [ 16%]
......................................................................................................... [ 33%]
......................................................................................................... [ 50%]
......................................................................................................... [ 67%]
......................................................................................................... [ 83%]
...............................................F.....................................................     [100%]
=================================================== FAILURES =================================================== 
________________________________ TestModelTraining.test_train_model_eval_profit ________________________________ 

self = <test_trainer.TestModelTraining object at 0x0000020D4276E8F0>
mock_plot_importance = <MagicMock name='plot_importance' id='2255804174816'>
mock_plt_close = <MagicMock name='close' id='2255975806960'>
mock_plt_savefig = <MagicMock name='savefig' id='2255976268656'>
mock_lgb_classifier = <MagicMock name='LGBMClassifier' id='2255976685552'>
mock_json_dump = <MagicMock name='dump' id='2255976683824'>
mock_joblib_dump = <MagicMock name='dump' id='2255976718896'>
sample_training_data =           Open        High        Low       Close  ...  StochRSI  TrendStrength  Volatility  Label
0   103.745401  110...101.078914  117.798755  90.853475  107.545429  ...  0.417898       0.085384    2.617692    
  0

[100 rows x 13 columns]

    @patch('ml.trainer.joblib.dump')
    @patch('ml.trainer.json.dump')
    @patch('ml.trainer.lgb.LGBMClassifier')
    @patch('ml.trainer.plt.savefig')
    @patch('ml.trainer.plt.close')
    @patch('ml.trainer.lgb.plot_importance')
    def test_train_model_eval_profit(self, mock_plot_importance, mock_plt_close,
                                   mock_plt_savefig, mock_lgb_classifier,
                                   mock_json_dump, mock_joblib_dump, sample_training_data):
        """Test model training with profit evaluation."""
        with tempfile.TemporaryDirectory() as temp_dir:
            model_path = os.path.join(temp_dir, 'model.pkl')

            # Mock LightGBM classifier
            mock_model = MagicMock()
            mock_lgb_classifier.return_value = mock_model

            mock_json_dump.return_value = None
            mock_joblib_dump.return_value = None
            mock_plt_savefig.return_value = None
            mock_plt_close.return_value = None
            mock_plot_importance.return_value = None

            # Train with profit evaluation
>           train_model(
                sample_training_data,
                model_path,
                eval_profit=True,
                n_splits=2
            )

tests\test_trainer.py:545:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
ml\trainer.py:563: in train_model
    plt.figure(figsize=(10, 8))
..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\matplotlib\pyplot.py:934: in figure
    manager = new_figure_manager(
..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\matplotlib\pyplot.py:465: in new_figure_manager
    return _get_backend_mod().new_figure_manager(*args, **kwargs)
..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\matplotlib\backend_bases.py:3402: in new_figure_manager
    return cls.new_figure_manager_given_figure(num, fig)
..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\matplotlib\backend_bases.py:3407: in new_figure_manager_given_figure
    return cls.FigureCanvas.new_manager(figure, num)
..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\matplotlib\backend_bases.py:1785: in new_manager
    return cls.manager_class.create_with_canvas(cls, figure, num)
..\..\..\AppData\Local\Programs\Python\Python310\lib\site-packages\matplotlib\backends\_backend_tk.py:486: in create_with_canvas
    window = tk.Tk(className="matplotlib")
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <tkinter.Tk object .>, screenName = None, baseName = 'pytest', className = 'matplotlib', useTk = True     
sync = False, use = None

    def __init__(self, screenName=None, baseName=None, className='Tk',
                 useTk=True, sync=False, use=None):
        """Return a new top level widget on screen SCREENNAME. A new Tcl interpreter will
        be created. BASENAME will be used for the identification of the profile file (see
        readprofile).
        It is constructed from sys.argv[0] without extensions if None is given. CLASSNAME
        is the name of the widget class."""
        self.master = None
        self.children = {}
        self._tkloaded = False
        # to avoid recursions in the getattr code in case of failure, we
        # ensure that self.tk is always _something_.
        self.tk = None
        if baseName is None:
            import os
            baseName = os.path.basename(sys.argv[0])
            baseName, ext = os.path.splitext(baseName)
            if ext not in ('.py', '.pyc'):
                baseName = baseName + ext
        interactive = False
>       self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)   
E       _tkinter.TclError: invalid command name "tcl_findLibrary"

..\..\..\AppData\Local\Programs\Python\Python310\lib\tkinter\__init__.py:2299: TclError
---------------------------------------------- Captured log call ----------------------------------------------- 
WARNING  root:trainer.py:416 Fold 1 - Empty predictions or test set, skipping fold
WARNING  root:trainer.py:416 Fold 2 - Empty predictions or test set, skipping fold
=============================================== warnings summary =============================================== 
tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_rate_limit_handling
tests/test_regression.py::TestRegression::test_portfolio_initialization_with_invalid_data
  C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\unraisableexception.py:80: PytestUnraisableExceptionWarning: Exception ignored in: <function _SSLProtocolTransport.__del__ at 0x0000020D29F66DD0> 

  Traceback (most recent call last):
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
      self._transport.write(chunk)
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 365, in write      self._loop_writing(data=bytes(data))
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 401, in _loop_writing
      self._write_fut = self._loop._proactor.send(self._sock, data)
  AttributeError: 'NoneType' object has no attribute 'send'

  During handling of the above exception, another exception occurred:

  Traceback (most recent call last):
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 321, in __del__     
      self.close()
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 316, in close       
      self._ssl_protocol._start_shutdown()
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 599, in _start_shutdown
      self._write_appdata(b'')
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 604, in _write_appdata
      self._process_write_backlog()
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 712, in _process_write_backlog
      self._fatal_error(exc, 'Fatal error on SSL transport')
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 726, in _fatal_error      self._transport._force_close(exc)
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 152, in _force_close
      self._loop.call_soon(self._call_connection_lost, exc)
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 753, in call_soon      self._check_closed()
    File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py", line 515, in _check_closed
      raise RuntimeError('Event loop is closed')
  RuntimeError: Event loop is closed

    warnings.warn(pytest.PytestUnraisableExceptionWarning(msg))

tests/test_discord_integration.py::TestDiscordIntegration::test_bot_integration_shutdown
  C:\Users\TU\Desktop\new project\N1V1\tests\test_discord_integration.py:325: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    await discord_bot_integration_notifier.shutdown()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_main.py::TestMainFunction::test_main_status_mode
  C:\Users\TU\Desktop\new project\N1V1\main.py:222: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    bot_engine.print_status_table()
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_signal_router.py::test_retry_async_call_with_custom_backoff
  C:\Users\TU\Desktop\new project\N1V1\core\signal_router.py:788: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    await asyncio.sleep(max(0.0, sleep_for))
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.
-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================================== short test summary info ============================================ 
FAILED tests/test_trainer.py::TestModelTraining::test_train_model_eval_profit - _tkinter.TclError: invalid command name "tcl_findLibrary"
        Fail fast if running in live mode and required secrets are missing or partial.

        Rules:
          - When environment.mode == 'live' (case-insensitive):
            * If exchange.sandbox is False (or not present), require exchange.api_key and exchange.api_secret.
            * If Discord notifications are enabled, require either webhook_url OR both bot_token and channel_id.
        This method raises SecurityException on missing/partial secrets so callers can fail fast.
        """
        mode = self._config.get("environment", {}).get("mode", "paper")
        if isinstance(mode, str) and mode.lower() == "live":
            # Get credential manager for secure credential access
            cred_manager = get_credential_manager()

            # Exchange credentials check (skip enforcement when sandbox == True)
            exch = self._config.get("exchange", {}) or {}
            sandbox = bool(exch.get("sandbox", False))

            if not sandbox:
                api_key = cred_manager.get_credential('exchange_api_key')
                api_secret = cred_manager.get_credential('exchange_api_secret')

                if not api_key or not api_secret:
                    error_msg = sanitize_error_message(
                        "Live mode requires exchange credentials: provide exchange.api_key and exchange.api_secret "
                        "(via config file or environment variables CRYPTOBOT_EXCHANGE_API_KEY / CRYPTOBOT_EXCHANGE_API_SECRET)."
                    )
>                   raise SecurityException(error_msg)
E                   utils.security.SecurityException: Live mode requires exchange credentials: provide exchange.api_key and exchange.api_secret (via config file or environment variables CRYPTOBOT_EXCHANGE_API_KEY / CRYPTOBOT_EXCHANGE_API_SECRET).

utils\config_loader.py:592: SecurityException
_________________________________ TestDataPersistence.test_save_results_creates_files _________________________________

self = <test_cross_asset_validation.TestDataPersistence object at 0x00000266202327A0>
validation_config = {'asset_selector': {'asset_weights': 'equal', 'correlation_filter': False, 'max_assets': 3, 'validation_assets': [{'na...name': 'binance'}, 'output_dir': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmplcykzc8j', 'parallel_validation': False, ...}

    def test_save_results_creates_files(self, validation_config):
        """Test that save_results creates the expected files."""
        validator = CrossAssetValidator(validation_config)

        # Create a mock result
        result = CrossAssetValidationResult(
            strategy_name='TestStrategy',
            primary_asset='BTC/USDT',
            validation_assets=[],
            asset_results=[],
            aggregate_metrics={},
            pass_rate=0.0,
            overall_pass=False,
            robustness_score=0.0,
            timestamp=datetime.now(),
            total_time=10.0
        )

        validator._save_results(result)

        # Check that files were created
        output_dir = validation_config['output_dir']
        assert os.path.exists(os.path.join(output_dir, 'cross_asset_validation_results.json'))
        assert os.path.exists(os.path.join(output_dir, 'cross_asset_validation_summary.json'))
>       assert os.path.exists(os.path.join(output_dir, 'cross_asset_validation_summary.csv'))
E       AssertionError: assert False
E        +  where False = <function exists at 0x000002667BCB2B00>('C:\\Users\\TU\\AppData\\Local\\Temp\\tmplcykzc8j\\cross_asset_validation_summary.csv')
E        +    where <function exists at 0x000002667BCB2B00> = <module 'ntpath' from 'C:\\Users\\TU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ntpath.py'>.exists
E        +      where <module 'ntpath' from 'C:\\Users\\TU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ntpath.py'> = os.path
E        +    and   'C:\\Users\\TU\\AppData\\Local\\Temp\\tmplcykzc8j\\cross_asset_validation_summary.csv' = <function join at 0x000002667BCB3400>('C:\\Users\\TU\\AppData\\Local\\Temp\\tmplcykzc8j', 'cross_asset_validation_summary.csv')
E        +      where <function join at 0x000002667BCB3400> = <module 'ntpath' from 'C:\\Users\\TU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ntpath.py'>.join
E        +        where <module 'ntpath' from 'C:\\Users\\TU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ntpath.py'> = os.path

tests\test_cross_asset_validation.py:730: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,757 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
INFO:CrossAssetValidator:Cross-Asset Validator initialized
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
2025-09-08 16:14:17,761 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
INFO:CrossAssetValidator:Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     CrossAssetValidator:cross_asset_validation.py:501 Cross-Asset Validator initialized
INFO     CrossAssetValidator:cross_asset_validation.py:869 Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmplcykzc8j
_____________________________________ TestDataPersistence.test_csv_summary_format _____________________________________

self = <test_cross_asset_validation.TestDataPersistence object at 0x00000266202316F0>
validation_config = {'asset_selector': {'asset_weights': 'equal', 'correlation_filter': False, 'max_assets': 3, 'validation_assets': [{'na...name': 'binance'}, 'output_dir': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp_tpaojq7', 'parallel_validation': False, ...}

    def test_csv_summary_format(self, validation_config):
        """Test CSV summary file format."""
        validator = CrossAssetValidator(validation_config)

        # Create mock asset results
        asset_result = AssetValidationResult(
            asset=ValidationAsset('ETH/USDT', 'Ethereum'),
            optimized_params={'param': 'value'},
            primary_metrics={'sharpe_ratio': 1.0},
            validation_metrics={'sharpe_ratio': 0.8, 'win_rate': 0.5},
            pass_criteria={'sharpe_ratio': True, 'win_rate': True},
            overall_pass=True,
            validation_time=5.0
        )

        result = CrossAssetValidationResult(
            strategy_name='TestStrategy',
            primary_asset='BTC/USDT',
            validation_assets=[asset_result.asset],
            asset_results=[asset_result],
            aggregate_metrics={},
            pass_rate=1.0,
            overall_pass=True,
            robustness_score=0.8,
            timestamp=datetime.now(),
            total_time=5.0
        )

        validator._save_results(result)

        # Read CSV and verify format
        csv_path = os.path.join(validation_config['output_dir'], 'cross_asset_validation_summary.csv')
        df = pd.read_csv(csv_path)

        assert len(df) == 1
        assert df.iloc[0]['asset_symbol'] == 'ETH/USDT'
>       assert df.iloc[0]['overall_pass'] is True
E       assert True is True

tests\test_cross_asset_validation.py:768: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
2025-09-08 16:14:17,790 - CrossAssetValidator - INFO - Cross-Asset Validator initialized
INFO:CrossAssetValidator:Cross-Asset Validator initialized
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
2025-09-08 16:14:17,801 - CrossAssetValidator - INFO - Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
INFO:CrossAssetValidator:Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     CrossAssetValidator:cross_asset_validation.py:501 Cross-Asset Validator initialized
INFO     CrossAssetValidator:cross_asset_validation.py:869 Cross-asset validation results saved to C:\Users\TU\AppData\Local\Temp\tmp_tpaojq7
___________________ TestMonitoringPerformanceIntegration.test_performance_metrics_monitoring_system ___________________

self = <test_cross_feature_integration.TestMonitoringPerformanceIntegration object at 0x000002662059C2E0>

    @pytest.mark.asyncio
    async def test_performance_metrics_monitoring_system(self):
        """Test that performance metrics are accurately captured in monitoring system."""
        await self.monitor.start_monitoring()

        # Perform profiled operations
        operations = ["fast_operation", "medium_operation", "slow_operation"]
        expected_times = [0.001, 0.01, 0.1]

        for op, expected_time in zip(operations, expected_times):
            with self.profiler.profile_function(op):
                time.sleep(expected_time)

        # Check that monitoring captured performance data
        status = await self.monitor.get_performance_status()

        # Should have performance baselines
>       assert status["total_baselines"] > 0
E       assert 0 > 0

tests\test_cross_feature_integration.py:301: AssertionError
________________ TestMonitoringPerformanceIntegration.test_monitoring_performance_profiling_operations ________________

self = <test_cross_feature_integration.TestMonitoringPerformanceIntegration object at 0x000002662059C5B0>

    @pytest.mark.asyncio
    async def test_monitoring_performance_profiling_operations(self):
        """Test monitoring system performance during profiling operations."""
        await self.monitor.start_monitoring()

        # Measure monitoring overhead during profiling
        start_time = time.time()

        # Perform intensive profiling operations
        for i in range(100):
            with self.profiler.profile_function(f"intensive_op_{i}"):
                # Simulate some work
                data = np.random.random(1000)
                result = np.sum(data ** 2)

        profiling_time = time.time() - start_time

        # Get monitoring status
        status = await self.monitor.get_performance_status()

        # Monitoring should not significantly impact performance
        assert profiling_time < 5.0, f"Profiling took {profiling_time:.2f}s (too slow)"
>       assert status["system_health"] > 50, f"System health too low: {status['system_health']}"
E       AssertionError: System health too low: 50.0
E       assert 50.0 > 50

tests\test_cross_feature_integration.py:336: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:core.metrics_collector:Metric performance_monitoring_duration_seconds already registered, returning existing
WARNING:core.metrics_collector:Metric performance_anomalies_total already registered, returning existing
WARNING:core.metrics_collector:Metric performance_alerts_total already registered, returning existing
WARNING:core.metrics_collector:Metric performance_system_health_score already registered, returning existing
WARNING:core.metrics_collector:Metric performance_baseline_count already registered, returning existing
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  core.metrics_collector:metrics_collector.py:186 Metric performance_monitoring_duration_seconds already registered, returning existing
WARNING  core.metrics_collector:metrics_collector.py:186 Metric performance_anomalies_total already registered, returning existing
WARNING  core.metrics_collector:metrics_collector.py:186 Metric performance_alerts_total already registered, returning existing
WARNING  core.metrics_collector:metrics_collector.py:186 Metric performance_system_health_score already registered, returning existing
WARNING  core.metrics_collector:metrics_collector.py:186 Metric performance_baseline_count already registered, returning existing
_________________________ TestDiscordIntegration.test_webhook_integration_basic_notification __________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066CB20>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x0000026623280280>

    @pytest.mark.asyncio
    async def test_webhook_integration_basic_notification(self, discord_webhook_notifier):
        """Test basic notification sending via webhook integration."""
        result = await discord_webhook_notifier.send_notification("Integration Test: Basic notification")

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:147: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
_____________________________ TestDiscordIntegration.test_webhook_integration_with_embed ______________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066CE50>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x00000266231BC700>

    @pytest.mark.asyncio
    async def test_webhook_integration_with_embed(self, discord_webhook_notifier):
        """Test notification with embed via webhook integration."""
        embed_data = {
            "title": "Integration Test Embed",
            "description": "Testing embed functionality",
            "color": 0x00FF00,
            "fields": [
                {"name": "Test Field", "value": "Test Value", "inline": True}
            ]
        }

        result = await discord_webhook_notifier.send_notification("Integration Test: With embed", embed_data)

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:163: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
_____________________________ TestDiscordIntegration.test_webhook_integration_trade_alert _____________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066D1B0>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x0000026622FB7550>
sample_trade_data = {'amount': 1.0, 'mode': 'live', 'pnl': 100.0, 'price': 50000.0, ...}

    @pytest.mark.asyncio
    async def test_webhook_integration_trade_alert(self, discord_webhook_notifier, sample_trade_data):
        """Test trade alert via webhook integration."""
        result = await discord_webhook_notifier.send_trade_alert(sample_trade_data)

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:170: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
____________________________ TestDiscordIntegration.test_webhook_integration_signal_alert _____________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066D4E0>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x0000026622FE7F70>
sample_signal = TradingSignal(strategy_id='integration_test_strategy', symbol='BTC/USDT', signal_type=<SignalType.ENTRY_LONG: 1>, sign...00'), current_price=None, timestamp=1757297672202, stop_loss=None, take_profit=None, trailing_stop=None, metadata=None)

    @pytest.mark.asyncio
    async def test_webhook_integration_signal_alert(self, discord_webhook_notifier, sample_signal):
        """Test signal alert via webhook integration."""
        result = await discord_webhook_notifier.send_signal_alert(sample_signal)

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:177: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Unexpected error sending Discord notification
Traceback (most recent call last):
  File "C:\Users\TU\Desktop\new project\N1V1\notifier\discord_bot.py", line 346, in send_notification
    resp = await self.session.post(url, json=payload, headers=headers)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\aiohttp\client.py", line 430, in _request
    data = payload.JsonPayload(json, dumps=self._json_serialize)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\aiohttp\payload.py", line 396, in __init__
    dumps(value).encode(encoding),
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type SignalType is not JSON serializable
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:394 Unexpected error sending Discord notification
Traceback (most recent call last):
  File "C:\Users\TU\Desktop\new project\N1V1\notifier\discord_bot.py", line 346, in send_notification
    resp = await self.session.post(url, json=payload, headers=headers)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\aiohttp\client.py", line 430, in _request
    data = payload.JsonPayload(json, dumps=self._json_serialize)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\aiohttp\payload.py", line 396, in __init__
    dumps(value).encode(encoding),
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type SignalType is not JSON serializable
_____________________________ TestDiscordIntegration.test_webhook_integration_error_alert _____________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066D810>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x000002662321FCD0>
sample_error_data = {'component': 'DiscordIntegrationTest', 'error': 'Integration test error', 'timestamp': '2023-01-01T00:00:00Z'}

    @pytest.mark.asyncio
    async def test_webhook_integration_error_alert(self, discord_webhook_notifier, sample_error_data):
        """Test error alert via webhook integration."""
        result = await discord_webhook_notifier.send_error_alert(sample_error_data)

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:184: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
_________________________ TestDiscordIntegration.test_webhook_integration_performance_report __________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066D570>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x00000266231DE680>
sample_performance_data = {'max_loss': -500.0, 'max_win': 1000.0, 'sharpe_ratio': 1.2, 'total_pnl': 5000.0, ...}

    @pytest.mark.asyncio
    async def test_webhook_integration_performance_report(self, discord_webhook_notifier, sample_performance_data):
        """Test performance report via webhook integration."""
        result = await discord_webhook_notifier.send_performance_report(sample_performance_data)

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:191: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
___________________________ TestDiscordIntegration.test_bot_integration_basic_notification ____________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066CD60>
discord_bot_integration_notifier = <notifier.discord_bot.DiscordNotifier object at 0x0000026622FE7880>

    @pytest.mark.asyncio
    async def test_bot_integration_basic_notification(self, discord_bot_integration_notifier):
        """Test basic notification sending via bot integration."""
        result = await discord_bot_integration_notifier.send_notification("Integration Test: Bot notification")

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:198: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
ERROR:asyncio:Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002662321D450>
ERROR:asyncio:Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x0000026623155540>, 33216.75)]']
connector: <aiohttp.connector.TCPConnector object at 0x000002662321D330>
ERROR:asyncio:Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x000002662321DA20>
transport: <_ProactorSocketTransport fd=-1 read=<_OverlappedFuture cancelled>>
Traceback (most recent call last):
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 365, in write
    self._loop_writing(data=bytes(data))
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 401, in _loop_writing
    self._write_fut = self._loop._proactor.send(self._sock, data)
AttributeError: 'NoneType' object has no attribute 'send'
ERROR:asyncio:Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x00000266231BCAF0>
ERROR:asyncio:Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x00000266231574C0>, 33217.281)]']
connector: <aiohttp.connector.TCPConnector object at 0x00000266231BC9A0>
ERROR:asyncio:Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x00000266231BC3D0>
transport: <_ProactorSocketTransport fd=5524 read=<_OverlappedFuture cancelled>>
Traceback (most recent call last):
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 365, in write
    self._loop_writing(data=bytes(data))
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 401, in _loop_writing
    self._write_fut = self._loop._proactor.send(self._sock, data)
AttributeError: 'NoneType' object has no attribute 'send'
ERROR:asyncio:Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000026622FB4130>
ERROR:asyncio:Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x0000026623156B00>, 33217.734)]']
connector: <aiohttp.connector.TCPConnector object at 0x0000026622FB48E0>
ERROR:asyncio:Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x0000026623084040>
transport: <_ProactorSocketTransport fd=5816 read=<_OverlappedFuture cancelled>>
Traceback (most recent call last):
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 365, in write
    self._loop_writing(data=bytes(data))
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 401, in _loop_writing
    self._write_fut = self._loop._proactor.send(self._sock, data)
AttributeError: 'NoneType' object has no attribute 'send'
ERROR:asyncio:Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x0000026622FE4190>
ERROR:asyncio:Unclosed client session
client_session: <aiohttp.client.ClientSession object at 0x000002662321CE80>
ERROR:asyncio:Unclosed connector
connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x000002662540FE80>, 33218.312)]']
connector: <aiohttp.connector.TCPConnector object at 0x000002662321EB00>
ERROR:asyncio:Fatal error on SSL transport
protocol: <asyncio.sslproto.SSLProtocol object at 0x000002662321EF80>
transport: <_ProactorSocketTransport fd=-1 read=<_OverlappedFuture cancelled>>
Traceback (most recent call last):
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\sslproto.py", line 690, in _process_write_backlog
    self._transport.write(chunk)
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 365, in write
    self._loop_writing(data=bytes(data))
  File "C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\asyncio\proactor_events.py", line 401, in _loop_writing
    self._write_fut = self._loop._proactor.send(self._sock, data)
AttributeError: 'NoneType' object has no attribute 'send'
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
_______________________________ TestDiscordIntegration.test_bot_integration_trade_alert _______________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066DBD0>
discord_bot_integration_notifier = <notifier.discord_bot.DiscordNotifier object at 0x000002662545A020>
sample_trade_data = {'amount': 1.0, 'mode': 'live', 'pnl': 100.0, 'price': 50000.0, ...}

    @pytest.mark.asyncio
    async def test_bot_integration_trade_alert(self, discord_bot_integration_notifier, sample_trade_data):
        """Test trade alert via bot integration."""
        result = await discord_bot_integration_notifier.send_trade_alert(sample_trade_data)

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:205: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
_________________________ TestDiscordIntegration.test_webhook_integration_rate_limit_handling _________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066DEA0>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x000002662543E8C0>

    @pytest.mark.asyncio
    async def test_webhook_integration_rate_limit_handling(self, discord_webhook_notifier):
        """Test rate limit handling in webhook integration."""
        # Send multiple notifications quickly to potentially trigger rate limits
        tasks = []
        for i in range(10):
            task = discord_webhook_notifier.send_notification(f"Rate limit test {i}")
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # At least some should succeed
        success_count = sum(1 for result in results if result is True)
>       assert success_count > 0
E       assert 0 > 0

tests\test_discord_integration.py:220: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
____________________________ TestDiscordIntegration.test_webhook_integration_large_payload ____________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066E470>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x0000026622DCAE00>

    @pytest.mark.asyncio
    async def test_webhook_integration_large_payload(self, discord_webhook_notifier):
        """Test handling of large payloads in webhook integration."""
        # Create a large embed with many fields
        large_embed = {
            "title": "Large Payload Test",
            "description": "Testing with a large number of fields",
            "color": 0xFF0000,
            "fields": []
        }

        # Add many fields to create a large payload
        for i in range(25):  # Discord allows up to 25 fields
            large_embed["fields"].append({
                "name": f"Field {i}",
                "value": f"Value {i} - This is a test value for field {i}",
                "inline": True
            })

        result = await discord_webhook_notifier.send_notification("Large payload test", large_embed)

>       assert result is True
E       assert False is True

tests\test_discord_integration.py:272: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
_________________________ TestDiscordIntegration.test_webhook_integration_special_characters __________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066E770>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x00000266253CDF00>

    @pytest.mark.asyncio
    async def test_webhook_integration_special_characters(self, discord_webhook_notifier):
        """Test handling of special characters in webhook integration."""
        special_message = "Special chars: éñüñ 中文 🚀 💯 🔥 🔥 🔥"
        result = await discord_webhook_notifier.send_notification(special_message)
>       assert result is True
E       assert False is True

tests\test_discord_integration.py:279: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
____________________________ TestDiscordIntegration.test_webhook_integration_empty_content ____________________________

self = <test_discord_integration.TestDiscordIntegration object at 0x000002662066EA70>
discord_webhook_notifier = <notifier.discord_bot.DiscordNotifier object at 0x0000026622E13FD0>

    @pytest.mark.asyncio
    async def test_webhook_integration_empty_content(self, discord_webhook_notifier):
        """Test handling of empty content in webhook integration."""
        result = await discord_webhook_notifier.send_notification("")
>       assert result is True
E       assert False is True

tests\test_discord_integration.py:285: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
INFO:notifier.discord_bot:Discord webhook notifications enabled
------------------------------------------------- Captured log setup --------------------------------------------------
INFO     notifier.discord_bot:discord_bot.py:109 Discord webhook notifications enabled
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:notifier.discord_bot:Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    notifier.discord_bot:discord_bot.py:383 Discord notification failed: status=401 body={"message": "401: Unauthorized", "code": 0}
______________________________ TestDocstringStandardizer.test_docstring_standardization _______________________________

self = <test_docstring_standardization.TestDocstringStandardizer object at 0x00000266206E0E20>

    def test_docstring_standardization(self):
        """Test docstring standardization."""
        # Original docstring
        original = '''"""Simple function."""'''

        # Standardized version
        standardized = self.standardizer.standardize_docstring(
            original,
            function_name="process_data",
            args=["data", "config"],
            returns="Processed result",
            raises=["ValueError"]
        )

        assert "Args:" in standardized
        assert "Returns:" in standardized
        assert "Raises:" in standardized
>       assert "process_data" in standardized
E       assert 'process_data' in '"""Simple function."""\n\nArgs:\n    data: Description of data\n    config: Description of config\n\n\nReturns:\n    Processed result\n\n\nRaises:\n    ValueError: Description of when ValueError is raised\n'

tests\test_docstring_standardization.py:142: AssertionError
______________________ TestDocstringStandardizerIntegration.test_analyze_codebase_documentation _______________________

self = <test_docstring_standardization.TestDocstringStandardizerIntegration object at 0x00000266206E1690>

    def test_analyze_codebase_documentation(self):
        """Test full codebase documentation analysis."""
>       results = self.standardizer.analyze_codebase_documentation()
E       AttributeError: 'TestDocstringStandardizerIntegration' object has no attribute 'standardizer'

tests\test_docstring_standardization.py:189: AttributeError
_________________________ TestDocstringStandardizerIntegration.test_quality_score_calculation _________________________

self = <test_docstring_standardization.TestDocstringStandardizerIntegration object at 0x00000266206E18A0>

    def test_quality_score_calculation(self):
        """Test quality score calculation."""
        # Create some mock issues
>       self.standardizer._create_issue(
            "test.py", 1, "test_func", "missing", "Missing docstring",
            "medium", "Add docstring"
        )
E       AttributeError: 'TestDocstringStandardizerIntegration' object has no attribute 'standardizer'

tests\test_docstring_standardization.py:209: AttributeError
________________________ TestDocstringStandardizerIntegration.test_recommendations_generation _________________________

self = <test_docstring_standardization.TestDocstringStandardizerIntegration object at 0x00000266206E1AB0>

    def test_recommendations_generation(self):
        """Test generation of improvement recommendations."""
>       recommendations = self.standardizer._generate_recommendations()
E       AttributeError: 'TestDocstringStandardizerIntegration' object has no attribute 'standardizer'

tests\test_docstring_standardization.py:220: AttributeError
________________ TestMultiTimeframeRegimeIntegration.test_regime_forecasting_with_multi_timeframe_data ________________

self = <test_integration_all_features.TestMultiTimeframeRegimeIntegration object at 0x000002662084ED10>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x000002661D0F57E0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpbi85mqhf'

    async def test_regime_forecasting_with_multi_timeframe_data(self, generate_regime_data, temp_dir):
        """Test regime forecasting using multi-timeframe data."""
        # Setup timeframe manager
        tf_manager = TimeframeManager(Mock(), {})
        await tf_manager.initialize()

        # Generate multi-timeframe data for bull market
        bull_data = generate_regime_data("bull_market", n_points=200)

        # Mock data fetcher to return multi-timeframe data
        mock_data_fetcher = Mock()
        async def mock_get_historical_data(*args, **kwargs):
            return bull_data

        mock_data_fetcher.get_historical_data = mock_get_historical_data

        # Setup timeframe manager with mock
        tf_manager = TimeframeManager(mock_data_fetcher, {})
        await tf_manager.initialize()

        # Add symbol and get multi-timeframe data
        tf_manager.add_symbol("BTC/USDT", ["1h", "4h"])
        mtf_data = await tf_manager.fetch_multi_timeframe_data("BTC/USDT")

        # Setup regime forecaster
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_integration_all_features.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x000002662694E4D0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpbi85mqhf'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
____________________ TestMultiTimeframeRegimeIntegration.test_regime_transition_detection_with_mtf ____________________

self = <test_integration_all_features.TestMultiTimeframeRegimeIntegration object at 0x000002662084F010>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x00000266268AF520>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpjqh7a1tf'

    async def test_regime_transition_detection_with_mtf(self, generate_regime_data, temp_dir):
        """Test regime transition detection using multi-timeframe analysis."""
        # Generate data for regime transition
        bull_data = generate_regime_data("bull_market", n_points=100)
        bear_data = generate_regime_data("bear_market", n_points=100)

        # Combine data to simulate transition
        transition_data = pd.concat([bull_data, bear_data])

        # Setup components
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_integration_all_features.py:86:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266268AAC80>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpjqh7a1tf'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_____________ TestStrategyGeneratorSelfHealingIntegration.test_strategy_generation_with_health_monitoring _____________

self = <test_integration_all_features.TestStrategyGeneratorSelfHealingIntegration object at 0x000002662084EE60>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpdouemn_g'

    async def test_strategy_generation_with_health_monitoring(self, test_config, temp_dir):
        """Test strategy generation with health monitoring."""
        # Setup self-healing engine
        healing_config = {"monitoring": {"heartbeat_interval": 5}}
        healing_engine = SelfHealingEngine(healing_config)

        # Setup strategy generator
        sg_config = test_config.get("strategy_generator", {})
        sg_config["model_path"] = temp_dir
        sg_config["population_size"] = 5

        strategy_generator = StrategyGenerator(sg_config)
        await strategy_generator.initialize()

        # Register strategy generator with self-healing engine
        healing_engine.register_component(
            "strategy_generator_main",
            ComponentType.EXTERNAL_SERVICE,
            strategy_generator,
            critical=False
        )

        # Start healing engine
        await healing_engine.start()

        # Generate strategies while monitoring health
>       await strategy_generator.evolve()
E       AttributeError: 'StrategyGenerator' object has no attribute 'evolve'

tests\test_integration_all_features.py:134: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:core.watchdog:Overdue heartbeat detected for strategy_generator_main
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for strategy_generator_main
_____________ TestStrategyGeneratorSelfHealingIntegration.test_strategy_deployment_with_failure_recovery ______________

self = <test_integration_all_features.TestStrategyGeneratorSelfHealingIntegration object at 0x000002662084CA00>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpjujdy53t'

    async def test_strategy_deployment_with_failure_recovery(self, test_config, temp_dir):
        """Test strategy deployment with failure recovery."""
        # Setup components
        healing_config = {"monitoring": {"heartbeat_interval": 5}}
        healing_engine = SelfHealingEngine(healing_config)

        sg_config = test_config.get("strategy_generator", {})
        sg_config["model_path"] = temp_dir
        sg_config["population_size"] = 3

        strategy_generator = StrategyGenerator(sg_config)
        await strategy_generator.initialize()

        # Register components
        healing_engine.register_component(
            "strategy_generator_main",
            ComponentType.EXTERNAL_SERVICE,
            strategy_generator,
            critical=False
        )

        # Start monitoring
        await healing_engine.start()

        # Simulate strategy generation failure
        await healing_engine.send_heartbeat(
            component_id="strategy_generator_main",
            status=ComponentStatus.CRITICAL,
            latency_ms=5000.0,
            error_count=5,
            custom_metrics={'error': 'generation_failed'}
        )

        # Allow time for failure detection
        await asyncio.sleep(1)

        # Check that failure was detected
>       assert healing_engine.watchdog_service.failures_detected > 0
E       assert 0 > 0
E        +  where 0 = <core.watchdog.WatchdogService object at 0x000002662694E8C0>.failures_detected
E        +    where <core.watchdog.WatchdogService object at 0x000002662694E8C0> = <core.self_healing_engine.SelfHealingEngine object at 0x000002662694E830>.watchdog_service

tests\test_integration_all_features.py:191: AssertionError
_____________________________ TestFullSystemIntegration.test_end_to_end_trading_workflow ______________________________

self = <test_integration_all_features.TestFullSystemIntegration object at 0x000002662084F6D0>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpwixfpg_p'
synthetic_market_data =                              open          high           low         close      volume
timestamp                     ...923835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]

    async def test_end_to_end_trading_workflow(self, test_config, temp_dir, synthetic_market_data):
        """Test complete end-to-end trading workflow with all features."""
        # Setup all components
        config = test_config.copy()
        config["self_healing"] = {"enabled": True, "heartbeat_interval": 5}

        # Initialize components
        healing_engine = SelfHealingEngine(config)

        tf_manager = TimeframeManager(Mock(), config.get("multi_timeframe", {}))
        await tf_manager.initialize()

>       regime_forecaster = RegimeForecaster({
            "model_path": temp_dir,
            **config.get("regime_forecasting", {})
        })

tests\test_integration_all_features.py:213:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026626C73070>
config = {'confidence_threshold': 0.7, 'enabled': True, 'forecast_horizon': 24, 'model_path': 'models/test_regime_forecaster'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
________________________ TestFullSystemIntegration.test_cross_feature_performance_optimization ________________________

self = <test_integration_all_features.TestFullSystemIntegration object at 0x000002662084FA00>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpq45wap__'
performance_timer = <conftest.performance_timer.<locals>.PerformanceTimer object at 0x0000026626C6D030>

    async def test_cross_feature_performance_optimization(self, test_config, temp_dir, performance_timer):
        """Test performance optimization across all features."""
        # Setup components
        config = test_config.copy()

        healing_engine = SelfHealingEngine(config)
        tf_manager = TimeframeManager(Mock(), {})
        await tf_manager.initialize()

>       regime_forecaster = RegimeForecaster({"model_path": temp_dir})

tests\test_integration_all_features.py:311:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026626C6CCD0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpq45wap__'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_____________________________ TestFullSystemIntegration.test_resource_contention_handling _____________________________

self = <test_integration_all_features.TestFullSystemIntegration object at 0x000002662084FD30>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpux6r1jta'
memory_monitor = <conftest.memory_monitor.<locals>.MemoryMonitor object at 0x0000026626C6CF40>

    async def test_resource_contention_handling(self, test_config, temp_dir, memory_monitor):
        """Test resource contention handling across features."""
        # Setup multiple components
        healing_engine = SelfHealingEngine(test_config)

        components = []
        for i in range(10):
            mock_component = Mock()
            comp_id = f"component_{i}"
            healing_engine.register_component(
                comp_id, ComponentType.STRATEGY, mock_component, critical=False
            )
            components.append((comp_id, mock_component))

        # Start monitoring
        await healing_engine.start()

        memory_monitor.start()

        try:
            # Simulate high-frequency heartbeats from all components
            tasks = []
            for comp_id, _ in components:
                for j in range(5):  # 5 heartbeats per component
                    task = healing_engine.send_heartbeat(
                        component_id=comp_id,
                        status=ComponentStatus.HEALTHY,
                        latency_ms=50.0,
                        error_count=0
                    )
                    tasks.append(task)

            # Execute all heartbeats concurrently
            await asyncio.gather(*tasks)

            # Check memory usage
            memory_delta = memory_monitor.get_memory_delta()
            assert memory_delta < 100  # Reasonable memory usage

            # Verify all heartbeats processed
>           assert healing_engine.watchdog_service.heartbeats_received == 50
E           assert 51 == 50
E            +  where 51 = <core.watchdog.WatchdogService object at 0x000002662694E8C0>.heartbeats_received
E            +    where <core.watchdog.WatchdogService object at 0x000002662694E8C0> = <core.self_healing_engine.SelfHealingEngine object at 0x0000026626C6F100>.watchdog_service

tests\test_integration_all_features.py:394: AssertionError

During handling of the above exception, another exception occurred:

self = <test_integration_all_features.TestFullSystemIntegration object at 0x000002662084FD30>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpux6r1jta'
memory_monitor = <conftest.memory_monitor.<locals>.MemoryMonitor object at 0x0000026626C6CF40>

    async def test_resource_contention_handling(self, test_config, temp_dir, memory_monitor):
        """Test resource contention handling across features."""
        # Setup multiple components
        healing_engine = SelfHealingEngine(test_config)

        components = []
        for i in range(10):
            mock_component = Mock()
            comp_id = f"component_{i}"
            healing_engine.register_component(
                comp_id, ComponentType.STRATEGY, mock_component, critical=False
            )
            components.append((comp_id, mock_component))

        # Start monitoring
        await healing_engine.start()

        memory_monitor.start()

        try:
            # Simulate high-frequency heartbeats from all components
            tasks = []
            for comp_id, _ in components:
                for j in range(5):  # 5 heartbeats per component
                    task = healing_engine.send_heartbeat(
                        component_id=comp_id,
                        status=ComponentStatus.HEALTHY,
                        latency_ms=50.0,
                        error_count=0
                    )
                    tasks.append(task)

            # Execute all heartbeats concurrently
            await asyncio.gather(*tasks)

            # Check memory usage
            memory_delta = memory_monitor.get_memory_delta()
            assert memory_delta < 100  # Reasonable memory usage

            # Verify all heartbeats processed
            assert healing_engine.watchdog_service.heartbeats_received == 50

        finally:
>           await healing_engine.stop()

tests\test_integration_all_features.py:397:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\self_healing_engine.py:884: in stop
    await self.watchdog_service.stop()
core\watchdog.py:740: in stop
    self._monitoring_task.cancel()
..\..\..\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py:753: in call_soon
    self._check_closed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ProactorEventLoop running=False closed=True debug=False>

    def _check_closed(self):
        if self._closed:
>           raise RuntimeError('Event loop is closed')
E           RuntimeError: Event loop is closed

..\..\..\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py:515: RuntimeError
________________________ TestFailureScenariosAcrossFeatures.test_cascading_failure_prevention _________________________

self = <test_integration_all_features.TestFailureScenariosAcrossFeatures object at 0x000002662087C250>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp011ju6xz'

    async def test_cascading_failure_prevention(self, test_config, temp_dir):
        """Test prevention of cascading failures across features."""
        # Setup components
        healing_engine = SelfHealingEngine(test_config)

        # Create interdependent components
        tf_manager = TimeframeManager(Mock(), {})
        await tf_manager.initialize()

>       regime_forecaster = RegimeForecaster({"model_path": temp_dir})

tests\test_integration_all_features.py:413:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266268C1F00>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp011ju6xz'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
____________________ TestFailureScenariosAcrossFeatures.test_recovery_coordination_across_features ____________________

self = <test_integration_all_features.TestFailureScenariosAcrossFeatures object at 0x000002662087C850>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp9gbql6of'

    async def test_recovery_coordination_across_features(self, test_config, temp_dir):
        """Test recovery coordination across multiple features."""
        # Setup components
        healing_engine = SelfHealingEngine(test_config)

        # Register components from different features
        tf_manager = TimeframeManager(Mock(), {})
        await tf_manager.initialize()

>       regime_forecaster = RegimeForecaster({"model_path": temp_dir})

tests\test_integration_all_features.py:495:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266268786A0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp9gbql6of'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_____________________________ TestRealisticMarketScenarios.test_high_volatility_scenario ______________________________

self = <test_integration_all_features.TestRealisticMarketScenarios object at 0x000002662087CD30>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626955BD0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmps1yqqqh2'

    async def test_high_volatility_scenario(self, generate_regime_data, temp_dir):
        """Test high volatility market scenario."""
        # Generate high volatility data
        hv_data = generate_regime_data("high_volatility", n_points=500)

        # Setup all features
        healing_engine = SelfHealingEngine({})

        tf_manager = TimeframeManager(Mock(), {})
        await tf_manager.initialize()

>       regime_forecaster = RegimeForecaster({"model_path": temp_dir})

tests\test_integration_all_features.py:552:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026626A5D6F0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmps1yqqqh2'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
____________________________ TestRealisticMarketScenarios.test_regime_transition_scenario _____________________________

self = <test_integration_all_features.TestRealisticMarketScenarios object at 0x000002662087D060>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x00000266269563B0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp1ln75fer'

    async def test_regime_transition_scenario(self, generate_regime_data, temp_dir):
        """Test regime transition scenario."""
        # Generate data for regime transition
        bull_data = generate_regime_data("bull_market", n_points=200)
        bear_data = generate_regime_data("bear_market", n_points=200)
        transition_data = pd.concat([bull_data, bear_data])

        # Setup features
        healing_engine = SelfHealingEngine({})

>       regime_forecaster = RegimeForecaster({"model_path": temp_dir})

tests\test_integration_all_features.py:618:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266268E8940>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp1ln75fer'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_________________________________ TestProductionReadiness.test_24_hour_stability_test _________________________________

self = <test_integration_all_features.TestProductionReadiness object at 0x000002662087CA90>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpkzpn0229'

    async def test_24_hour_stability_test(self, test_config, temp_dir):
        """Test 24-hour stability (simulated)."""
        # Setup full system
        healing_engine = SelfHealingEngine(test_config)

        tf_manager = TimeframeManager(Mock(), {})
        await tf_manager.initialize()

>       regime_forecaster = RegimeForecaster({"model_path": temp_dir})

tests\test_integration_all_features.py:676:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266269DB730>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpkzpn0229'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_______________________________ TestProductionReadiness.test_resource_usage_under_load ________________________________

self = <test_integration_all_features.TestProductionReadiness object at 0x000002662087C730>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpiyi2gxjf'
memory_monitor = <conftest.memory_monitor.<locals>.MemoryMonitor object at 0x000002662533E170>

    async def test_resource_usage_under_load(self, test_config, temp_dir, memory_monitor):
        """Test resource usage under sustained load."""
        # Setup system
        healing_engine = SelfHealingEngine(test_config)

        # Register many components to simulate load
        for i in range(20):
            mock_component = Mock()
            healing_engine.register_component(
                f"comp_{i}",
                ComponentType.STRATEGY,
                mock_component,
                critical=False
            )

        await healing_engine.start()

        memory_monitor.start()

        try:
            # Simulate sustained load
            for i in range(100):
                # Send heartbeats to all components
                tasks = []
                for j in range(20):
                    task = healing_engine.send_heartbeat(
                        component_id=f"comp_{j}",
                        status=ComponentStatus.HEALTHY,
                        latency_ms=50.0 + np.random.normal(0, 5),
                        error_count=np.random.poisson(0.01)
                    )
                    tasks.append(task)

                await asyncio.gather(*tasks)
                await asyncio.sleep(0.01)

            # Check memory usage
            memory_delta = memory_monitor.get_memory_delta()
            assert memory_delta < 200  # Reasonable memory usage for sustained load

            # Verify all heartbeats processed
>           assert healing_engine.watchdog_service.heartbeats_received == 2000  # 100 iterations * 20 components
E           assert 2055 == 2000
E            +  where 2055 = <core.watchdog.WatchdogService object at 0x000002662694E8C0>.heartbeats_received
E            +    where <core.watchdog.WatchdogService object at 0x000002662694E8C0> = <core.self_healing_engine.SelfHealingEngine object at 0x000002662533EB30>.watchdog_service

tests\test_integration_all_features.py:786: AssertionError
------------------------------------------------ Captured stderr setup ------------------------------------------------
ERROR:asyncio:Task was destroyed but it is pending!
task: <Task pending name='Task-2520' coro=<WatchdogService._monitoring_loop() running at C:\Users\TU\Desktop\new project\N1V1\core\watchdog.py:787> wait_for=<Future cancelled>>
ERROR:asyncio:Task was destroyed but it is pending!
task: <Task pending name='Task-2521' coro=<WatchdogService._recovery_loop() running at C:\Users\TU\Desktop\new project\N1V1\core\watchdog.py:800> wait_for=<Future pending cb=[Task.task_wakeup()]>>
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_4
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_5
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_4
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_5
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_12 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_9 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_0
WARNING:core.watchdog:Failure detected: comp_3 - Logic error or external service degradation
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_1
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_2
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_3
WARNING:core.watchdog:Failure detected: comp_10 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_18 - Logic error or external service degradation
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_4
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_5
WARNING:core.watchdog:Failure detected: comp_0 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_2 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_6 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_10 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_3 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for strategy_generator_main
WARNING:core.watchdog:Failure detected: comp_1 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_0
WARNING:core.watchdog:Failure detected: comp_3 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_5 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_1
WARNING:core.watchdog:Overdue heartbeat detected for component_2
WARNING:core.watchdog:Failure detected: comp_0 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_17 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_3
WARNING:core.watchdog:Overdue heartbeat detected for component_4
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_5
WARNING:core.watchdog:Overdue heartbeat detected for component_6
WARNING:core.watchdog:Failure detected: comp_13 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_7
WARNING:core.watchdog:Failure detected: comp_8 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_8
WARNING:core.watchdog:Overdue heartbeat detected for component_9
WARNING:core.watchdog:Failure detected: comp_2 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_0
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_1
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_2
WARNING:core.watchdog:Failure detected: comp_14 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_3
WARNING:core.watchdog:Failure detected: comp_5 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_4
WARNING:core.watchdog:Failure detected: comp_16 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_5
WARNING:core.watchdog:Failure detected: comp_6 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_2 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_14 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_6 - Logic error or external service degradation
WARNING:core.watchdog:Overdue heartbeat detected for strategy_generator_main
WARNING:core.watchdog:Failure detected: comp_19 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_0
WARNING:core.watchdog:Failure detected: comp_10 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_1
WARNING:core.watchdog:Failure detected: comp_6 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_2
WARNING:core.watchdog:Overdue heartbeat detected for component_3
WARNING:core.watchdog:Overdue heartbeat detected for component_4
WARNING:core.watchdog:Overdue heartbeat detected for component_5
WARNING:core.watchdog:Failure detected: comp_6 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_6
WARNING:core.watchdog:Overdue heartbeat detected for component_7
WARNING:core.watchdog:Failure detected: comp_7 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_16 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_8
WARNING:core.watchdog:Failure detected: comp_14 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_19 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_9
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_0
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_1
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_2
WARNING:core.watchdog:Failure detected: comp_1 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_12 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_17 - Logic error or external service degradation
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_3
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_4
WARNING:core.watchdog:Failure detected: comp_13 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_5
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for strategy_generator_main
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for component_0
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for component_1
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for component_2
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for component_3
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for component_4
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for component_5
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for component_6
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Overdue heartbeat detected for component_7
WARNING:core.watchdog:Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_13 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_8
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_14 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_17 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_9
WARNING:core.watchdog:Failure detected: comp_10 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_0
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_1
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_2
WARNING:core.watchdog:Failure detected: comp_9 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_16 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_3
WARNING:core.watchdog:Failure detected: comp_15 - Logic error or external service degradation
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_4
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_5
WARNING:core.watchdog:Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: comp_6 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_7 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_14 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_1 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_2 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_8 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_12 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for strategy_generator_main
WARNING:core.watchdog:Failure detected: comp_1 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_16 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_0
WARNING:core.watchdog:Failure detected: comp_11 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_12 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_13 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_1
WARNING:core.watchdog:Failure detected: comp_10 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_13 - Logic error or external service degradation
WARNING:core.watchdog:Overdue heartbeat detected for component_2
WARNING:core.watchdog:Overdue heartbeat detected for component_3
WARNING:core.watchdog:Failure detected: comp_0 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_10 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_4
WARNING:core.watchdog:Overdue heartbeat detected for component_5
WARNING:core.watchdog:Overdue heartbeat detected for component_6
WARNING:core.watchdog:Failure detected: comp_10 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_7
WARNING:core.watchdog:Overdue heartbeat detected for component_8
WARNING:core.watchdog:Failure detected: comp_1 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_14 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for component_9
WARNING:core.watchdog:Failure detected: comp_3 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_6 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_16 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_0
WARNING:core.watchdog:Failure detected: comp_13 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_1
WARNING:core.watchdog:Failure detected: comp_15 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_2
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_3
WARNING:core.watchdog:Failure detected: comp_15 - High system load or network congestion
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_4
WARNING:core.watchdog:Overdue heartbeat detected for critical_comp_5
WARNING:core.watchdog:Failure detected: comp_2 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_6 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_13 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_1 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_2 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_3 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_4 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_6 - Logic error or external service degradation
WARNING:core.watchdog:Failure detected: comp_7 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_12 - High system load or network congestion
WARNING:core.watchdog:Failure detected: comp_17 - High system load or network congestion
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_4
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_5
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_4
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_5
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_0
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_1
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_2
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_3
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_4
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_5
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for strategy_generator_main
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_0
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_1
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_2
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_3
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_4
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_5
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_6
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_7
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_8
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_9
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_0
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_1
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_2
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_3
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_4
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_5
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for strategy_generator_main
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_0
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_1
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_2
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_3
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_4
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_5
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_6
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_7
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_8
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_9
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_0
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_1
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_2
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_3
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_4
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_5
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for strategy_generator_main
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_0
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_1
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_2
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_3
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_4
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_5
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_6
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_18 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_19 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_7
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_5 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_8
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_9
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_0
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_1
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_2
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_9 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_3
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_4
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_5
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_8 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for strategy_generator_main
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_0
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_11 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_1
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_2
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_3
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_0 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_4
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_5
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_6
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_10 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_7
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_8
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_14 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for component_9
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_16 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_0
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_1
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_2
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_3
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_15 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_4
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for critical_comp_5
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_13 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_1 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_2 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_3 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_4 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_6 - Logic error or external service degradation
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_7 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_12 - High system load or network congestion
WARNING  core.watchdog:watchdog.py:773 Failure detected: comp_17 - High system load or network congestion
__________________________________ TestProductionReadiness.test_security_integration __________________________________

self = <test_integration_all_features.TestProductionReadiness object at 0x000002662087C0D0>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}

    async def test_security_integration(self, test_config):
        """Test security integration with all features."""
        # Setup system
        healing_engine = SelfHealingEngine(test_config)

        # Register components
        mock_component = Mock()
        healing_engine.register_component(
            "secure_comp",
            ComponentType.BOT_ENGINE,
            mock_component,
            critical=True
        )

        await healing_engine.start()

        try:
            # Test that heartbeat data is handled securely
            # (In real implementation, this would test encryption, authentication, etc.)

            await healing_engine.send_heartbeat(
                component_id="secure_comp",
                status=ComponentStatus.HEALTHY,
                latency_ms=50.0,
                error_count=0,
                custom_metrics={'secure_data': 'test'}
            )

            # Verify heartbeat was processed
>           assert healing_engine.watchdog_service.heartbeats_received == 1
E           assert 2056 == 1
E            +  where 2056 = <core.watchdog.WatchdogService object at 0x000002662694E8C0>.heartbeats_received
E            +    where <core.watchdog.WatchdogService object at 0x000002662694E8C0> = <core.self_healing_engine.SelfHealingEngine object at 0x00000266269D2200>.watchdog_service

tests\test_integration_all_features.py:820: AssertionError
_________________________________ TestMainFunction.test_main_fastapi_mode_unavailable _________________________________

self = <MagicMock name='exit' id='2637757719952'>, args = (1,), kwargs = {}
msg = "Expected 'exit' to be called once. Called 2 times.\nCalls: [call(1), call(1)]."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'exit' to be called once. Called 2 times.
E           Calls: [call(1), call(1)].

..\..\..\AppData\Local\Programs\Python\Python310\lib\unittest\mock.py:940: AssertionError

During handling of the above exception, another exception occurred:

self = <test_main.TestMainFunction object at 0x00000266209B0940>
mock_getenv = <MagicMock name='getenv' id='2637756270384'>
mock_parse_args = <MagicMock name='parse_arguments' id='2637756273312'>

    @patch('main.parse_arguments')
    @patch('os.getenv')
    @patch('main.FASTAPI_AVAILABLE', False)
    @pytest.mark.asyncio
    async def test_main_fastapi_mode_unavailable(self, mock_getenv, mock_parse_args):
        """Test main function when FastAPI is requested but unavailable."""
        # Setup mocks
        mock_args = MagicMock()
        mock_args.api = True
        mock_parse_args.return_value = mock_args

        with patch('sys.exit') as mock_exit, \
             patch('main.logging.getLogger') as mock_logger:

            mock_logger.return_value = MagicMock()

            # Mock uvicorn.run to avoid the actual call
            with patch('main.uvicorn') as mock_uvicorn:
                mock_uvicorn.run = MagicMock()
                await main()

            # Verify exit was called with error
>           mock_exit.assert_called_once_with(1)
E           AssertionError: Expected 'exit' to be called once. Called 2 times.
E           Calls: [call(1), call(1)].

tests\test_main.py:350: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
INFO:utils.config_loader:Configuration loaded successfully
INFO:utils.logger:TradeLogger subscribed to event bus for event-driven logging
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     utils.config_loader:config_loader.py:450 Configuration loaded successfully
INFO     utils.logger:logger.py:603 TradeLogger subscribed to event bus for event-driven logging
___________________________ test_ml_confirmation_rejects_weak_signal_on_opposite_prediction ___________________________

    @pytest.mark.asyncio
    async def test_ml_confirmation_rejects_weak_signal_on_opposite_prediction():
        """Test that ML confirmation rejects a weak signal when ML predicts opposite direction with high confidence."""

        # Create router with ML enabled
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_ml_signal_router.py:27:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266269F5DE0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________ test_ml_confirmation_accepts_signal_on_matching_prediction ______________________________

    @pytest.mark.asyncio
    async def test_ml_confirmation_accepts_signal_on_matching_prediction():
        """Test that ML confirmation accepts a signal when ML predicts the same direction with high confidence."""

        # Create a mock ML model and prediction function
        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": 1, "confidence": 0.8}])  # Same direction, high confidence

        def mock_predict(model, features_df):
            return mock_prediction_df

        # Create router with ML enabled
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_ml_signal_router.py:84:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026626CBE860>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________________ test_ml_confirmation_skips_when_no_features _____________________________________

    @pytest.mark.asyncio
    async def test_ml_confirmation_skips_when_no_features():
        """Test that ML confirmation is skipped when no feature data is available."""

        # Create router with ML enabled
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_ml_signal_router.py:127:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026626DEAE30>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________ TestWalkForwardOptimizer.test_initialization _____________________________________

self = <test_optimization.TestWalkForwardOptimizer object at 0x00000266221A11B0>

    def test_initialization(self):
        """Test WalkForwardOptimizer initialization."""
        config = {
            'train_window_days': 90,
            'test_window_days': 30,
            'rolling': True,
            'min_observations': 1000,
            'improvement_threshold': 0.05
        }
        optimizer = WalkForwardOptimizer(config)

>       assert optimizer.train_window_days == 90
E       AttributeError: 'WalkForwardOptimizer' object has no attribute 'train_window_days'

tests\test_optimization.py:72: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:39,405 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
___________________________________ TestWalkForwardOptimizer.test_window_generation ___________________________________

self = <test_optimization.TestWalkForwardOptimizer object at 0x00000266221A13C0>

    def test_window_generation(self):
        """Test walk-forward window generation."""
        config = {'train_window_days': 90, 'test_window_days': 30, 'min_observations': 100}
        optimizer = WalkForwardOptimizer(config)

        # Create mock data with more points to generate windows
        dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')  # 2 years of data
        data = pd.DataFrame({'close': np.random.randn(len(dates))}, index=dates)

>       windows = optimizer._generate_windows(data)
E       AttributeError: 'WalkForwardOptimizer' object has no attribute '_generate_windows'

tests\test_optimization.py:86: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:39,444 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,444 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
________________________________ TestWalkForwardOptimizer.test_parameter_combinations _________________________________

self = <test_optimization.TestWalkForwardOptimizer object at 0x00000266221A15D0>

    def test_parameter_combinations(self):
        """Test parameter combination generation."""
        config = {}
        optimizer = WalkForwardOptimizer(config)

>       combinations = optimizer._generate_param_combinations()
E       AttributeError: 'WalkForwardOptimizer' object has no attribute '_generate_param_combinations'

tests\test_optimization.py:102: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:39,489 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,489 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,489 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
____________________________________ TestOptimizerFactory.test_create_from_config _____________________________________

self = <test_optimization.TestOptimizerFactory object at 0x00000266221C00D0>

    def test_create_from_config(self):
        """Test creation from full config."""
        config = {
            'optimization': {
                'enabled': True,
                'mode': 'wfo',
                'train_window_days': 60
            }
        }

        optimizer = OptimizerFactory.create_from_config(config)
        assert isinstance(optimizer, WalkForwardOptimizer)
>       assert optimizer.train_window_days == 60
E       AttributeError: 'WalkForwardOptimizer' object has no attribute 'train_window_days'

tests\test_optimization.py:648: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:39,756 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,756 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,756 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,756 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,756 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
______________________________ TestCrossPairValidation.test_cross_pair_validation_basic _______________________________

self = <test_optimization.TestCrossPairValidation object at 0x00000266221C0850>

    def test_cross_pair_validation_basic(self):
        """Test basic cross-pair validation functionality."""
        config = {'min_observations': 100}
        optimizer = WalkForwardOptimizer(config)

        # Mock strategy class
        mock_strategy = Mock()

        # Create mock data for multiple pairs
        dates = pd.date_range('2023-01-01', periods=200, freq='D')
        data_dict = {
            'BTC/USDT': pd.DataFrame({'close': np.random.randn(200).cumsum() + 100}, index=dates),
            'ETH/USDT': pd.DataFrame({'close': np.random.randn(200).cumsum() + 50}, index=dates),
            'SOL/USDT': pd.DataFrame({'close': np.random.randn(200).cumsum() + 20}, index=dates)
        }

        train_pair = 'BTC/USDT'
        validation_pairs = ['ETH/USDT', 'SOL/USDT']

        # Mock the optimize method to return some params
        optimizer.optimize = Mock(return_value={'rsi_period': 14, 'overbought': 70, 'oversold': 30})

>       results = optimizer.cross_pair_validation(
            mock_strategy, data_dict, train_pair, validation_pairs
        )
E       AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'

tests\test_optimization.py:824: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:39,835 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,835 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,835 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,835 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,835 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,835 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
______________________ TestCrossPairValidation.test_cross_pair_validation_empty_validation_pairs ______________________

self = <test_optimization.TestCrossPairValidation object at 0x00000266221C1F00>

    def test_cross_pair_validation_empty_validation_pairs(self):
        """Test cross-pair validation with empty validation pairs."""
        config = {'min_observations': 100}
        optimizer = WalkForwardOptimizer(config)

        mock_strategy = Mock()

        dates = pd.date_range('2023-01-01', periods=200, freq='D')
        data_dict = {
            'BTC/USDT': pd.DataFrame({'close': np.random.randn(200).cumsum() + 100}, index=dates)
        }

        train_pair = 'BTC/USDT'
        validation_pairs = []

        optimizer.optimize = Mock(return_value={'rsi_period': 14})

>       results = optimizer.cross_pair_validation(
            mock_strategy, data_dict, train_pair, validation_pairs
        )
E       AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'

tests\test_optimization.py:864: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:39,885 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,885 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,885 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,885 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,885 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,885 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,885 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
________________________ TestCrossPairValidation.test_cross_pair_validation_missing_train_pair ________________________

self = <test_optimization.TestCrossPairValidation object at 0x00000266221C2110>

    def test_cross_pair_validation_missing_train_pair(self):
        """Test cross-pair validation with missing train pair data."""
        config = {'min_observations': 100}
        optimizer = WalkForwardOptimizer(config)

        mock_strategy = Mock()

        data_dict = {
            'ETH/USDT': pd.DataFrame({'close': [1, 2, 3]})
        }

        train_pair = 'BTC/USDT'
        validation_pairs = ['ETH/USDT']

>       results = optimizer.cross_pair_validation(
            mock_strategy, data_dict, train_pair, validation_pairs
        )
E       AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'

tests\test_optimization.py:886: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:39,928 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,928 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,928 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,928 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,928 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,928 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,928 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,928 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
________________________ TestCrossPairValidation.test_cross_pair_validation_insufficient_data _________________________

self = <test_optimization.TestCrossPairValidation object at 0x00000266221C2320>

    def test_cross_pair_validation_insufficient_data(self):
        """Test cross-pair validation with insufficient data."""
        config = {'min_observations': 1000}  # High requirement
        optimizer = WalkForwardOptimizer(config)

        mock_strategy = Mock()

        dates = pd.date_range('2023-01-01', periods=50, freq='D')  # Small dataset
        data_dict = {
            'BTC/USDT': pd.DataFrame({'close': np.random.randn(50).cumsum() + 100}, index=dates)
        }

        train_pair = 'BTC/USDT'
        validation_pairs = []

>       results = optimizer.cross_pair_validation(
            mock_strategy, data_dict, train_pair, validation_pairs
        )
E       AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'

tests\test_optimization.py:908: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:39,976 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
_____________________________ TestCrossPairValidation.test_save_cross_validation_results ______________________________

self = <test_optimization.TestCrossPairValidation object at 0x00000266221C2530>

    def test_save_cross_validation_results(self):
        """Test saving cross-validation results."""
        config = {}
        optimizer = WalkForwardOptimizer(config)

        results = {
            "train_pair": "BTC/USDT",
            "validation_pairs": ["ETH/USDT"],
            "best_params": {"rsi_period": 14},
            "results": {
                "BTC/USDT": {
                    "sharpe_ratio": 1.5,
                    "max_drawdown": 0.1,
                    "profit_factor": 1.2,
                    "total_return": 0.15,
                    "win_rate": 0.6,
                    "total_trades": 50,
                    "expectancy": 0.02
                }
            }
        }

        # Test saving
>       optimizer._save_cross_validation_results(results)
E       AttributeError: 'WalkForwardOptimizer' object has no attribute '_save_cross_validation_results'

tests\test_optimization.py:938: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,018 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
___________________________ TestCrossPairValidation.test_cross_pair_validation_with_config ____________________________

self = <test_optimization.TestCrossPairValidation object at 0x00000266221C2740>

    def test_cross_pair_validation_with_config(self):
        """Test cross-pair validation using config settings."""
        config = {
            'min_observations': 100,
            'train_window_days': 90,
            'test_window_days': 30
        }
        optimizer = WalkForwardOptimizer(config)

        # Mock strategy class
        mock_strategy = Mock()

        # Create mock data for multiple pairs
        dates = pd.date_range('2023-01-01', periods=300, freq='D')
        data_dict = {
            'BTC/USDT': pd.DataFrame({'close': np.random.randn(300).cumsum() + 100}, index=dates),
            'ETH/USDT': pd.DataFrame({'close': np.random.randn(300).cumsum() + 50}, index=dates),
            'SOL/USDT': pd.DataFrame({'close': np.random.randn(300).cumsum() + 20}, index=dates)
        }

        # Mock the optimize method
        optimizer.optimize = Mock(return_value={'rsi_period': 14, 'overbought': 70, 'oversold': 30})

        # Test with config-like parameters
>       results = optimizer.cross_pair_validation(
            mock_strategy, data_dict, 'BTC/USDT', ['ETH/USDT', 'SOL/USDT']
        )
E       AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'

tests\test_optimization.py:974: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,064 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
__________________ TestCrossPairValidation.test_cross_pair_validation_skip_if_empty_validation_pairs __________________

self = <test_optimization.TestCrossPairValidation object at 0x00000266221C2950>

    def test_cross_pair_validation_skip_if_empty_validation_pairs(self):
        """Test that cross-pair validation gracefully skips when validation_pairs is empty."""
        config = {'min_observations': 100}
        optimizer = WalkForwardOptimizer(config)

        mock_strategy = Mock()
        dates = pd.date_range('2023-01-01', periods=200, freq='D')
        data_dict = {
            'BTC/USDT': pd.DataFrame({'close': np.random.randn(200).cumsum() + 100}, index=dates)
        }

        # Mock the optimize method
        optimizer.optimize = Mock(return_value={'rsi_period': 14})

        # Test with empty validation pairs
>       results = optimizer.cross_pair_validation(
            mock_strategy, data_dict, 'BTC/USDT', []
        )
E       AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'

tests\test_optimization.py:1009: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
2025-09-08 16:21:40,110 - WalkForwardOptimizer - INFO - Walk-Forward Optimizer initialized
INFO:WalkForwardOptimizer:Walk-Forward Optimizer initialized
-------------------------------------------------- Captured log call --------------------------------------------------
INFO     WalkForwardOptimizer:walk_forward.py:324 Walk-Forward Optimizer initialized
_________________________________ TestProfilingAccuracy.test_function_timing_accuracy _________________________________

self = <test_performance_optimization.TestProfilingAccuracy object at 0x0000026622275030>

    def test_function_timing_accuracy(self):
        """Test function-level timing accuracy against high-precision references."""
        # Test function with known execution time
        def test_function():
            time.sleep(0.01)  # 10ms sleep
            return 42

        # Profile the function
        with self.profiler.profile_function("test_function"):
            start_time = time.perf_counter()
            result = test_function()
            end_time = time.perf_counter()

        reference_time = end_time - start_time

        # Check that profiler captured the timing
>       assert "test_function" in self.profiler.metrics_history
E       AssertionError: assert 'test_function' in deque([])
E        +  where deque([]) = <core.performance_profiler.PerformanceProfiler object at 0x00000266278179A0>.metrics_history
E        +    where <core.performance_profiler.PerformanceProfiler object at 0x00000266278179A0> = <test_performance_optimization.TestProfilingAccuracy object at 0x0000026622275030>.profiler

tests\test_performance_optimization.py:70: AssertionError
__________________________________ TestProfilingAccuracy.test_memory_usage_tracking ___________________________________

self = <test_performance_optimization.TestProfilingAccuracy object at 0x0000026622275270>

    def test_memory_usage_tracking(self):
        """Test memory usage tracking against actual allocations."""
        # Start memory tracking
        self.profiler.enable_memory_tracking()

        # Function that allocates memory
        def memory_test_function():
            data = []
            for i in range(1000):
                data.append([i] * 100)  # Allocate ~1000 lists of 100 integers each
            return data

>       initial_memory = self.profiler._get_memory_usage()
E       AttributeError: 'PerformanceProfiler' object has no attribute '_get_memory_usage'

tests\test_performance_optimization.py:88: AttributeError
_________________________________ TestProfilingAccuracy.test_io_operations_monitoring _________________________________

self = <test_performance_optimization.TestProfilingAccuracy object at 0x00000266222754B0>

    def test_io_operations_monitoring(self):
        """Test I/O operation monitoring captures relevant events."""
        # Create temporary file for I/O testing
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
            temp_file = f.name

        def io_test_function():
            # Perform file I/O operations
            with open(temp_file, 'w') as f:
                for i in range(1000):
                    f.write(f"Line {i}\n")

            with open(temp_file, 'r') as f:
                data = f.read()

            return len(data)

        with self.profiler.profile_function("io_test"):
            result = io_test_function()

        # Cleanup
        os.unlink(temp_file)

        # Check I/O monitoring
>       metrics = [m for m in self.profiler.metrics_history if m.function_name == "io_test"][-1]
E       IndexError: list index out of range

tests\test_performance_optimization.py:129: IndexError
________________________________ TestProfilingAccuracy.test_garbage_collection_impact _________________________________

self = <test_performance_optimization.TestProfilingAccuracy object at 0x00000266222756F0>

    def test_garbage_collection_impact(self):
        """Test garbage collection impact measurements."""
        def gc_test_function():
            # Create many objects to trigger GC
            objects = []
            for i in range(10000):
                objects.append({"data": [i] * 10})

            # Force garbage collection
            import gc
            collected = gc.collect()

            return collected

        with self.profiler.profile_function("gc_test"):
            result = gc_test_function()

>       metrics = [m for m in self.profiler.metrics_history if m.function_name == "gc_test"][-1]
E       IndexError: list index out of range

tests\test_performance_optimization.py:152: IndexError
___________________________________ TestProfilingAccuracy.test_cpu_usage_monitoring ___________________________________

self = <test_performance_optimization.TestProfilingAccuracy object at 0x0000026622275930>

    def test_cpu_usage_monitoring(self):
        """Test CPU usage monitoring during profiling."""
        def cpu_intensive_function():
            # CPU-intensive computation
            result = 0
            for i in range(100000):
                result += i ** 2
            return result

        with self.profiler.profile_function("cpu_test"):
            result = cpu_intensive_function()

>       metrics = [m for m in self.profiler.metrics_history if m.function_name == "cpu_test"][-1]
E       IndexError: list index out of range

tests\test_performance_optimization.py:174: IndexError
______________________________ TestProfilingAccuracy.test_concurrent_profiling_accuracy _______________________________

self = <test_performance_optimization.TestProfilingAccuracy object at 0x0000026622275B70>

    def test_concurrent_profiling_accuracy(self):
        """Test profiling accuracy under concurrent execution."""
        async def profiled_task(task_id: int):
            """Task that will be profiled."""
            await asyncio.sleep(0.01 * task_id)  # Variable sleep
            return task_id * 2

        async def run_concurrent_profiling():
            """Run multiple profiled tasks concurrently."""
            tasks = []
            for i in range(5):
                task = asyncio.create_task(
                    self.profiler.async_profile_function(
                        profiled_task(i),
                        f"concurrent_task_{i}"
                    )
                )
                tasks.append(task)

            results = await asyncio.gather(*tasks)
            return results

        # Run concurrent profiling
>       results = asyncio.run(run_concurrent_profiling())

tests\test_performance_optimization.py:203:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\..\..\AppData\Local\Programs\Python\Python310\lib\asyncio\runners.py:44: in run
    return loop.run_until_complete(main)
..\..\..\AppData\Local\Programs\Python\Python310\lib\asyncio\base_events.py:649: in run_until_complete
    return future.result()
tests\test_performance_optimization.py:199: in run_concurrent_profiling
    results = await asyncio.gather(*tasks)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.performance_profiler.PerformanceProfiler object at 0x00000266278BA4A0>
coro = <coroutine object TestProfilingAccuracy.test_concurrent_profiling_accuracy.<locals>.profiled_task at 0x0000026627810D60>
function_name = 'concurrent_task_0'

    async def async_profile_function(self, coro: Callable, function_name: str):
        """
        Profile an async function.

        Args:
            coro: Coroutine to profile
            function_name: Name of the function
        """
        if not self.is_profiling:
>           return await coro()
E           TypeError: 'coroutine' object is not callable

core\performance_profiler.py:517: TypeError
___________________________ TestOptimizationValidation.test_optimization_result_consistency ___________________________

self = <test_performance_optimization.TestOptimizationValidation object at 0x0000026622275A80>

    def test_optimization_result_consistency(self):
        """Test optimization results are consistent across multiple runs."""
        def test_computation():
            """Computation that should produce consistent results."""
            np.random.seed(42)  # Fixed seed for reproducibility
            data = np.random.random(100)
            return np.sum(data ** 2)

        # Run multiple times
        results = [test_computation() for _ in range(10)]

        # All results should be identical
        assert all(r == results[0] for r in results)

        # Verify expected value
>       assert abs(results[0] - 33.367) < 1e-3  # Expected value with seed 42
E       assert 2.4985118386735223 < 0.001
E        +  where 2.4985118386735223 = abs((30.868488161326475 - 33.367))

tests\test_performance_optimization.py:338: AssertionError
________________________________ TestPerformanceBenchmarks.test_vectorization_speedup _________________________________

self = <test_performance_optimization.TestPerformanceBenchmarks object at 0x0000026622276CE0>

    def test_vectorization_speedup(self):
        """Test vectorized operations achieve expected speedup (2-10x target)."""
        # Large test data
        size = 100000
        data = np.random.random(size)

        # Loop-based implementation
        def loop_based_sum(data):
            result = 0.0
            for x in data:
                result += x ** 2
            return result

        # Vectorized implementation
        def vectorized_sum(data):
            return np.sum(data ** 2)

        # Benchmark loop-based
        start_time = time.perf_counter()
        loop_result = loop_based_sum(data)
        loop_time = time.perf_counter() - start_time

        # Benchmark vectorized
        start_time = time.perf_counter()
        vectorized_result = vectorized_sum(data)
        vectorized_time = time.perf_counter() - start_time

        # Results should be approximately equal
>       assert abs(loop_result - vectorized_result) < 1e-10
E       assert 4.729372449219227e-10 < 1e-10
E        +  where 4.729372449219227e-10 = abs((33263.75086923589 - 33263.75086923542))

tests\test_performance_optimization.py:376: AssertionError
________________________________ TestPerformanceBenchmarks.test_baseline_establishment ________________________________

self = <test_performance_optimization.TestPerformanceBenchmarks object at 0x00000266222775E0>

    def test_baseline_establishment(self):
        """Test performance baseline establishment for components."""
        # Simulate collecting performance baselines
        baseline_data = []

        for i in range(20):  # Collect 20 samples
            start_time = time.perf_counter()

            # Simulate component execution
            data = np.random.random(1000)
            result = np.sum(data ** 2)

            execution_time = time.perf_counter() - start_time
            baseline_data.append(execution_time)

        # Calculate baseline statistics
        mean_time = statistics.mean(baseline_data)
        std_time = statistics.stdev(baseline_data)
        min_time = min(baseline_data)
        max_time = max(baseline_data)

        # Verify baseline quality
        assert len(baseline_data) == 20
        assert mean_time > 0
        assert std_time >= 0
        assert min_time <= mean_time <= max_time

        # Coefficient of variation should be reasonable (< 50%)
        cv = std_time / mean_time if mean_time > 0 else 0
>       assert cv < 0.5, f"Baseline variability too high: CV = {cv:.2f}"
E       AssertionError: Baseline variability too high: CV = 0.79
E       assert 0.7899758778467014 < 0.5

tests\test_performance_optimization.py:500: AssertionError
_________________________ TestRegimeForecasterInitialization.test_initialization_with_config __________________________

self = <test_regime_forecaster.TestRegimeForecasterInitialization object at 0x00000266222EEC80>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp49_gewzx'

    def test_initialization_with_config(self, test_config, temp_dir):
        """Test forecaster initialization with configuration."""
        config = test_config.get("regime_forecasting", {})
        config["model_path"] = temp_dir

>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:37:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266272D42E0>
config = {'confidence_threshold': 0.7, 'enabled': True, 'forecast_horizon': 24, 'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp49_gewzx'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'enabled'

strategies\regime\regime_forecaster.py:580: TypeError
____________________________ TestRegimeForecasterInitialization.test_async_initialization _____________________________

self = <test_regime_forecaster.TestRegimeForecasterInitialization object at 0x00000266222EEEF0>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpv45bezri'

    @pytest.mark.asyncio
    async def test_async_initialization(self, test_config, temp_dir):
        """Test asynchronous initialization."""
        config = test_config.get("regime_forecasting", {})
        config["model_path"] = temp_dir

>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:51:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266278818A0>
config = {'confidence_threshold': 0.7, 'enabled': True, 'forecast_horizon': 24, 'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpv45bezri'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'enabled'

strategies\regime\regime_forecaster.py:580: TypeError
________________________ TestRegimeForecasterInitialization.test_initialization_without_config ________________________

self = <test_regime_forecaster.TestRegimeForecasterInitialization object at 0x00000266222ED060>

    def test_initialization_without_config(self):
        """Test initialization with default configuration."""
        forecaster = RegimeForecaster({})

>       assert forecaster.forecast_horizon == 24
E       AttributeError: 'RegimeForecaster' object has no attribute 'forecast_horizon'

tests\test_regime_forecaster.py:62: AttributeError
________________________________ TestFeatureEngineering.test_feature_engineering_basic ________________________________

self = <test_regime_forecaster.TestFeatureEngineering object at 0x00000266222EF2B0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FEAF80>

    def test_feature_engineering_basic(self, generate_regime_data):
        """Test basic feature engineering."""
        # Generate bull market data
        data = generate_regime_data("bull_market", n_points=100)

        forecaster = RegimeForecaster({})

        # Test feature extraction
>       features = forecaster._extract_features(data)
E       AttributeError: 'RegimeForecaster' object has no attribute '_extract_features'

tests\test_regime_forecaster.py:78: AttributeError
__________________________ TestFeatureEngineering.test_feature_engineering_different_regimes __________________________

self = <test_regime_forecaster.TestFeatureEngineering object at 0x00000266222EF4C0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FEB5B0>

    def test_feature_engineering_different_regimes(self, generate_regime_data):
        """Test feature engineering across different market regimes."""
        regimes = ["bull_market", "bear_market", "sideways", "high_volatility"]

        forecaster = RegimeForecaster({})

        for regime in regimes:
            data = generate_regime_data(regime, n_points=50)

>           features = forecaster._extract_features(data)
E           AttributeError: 'RegimeForecaster' object has no attribute '_extract_features'

tests\test_regime_forecaster.py:97: AttributeError
_____________________________ TestFeatureEngineering.test_feature_engineering_edge_cases ______________________________

self = <test_regime_forecaster.TestFeatureEngineering object at 0x00000266222EF6D0>

    def test_feature_engineering_edge_cases(self):
        """Test feature engineering with edge cases."""
        forecaster = RegimeForecaster({})

        # Empty data
        empty_data = pd.DataFrame()
>       features = forecaster._extract_features(empty_data)
E       AttributeError: 'RegimeForecaster' object has no attribute '_extract_features'

tests\test_regime_forecaster.py:112: AttributeError
____________________________ TestFeatureEngineering.test_technical_indicators_calculation _____________________________

self = <test_regime_forecaster.TestFeatureEngineering object at 0x00000266222EF8E0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FE9480>

    def test_technical_indicators_calculation(self, generate_regime_data):
        """Test technical indicators calculation."""
        data = generate_regime_data("bull_market", n_points=100)

        forecaster = RegimeForecaster({})

        # Test RSI calculation
>       rsi = forecaster._calculate_rsi(data['close'], period=14)
E       AttributeError: 'RegimeForecaster' object has no attribute '_calculate_rsi'

tests\test_regime_forecaster.py:144: AttributeError
_____________________________________ TestModelTraining.test_model_initialization _____________________________________

self = <test_regime_forecaster.TestModelTraining object at 0x00000266222EFD90>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmppkkgcsvk'

    def test_model_initialization(self, test_config, temp_dir):
        """Test model initialization."""
        config = test_config.get("regime_forecasting", {})
        config["model_path"] = temp_dir

>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:173:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266277D5AB0>
config = {'confidence_threshold': 0.7, 'enabled': True, 'forecast_horizon': 24, 'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmppkkgcsvk'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'enabled'

strategies\regime\regime_forecaster.py:580: TypeError
__________________________________ TestModelTraining.test_training_data_preparation ___________________________________

self = <test_regime_forecaster.TestModelTraining object at 0x00000266222EFFA0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FE83A0>

    def test_training_data_preparation(self, generate_regime_data):
        """Test training data preparation."""
        # Generate training data for multiple regimes
        regimes = ["bull_market", "bear_market", "sideways"]
        training_data = []

        for regime in regimes:
            data = generate_regime_data(regime, n_points=200)
            training_data.append((data, regime))

        forecaster = RegimeForecaster({})

>       X, y = forecaster._prepare_training_data(training_data)
E       AttributeError: 'RegimeForecaster' object has no attribute '_prepare_training_data'

tests\test_regime_forecaster.py:190: AttributeError
________________________________________ TestModelTraining.test_model_training ________________________________________

self = <test_regime_forecaster.TestModelTraining object at 0x000002662231C250>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FE8040>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp4h7clnjd'

    @pytest.mark.asyncio
    async def test_model_training(self, generate_regime_data, temp_dir):
        """Test model training process."""
        # Generate training data
        training_data = []
        regimes = ["bull_market", "bear_market", "sideways"]

        for regime in regimes:
            data = generate_regime_data(regime, n_points=100)
            training_data.append((data, regime))

        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:216:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026626CC7AC0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp4h7clnjd'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
______________________________________ TestModelTraining.test_model_persistence _______________________________________

self = <test_regime_forecaster.TestModelTraining object at 0x000002662231C5B0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FEBC70>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp51_gur3t'

    @pytest.mark.asyncio
    async def test_model_persistence(self, generate_regime_data, temp_dir):
        """Test model saving and loading."""
        # Generate training data
        training_data = []
        regimes = ["bull_market", "bear_market"]

        for regime in regimes:
            data = generate_regime_data(regime, n_points=50)
            training_data.append((data, regime))

        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:237:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266279696C0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp51_gur3t'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
________________________________________ TestPrediction.test_regime_prediction ________________________________________

self = <test_regime_forecaster.TestPrediction object at 0x000002662231CA90>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FEB7F0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp3761d08a'

    @pytest.mark.asyncio
    async def test_regime_prediction(self, generate_regime_data, temp_dir):
        """Test regime prediction."""
        # Generate training data
        training_data = []
        regimes = ["bull_market", "bear_market", "sideways"]

        for regime in regimes:
            data = generate_regime_data(regime, n_points=100)
            training_data.append((data, regime))

        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:267:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627ABCC70>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp3761d08a'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
________________________________ TestPrediction.test_prediction_confidence_calibration ________________________________

self = <test_regime_forecaster.TestPrediction object at 0x000002662231CDC0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FE9A20>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpyoextjou'

    @pytest.mark.asyncio
    async def test_prediction_confidence_calibration(self, generate_regime_data, temp_dir):
        """Test prediction confidence calibration."""
        # Generate clear bull market data
        bull_data = generate_regime_data("bull_market", n_points=200)

        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:290:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627B58CD0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpyoextjou'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
______________________________________ TestPrediction.test_prediction_edge_cases ______________________________________

self = <test_regime_forecaster.TestPrediction object at 0x00000266222EEAD0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp7_xonwiq'

    @pytest.mark.asyncio
    async def test_prediction_edge_cases(self, temp_dir):
        """Test prediction with edge cases."""
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:311:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x000002662795FEE0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp7_xonwiq'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
________________________________________ TestPrediction.test_forecast_horizon _________________________________________

self = <test_regime_forecaster.TestPrediction object at 0x000002662231C640>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026627417760>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp64voycta'

    @pytest.mark.asyncio
    async def test_forecast_horizon(self, generate_regime_data, temp_dir):
        """Test forecast horizon functionality."""
        data = generate_regime_data("bull_market", n_points=100)

        config = {"model_path": temp_dir, "forecast_horizon": 48}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:336:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x000002662731C490>
config = {'forecast_horizon': 48, 'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp64voycta'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_______________________ TestIntegrationWithStrategySelector.test_strategy_selector_integration ________________________

self = <test_regime_forecaster.TestIntegrationWithStrategySelector object at 0x000002662231D120>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626755A20>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp8lzdrxjc'

    @pytest.mark.asyncio
    async def test_strategy_selector_integration(self, generate_regime_data, temp_dir):
        """Test integration with strategy selector."""
        # Setup forecaster
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:353:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627A22B90>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp8lzdrxjc'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_____________________ TestIntegrationWithStrategySelector.test_forecast_driven_strategy_selection _____________________

self = <test_regime_forecaster.TestIntegrationWithStrategySelector object at 0x000002662231D450>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FEBAC0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp0u6uy09r'

    @pytest.mark.asyncio
    async def test_forecast_driven_strategy_selection(self, generate_regime_data, temp_dir):
        """Test forecast-driven strategy selection."""
        # Setup forecaster
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:382:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x000002662756CEE0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp0u6uy09r'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_____________________________________ TestPerformance.test_prediction_performance _____________________________________

self = <test_regime_forecaster.TestPerformance object at 0x000002662231D960>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FEA7A0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpdbd3tqyk'
performance_timer = <conftest.performance_timer.<locals>.PerformanceTimer object at 0x00000266279E71C0>

    @pytest.mark.asyncio
    async def test_prediction_performance(self, generate_regime_data, temp_dir, performance_timer):
        """Test prediction performance."""
        data = generate_regime_data("bull_market", n_points=100)

        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:418:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266279E4F10>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpdbd3tqyk'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
__________________________________________ TestPerformance.test_memory_usage __________________________________________

self = <test_regime_forecaster.TestPerformance object at 0x000002662231DCC0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FE9480>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmph0qrg3c1'
memory_monitor = <conftest.memory_monitor.<locals>.MemoryMonitor object at 0x0000026627593910>

    @pytest.mark.asyncio
    async def test_memory_usage(self, generate_regime_data, temp_dir, memory_monitor):
        """Test memory usage during prediction."""
        data = generate_regime_data("bull_market", n_points=200)

        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:441:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627590130>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmph0qrg3c1'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_____________________________________ TestPerformance.test_concurrent_predictions _____________________________________

self = <test_regime_forecaster.TestPerformance object at 0x000002662231DFC0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x000002661D0F6E60>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpo3a3yb54'

    @pytest.mark.asyncio
    async def test_concurrent_predictions(self, generate_regime_data, temp_dir):
        """Test concurrent prediction handling."""
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:459:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627AFD390>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpo3a3yb54'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_________________________________________ TestModelUpdates.test_model_update __________________________________________

self = <test_regime_forecaster.TestModelUpdates object at 0x000002662231E4A0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x0000026626FE85E0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp8aem8qmc'

    @pytest.mark.asyncio
    async def test_model_update(self, generate_regime_data, temp_dir):
        """Test model update with new data."""
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:489:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627AB2AD0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp8aem8qmc'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_______________________________________ TestModelUpdates.test_model_versioning ________________________________________

self = <test_regime_forecaster.TestModelUpdates object at 0x000002662231E7D0>
generate_regime_data = <function generate_regime_data.<locals>._generate_regime_data at 0x000002661D0F5510>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpt6qf2dr6'

    @pytest.mark.asyncio
    async def test_model_versioning(self, generate_regime_data, temp_dir):
        """Test model versioning."""
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:515:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627B5B7F0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpt6qf2dr6'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
________________________________ TestErrorHandling.test_prediction_with_corrupted_data ________________________________

self = <test_regime_forecaster.TestErrorHandling object at 0x000002662231DB10>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpb2epj8jd'

    @pytest.mark.asyncio
    async def test_prediction_with_corrupted_data(self, temp_dir):
        """Test prediction with corrupted data."""
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:541:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627A832E0>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpb2epj8jd'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
____________________________________ TestErrorHandling.test_model_loading_failure _____________________________________

self = <test_regime_forecaster.TestErrorHandling object at 0x000002662231D4B0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpbcyv_z0t'

    @pytest.mark.asyncio
    async def test_model_loading_failure(self, temp_dir):
        """Test handling of model loading failures."""
        config = {"model_path": "/nonexistent/path"}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:561:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x00000266273447F0>
config = {'model_path': '/nonexistent/path'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_____________________________________ TestErrorHandling.test_empty_training_data ______________________________________

self = <test_regime_forecaster.TestErrorHandling object at 0x000002662231C880>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpp8wwttin'

    @pytest.mark.asyncio
    async def test_empty_training_data(self, temp_dir):
        """Test handling of empty training data."""
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:571:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627883670>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpp8wwttin'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
________________________________ TestErrorHandling.test_insufficient_data_for_training ________________________________

self = <test_regime_forecaster.TestErrorHandling object at 0x000002662231EAD0>
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpduf1o7al'

    @pytest.mark.asyncio
    async def test_insufficient_data_for_training(self, temp_dir):
        """Test handling of insufficient data for training."""
        config = {"model_path": temp_dir}
>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:585:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627B88100>
config = {'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpduf1o7al'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'

strategies\regime\regime_forecaster.py:580: TypeError
_________________________________ TestHealthMonitoring.test_health_check_integration __________________________________

self = <test_regime_forecaster.TestHealthMonitoring object at 0x000002662231F010>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpoj9zalyh'

    @pytest.mark.asyncio
    async def test_health_check_integration(self, test_config, temp_dir):
        """Test integration with health monitoring system."""
        from core.diagnostics import get_diagnostics_manager

        config = test_config.get("regime_forecasting", {})
        config["model_path"] = temp_dir

>       forecaster = RegimeForecaster(config)

tests\test_regime_forecaster.py:612:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <strategies.regime.regime_forecaster.RegimeForecaster object at 0x0000026627344FA0>
config = {'confidence_threshold': 0.7, 'enabled': True, 'forecast_horizon': 24, 'model_path': 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpoj9zalyh'}

    def __init__(self, config: Optional[Dict[str, Any]] = None):
>       self.config = ForecastingConfig(**(config or {}))
E       TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'enabled'

strategies\regime\regime_forecaster.py:580: TypeError
___________________________________ test_botengine_respects_order_manager_safe_mode ___________________________________

tmp_path = WindowsPath('C:/Users/TU/AppData/Local/Temp/pytest-of-TU/pytest-229/test_botengine_respects_order_0')

    @pytest.mark.asyncio
    async def test_botengine_respects_order_manager_safe_mode(tmp_path):
        """
        If OrderManager.safe_mode_active is True, BotEngine should skip trading cycles
        and not attempt to execute orders.
        """
        config = {
            "environment": {"mode": "paper"},
            "exchange": {"base_currency": "USDT", "markets": ["BTC/USDT"]},
            "trading": {"initial_balance": 1000.0},
            "risk_management": {},
            "notifications": {"discord": {"enabled": False}},
            "monitoring": {"terminal_display": False, "update_interval": 1},
            "strategies": {"active_strategies": [], "strategy_config": {}},
        }

        bot = BotEngine(config)
        # minimal initialization state
        bot.data_fetcher = AsyncMock()
        bot.strategies = []
        # OrderManager stub with safe_mode_active True
        class OMStub:
            safe_mode_active = True
            async def get_balance(self): return 1000
            async def get_equity(self): return 1000
            async def get_active_order_count(self): return 0
            async def get_open_position_count(self): return 0
        bot.order_manager = OMStub()

        # Risk manager stub
        class RMStub:
            async def evaluate_signal(self, *args, **kwargs): return True
        bot.risk_manager = RMStub()

        # Run one trading cycle; it should return early due to safe mode and not raise
>       await bot._trading_cycle()

tests\test_safe_mode.py:41:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\bot_engine.py:281: in _trading_cycle
    market_data = await self._fetch_market_data()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.bot_engine.BotEngine object at 0x00000266277DE920>

    async def _fetch_market_data(self) -> Dict[str, Any]:
        """Fetch market data with caching and multi-timeframe support."""
>       current_time = time.time()
E       NameError: name 'time' is not defined

core\bot_engine.py:301: NameError
_________________________________ TestFailureDetector.test_anomaly_detection_latency __________________________________

self = <test_self_healing_engine.TestFailureDetector object at 0x00000266223C9300>

    def test_anomaly_detection_latency(self):
        """Test latency anomaly detection."""
        detector = FailureDetector({"anomaly_threshold": 2.0})

        # Create normal heartbeats
        for i in range(10):
            heartbeat = HeartbeatMessage(
                component_id="test_comp",
                component_type="test_type",
                version="1.0",
                timestamp=datetime.now(),
                status=ComponentStatus.HEALTHY,
                latency_ms=50.0  # Normal latency
            )
            detector.process_heartbeat(heartbeat)

        # Create anomalous heartbeat
        anomalous_heartbeat = HeartbeatMessage(
            component_id="test_comp",
            component_type="test_type",
            version="1.0",
            timestamp=datetime.now(),
            status=ComponentStatus.HEALTHY,
            latency_ms=200.0  # High latency (anomaly)
        )

        diagnosis = detector.process_heartbeat(anomalous_heartbeat)

        # Should detect anomaly
        assert diagnosis is not None
        assert diagnosis.failure_type == FailureType.PERFORMANCE
>       assert diagnosis.severity == FailureSeverity.MEDIUM
E       AssertionError: assert <FailureSeverity.HIGH: 'high'> == <FailureSeverity.MEDIUM: 'medium'>
E        +  where <FailureSeverity.HIGH: 'high'> = FailureDiagnosis(component_id='test_comp', failure_type=<FailureType.PERFORMANCE: 'performance'>, severity=<FailureSev...y'], estimated_recovery_time=60, dependencies_affected=[], timestamp=datetime.datetime(2025, 9, 8, 16, 22, 30, 237674)).severity
E        +  and   <FailureSeverity.MEDIUM: 'medium'> = FailureSeverity.MEDIUM

tests\test_self_healing_engine.py:293: AssertionError
__________________________________ TestRecoveryOrchestrator.test_recovery_initiation __________________________________

self = <test_self_healing_engine.TestRecoveryOrchestrator object at 0x00000266223C9D50>

    @pytest.mark.asyncio
    async def test_recovery_initiation(self):
        """Test recovery action initiation."""
        registry = ComponentRegistry()
        orchestrator = HealingOrchestrator({}, registry)

        # Mock component
        mock_component = Mock()
        registry.register_component("test_comp", ComponentType.BOT_ENGINE, mock_component)

        # Mock diagnosis
        diagnosis = Mock()
        diagnosis.component_id = "test_comp"
        diagnosis.failure_type = FailureType.CONNECTIVITY
        diagnosis.severity = FailureSeverity.HIGH
        diagnosis.estimated_recovery_time = 60

        action = await orchestrator.initiate_healing("test_comp", FailureType.CONNECTIVITY,
                                                   FailureSeverity.HIGH, diagnosis.to_dict())

        assert action is not None
        assert action.component_id == "test_comp"
>       assert action.action_type == "test_recovery_connectivity"
E       AssertionError: assert '_recover_bot_engine' == 'test_recovery_connectivity'
E
E         - test_recovery_connectivity
E         + _recover_bot_engine

tests\test_self_healing_engine.py:372: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.self_healing_engine:Bot engine recovery failed: object Mock can't be used in 'await' expression
ERROR:core.self_healing_engine:Healing failed for test_comp
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.self_healing_engine:self_healing_engine.py:371 Bot engine recovery failed: object Mock can't be used in 'await' expression
ERROR    core.self_healing_engine:self_healing_engine.py:310 Healing failed for test_comp
_________________________________ TestRecoveryOrchestrator.test_priority_calculation __________________________________

self = <test_self_healing_engine.TestRecoveryOrchestrator object at 0x00000266223C9FF0>

    def test_priority_calculation(self):
        """Test recovery priority calculation."""
        orchestrator = HealingOrchestrator({}, ComponentRegistry())

        # Test different severity levels
>       assert orchestrator._calculate_priority(FailureSeverity.CRITICAL) == 10
E       AttributeError: 'HealingOrchestrator' object has no attribute '_calculate_priority'

tests\test_self_healing_engine.py:380: AttributeError
__________________________________ TestRecoveryOrchestrator.test_timeout_calculation __________________________________

self = <test_self_healing_engine.TestRecoveryOrchestrator object at 0x00000266223C9A20>

    def test_timeout_calculation(self):
        """Test recovery timeout calculation."""
        orchestrator = HealingOrchestrator({}, ComponentRegistry())

        # Test different combinations
        timeout = orchestrator._calculate_timeout(FailureSeverity.CRITICAL, FailureType.CONNECTIVITY)
>       assert timeout == 60  # Reduced for connectivity
E       assert 90 == 60

tests\test_self_healing_engine.py:391: AssertionError
________________________________ TestWatchdogService.test_overdue_heartbeat_detection _________________________________

self = <test_self_healing_engine.TestWatchdogService object at 0x00000266223CB490>

    @pytest.mark.asyncio
    async def test_overdue_heartbeat_detection(self):
        """Test overdue heartbeat detection."""
        watchdog = WatchdogService({})

        # Register component with short interval
        protocol = watchdog.register_component("test_comp", "test_type", 1)

        # Wait for overdue
        await asyncio.sleep(2)

        # Check for overdue heartbeats
        await watchdog._check_overdue_heartbeats()

        # Should have detected failure
>       assert watchdog.failures_detected > 0
E       assert 0 > 0
E        +  where 0 = <core.watchdog.WatchdogService object at 0x000002662792A080>.failures_detected

tests\test_self_healing_engine.py:534: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:core.watchdog:Overdue heartbeat detected for test_comp
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  core.watchdog:watchdog.py:820 Overdue heartbeat detected for test_comp
_______________________________ TestMonitoringDashboard.test_system_health_calculation ________________________________

self = <test_self_healing_engine.TestMonitoringDashboard object at 0x00000266223EC280>

    def test_system_health_calculation(self):
        """Test system health calculation."""
        registry = ComponentRegistry()
        watchdog = WatchdogService({})
        dashboard = MonitoringDashboard({}, registry, watchdog)

        # Mock registry stats
        registry.get_registry_stats = Mock(return_value={
            'total_components': 10,
            'healthy_components': 8,
            'failing_components': 2
        })

        # Mock watchdog stats
        watchdog.get_watchdog_stats = Mock(return_value={
            'heartbeats_received': 100,
            'failures_detected': 2
        })

        data = dashboard.get_dashboard_data()
        system_health = data['system_health']

>       assert system_health['overall_health'] == "DEGRADED"  # 20% failing
E       AssertionError: assert 'HEALTHY' == 'DEGRADED'
E
E         - DEGRADED
E         + HEALTHY

tests\test_self_healing_engine.py:657: AssertionError
___________________________________ TestMonitoringDashboard.test_failure_statistics ___________________________________

self = <test_self_healing_engine.TestMonitoringDashboard object at 0x00000266223EC6A0>

    def test_failure_statistics(self):
        """Test failure statistics calculation."""
        registry = ComponentRegistry()
        watchdog = WatchdogService({})
        dashboard = MonitoringDashboard({}, registry, watchdog)

        # Mock watchdog stats
        watchdog.get_watchdog_stats = Mock(return_value={
            'failures_detected': 5,
            'recoveries_initiated': 5,
            'recoveries_successful': 4
        })

        data = dashboard.get_dashboard_data()
        failure_stats = data['failure_stats']

>       assert failure_stats['total_failures'] == 5
E       assert 0 == 5

tests\test_self_healing_engine.py:693: AssertionError
________________________________ TestSelfHealingEngine.test_component_status_retrieval ________________________________

self = <test_self_healing_engine.TestSelfHealingEngine object at 0x00000266223ED360>

    def test_component_status_retrieval(self):
        """Test component status retrieval."""
        engine = SelfHealingEngine({})

        # Register component
        mock_component = Mock()
        engine.register_component("test_comp", ComponentType.BOT_ENGINE, mock_component)

>       status = engine.get_component_status("test_comp")

tests\test_self_healing_engine.py:780:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.self_healing_engine.SelfHealingEngine object at 0x0000026627AD3AC0>, component_id = 'test_comp'

    def get_component_status(self, component_id: str) -> Optional[Dict[str, Any]]:
        """Get status information for a specific component."""
        comp_info = self.component_registry.get_component(component_id)
        if not comp_info:
            return None

        watchdog_status = self.watchdog_service.get_component_status(component_id)

        return {
            'component_id': component_id,
            'component_type': comp_info.component_type.value,
            'critical': comp_info.critical,
            'consecutive_failures': comp_info.consecutive_failures,
            'last_health_check': comp_info.last_health_check.isoformat() if comp_info.last_health_check else None,
            'heartbeat_overdue': watchdog_status.get('is_overdue', True) if watchdog_status else True,
>           'last_heartbeat': watchdog_status.get('last_heartbeat', {}).get('timestamp') if watchdog_status else None,
            'dependencies': comp_info.dependencies,
            'recovery_priority': comp_info.recovery_priority
        }
E       AttributeError: 'NoneType' object has no attribute 'get'

core\self_healing_engine.py:1030: AttributeError
________________________________ TestIntegrationWithN1V1.test_diagnostics_integration _________________________________

self = <test_self_healing_engine.TestIntegrationWithN1V1 object at 0x00000266223EC8E0>

    @pytest.mark.asyncio
    async def test_diagnostics_integration(self):
        """Test integration with diagnostics system."""
        from core.diagnostics import get_diagnostics_manager

        engine = SelfHealingEngine({})
        diagnostics = get_diagnostics_manager()

        # Register health check
        async def check_self_healing_engine():
            stats = engine.get_engine_stats()
            total_components = stats['registry_stats']['total_components']

            status = HealthStatus.HEALTHY if total_components >= 0 else HealthStatus.DEGRADED

            return {
                'component': 'self_healing_engine',
                'status': status,
                'latency_ms': 10.0,
                'message': f'Engine healthy: {total_components} components monitored',
                'details': {'monitored_components': total_components}
            }

        diagnostics.register_health_check('self_healing_engine', check_self_healing_engine)

        # Run health check
>       state = await diagnostics.run_health_check()

tests\test_self_healing_engine.py:849:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\diagnostics.py:183: in run_health_check
    self.state.overall_status = self._calculate_overall_status(results)
core\diagnostics.py:202: in _calculate_overall_status
    if any(r.status == HealthStatus.CRITICAL for r in results):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x000002662794A440>

>   if any(r.status == HealthStatus.CRITICAL for r in results):
E   AttributeError: 'dict' object has no attribute 'status'

core\diagnostics.py:202: AttributeError
_________________________________ TestIntegrationWithN1V1.test_event_bus_integration __________________________________

self = <test_self_healing_engine.TestIntegrationWithN1V1 object at 0x00000266223ED660>

    @pytest.mark.asyncio
    async def test_event_bus_integration(self):
        """Test integration with event bus."""
>       from core.signal_router.events import get_default_enhanced_event_bus
E       ImportError: cannot import name 'get_default_enhanced_event_bus' from 'core.signal_router.events' (C:\Users\TU\Desktop\new project\N1V1\core\signal_router\events.py)

tests\test_self_healing_engine.py:857: ImportError
________________________________ TestPerformance.test_heartbeat_processing_performance ________________________________

self = <test_self_healing_engine.TestPerformance object at 0x00000266223EDAE0>
performance_timer = <conftest.performance_timer.<locals>.PerformanceTimer object at 0x000002662794A140>

    @pytest.mark.asyncio
    async def test_heartbeat_processing_performance(self, performance_timer):
        """Test heartbeat processing performance."""
        engine = SelfHealingEngine({})

        # Register component
        mock_component = Mock()
        engine.register_component("test_comp", ComponentType.BOT_ENGINE, mock_component)

        # Measure heartbeat processing time
        performance_timer.start()

        for i in range(100):
            await engine.send_heartbeat(
                component_id="test_comp",
                status=ComponentStatus.HEALTHY,
                latency_ms=50.0 + i * 0.1,
                error_count=0
            )

        performance_timer.stop()

        duration_ms = performance_timer.duration_ms()

        # Should process 100 heartbeats quickly
>       assert duration_ms < 1000  # Less than 1 second total
E       assert 11611.062 < 1000

tests\test_self_healing_engine.py:906: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
________________________________ TestPerformance.test_concurrent_heartbeat_processing _________________________________

self = <test_self_healing_engine.TestPerformance object at 0x00000266223EDD80>

    @pytest.mark.asyncio
    async def test_concurrent_heartbeat_processing(self):
        """Test concurrent heartbeat processing."""
        engine = SelfHealingEngine({})

        # Register multiple components
        for i in range(10):
            mock_component = Mock()
            engine.register_component(f"comp_{i}", ComponentType.STRATEGY, mock_component)

        # Send heartbeats concurrently
        tasks = []
        for i in range(10):
            task = engine.send_heartbeat(
                component_id=f"comp_{i}",
                status=ComponentStatus.HEALTHY,
                latency_ms=50.0,
                error_count=0
            )
            tasks.append(task)

        await asyncio.gather(*tasks)

        # All heartbeats should be processed
>       assert engine.watchdog_service.heartbeats_received == 10
E       assert 2167 == 10
E        +  where 2167 = <core.watchdog.WatchdogService object at 0x000002662694E8C0>.heartbeats_received
E        +    where <core.watchdog.WatchdogService object at 0x000002662694E8C0> = <core.self_healing_engine.SelfHealingEngine object at 0x0000026627A9CB80>.watchdog_service

tests\test_self_healing_engine.py:932: AssertionError
_____________________________________ TestReliability.test_long_running_stability _____________________________________

self = <test_self_healing_engine.TestReliability object at 0x00000266223EE5F0>

    @pytest.mark.asyncio
    async def test_long_running_stability(self):
        """Test long-running stability."""
        engine = SelfHealingEngine({})

        # Register component
        mock_component = Mock()
        engine.register_component("test_comp", ComponentType.BOT_ENGINE, mock_component)

        # Send heartbeats for extended period
        start_time = time.time()

        for i in range(100):
            await engine.send_heartbeat(
                component_id="test_comp",
                status=ComponentStatus.HEALTHY,
                latency_ms=50.0,
                error_count=0
            )
            await asyncio.sleep(0.01)  # Small delay

        duration = time.time() - start_time

        # Should complete within reasonable time
>       assert duration < 5  # Less than 5 seconds for 100 heartbeats
E       assert 13.831456184387207 < 5

tests\test_self_healing_engine.py:1058: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING:core.watchdog:Failure detected: test_comp - Memory leak in component or high memory pressure
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
WARNING  core.watchdog:watchdog.py:773 Failure detected: test_comp - Memory leak in component or high memory pressure
____________________________________________ test_retry_async_call_success ____________________________________________

mock_sleep = <AsyncMock name='sleep' id='2637776203360'>

    @pytest.mark.asyncio
    @patch("asyncio.sleep", new_callable=AsyncMock)
    async def test_retry_async_call_success(mock_sleep):
        """Test successful retry of async call."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:270:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627A82320>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_________________________________________ test_retry_async_call_with_failure __________________________________________

mock_sleep = <AsyncMock name='sleep' id='2637767472272'>

    @pytest.mark.asyncio
    @patch("asyncio.sleep", new_callable=AsyncMock)
    async def test_retry_async_call_with_failure(mock_sleep):
        """Test retry mechanism with temporary failure."""
        rm = FailingRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:285:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627314460>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
________________________________________ test_retry_async_call_exhaust_retries ________________________________________

mock_sleep = <AsyncMock name='sleep' id='2637776586448'>

    @pytest.mark.asyncio
    @patch("asyncio.sleep", new_callable=AsyncMock)
    async def test_retry_async_call_exhaust_retries(mock_sleep):
        """Test retry exhaustion."""
        rm = FailingRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:301:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627BCA530>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
___________________________________ test_concurrent_signal_processing_no_conflicts ____________________________________

    @pytest.mark.asyncio
    async def test_concurrent_signal_processing_no_conflicts():
        """
        Spawn many concurrent signals for distinct symbols and ensure all are approved
        and stored in active_signals without loss or duplication.
        """
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:332:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627B70100>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_________________________________________ test_conflicting_signals_resolution _________________________________________

    @pytest.mark.asyncio
    async def test_conflicting_signals_resolution():
        """
        Submit two conflicting signals for the same symbol concurrently (opposite directions)
        and verify conflict resolution keeps the stronger signal.
        """
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:362:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627B6D780>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
__________________________________________ test_signal_router_init_defaults ___________________________________________

dummy_risk_manager = <test_signal_router.DummyRiskManager object at 0x0000026627B91180>

    def test_signal_router_init_defaults(dummy_risk_manager):
        """Test SignalRouter initialization with default parameters."""
>       router = SignalRouter(risk_manager=dummy_risk_manager)

tests\test_signal_router.py:420:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627B905E0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_ml_confirmation_weak_signal_approved ______________________________________

    @pytest.mark.asyncio
    async def test_ml_confirmation_weak_signal_approved():
        """Test that ML confirmation approves a weak signal when ML predicts same direction with high confidence."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength
        from core.signal_router import SignalRouter

        # Create a mock ML model and prediction function
        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": 1, "confidence": 0.8}])  # Same direction, high confidence

        def mock_predict(model, features_df):
            return mock_prediction_df

        # Create router with ML enabled
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:463:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627961F30>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_ml_confirmation_weak_signal_rejected ______________________________________

    @pytest.mark.asyncio
    async def test_ml_confirmation_weak_signal_rejected():
        """Test that ML confirmation rejects a weak signal when ML predicts opposite direction with high confidence."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength
        from core.signal_router import SignalRouter

        # Create a mock ML model and prediction function
        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": -1, "confidence": 0.8}])  # Opposite direction, high confidence

        def mock_predict(model, features_df):
            return mock_prediction_df

        # Create router with ML enabled
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:532:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x000002662739D060>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________________ test_ml_confirmation_strong_signal_bypassed _____________________________________

    @pytest.mark.asyncio
    async def test_ml_confirmation_strong_signal_bypassed():
        """Test that strong signals bypass ML confirmation even when ML disagrees."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength
        from core.signal_router import SignalRouter

        # Create a mock ML model and prediction function
        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": -1, "confidence": 0.8}])  # Opposite direction, high confidence

        def mock_predict(model, features_df):
            return mock_prediction_df

        # Create router with ML enabled
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:590:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627968580>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
___________________________________ test_ml_predict_called_with_dataframe_features ____________________________________

    @pytest.mark.asyncio
    async def test_ml_predict_called_with_dataframe_features():
        """Test that ml_predict is called when features are provided as DataFrame."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength

        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": 1, "confidence": 0.8}])

        def mock_predict(model, features_df):
            return mock_prediction_df

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:647:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266272D8A30>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_ml_predict_called_with_dict_features ______________________________________

    @pytest.mark.asyncio
    async def test_ml_predict_called_with_dict_features():
        """Test that ml_predict is called when features are provided as dict."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength

        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": 1, "confidence": 0.8}])

        def mock_predict(model, features_df):
            return mock_prediction_df

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:691:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x000002662781F640>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________________ test_ml_predict_called_with_series_features _____________________________________

    @pytest.mark.asyncio
    async def test_ml_predict_called_with_series_features():
        """Test that ml_predict is called when features are provided as pd.Series."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength

        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": 1, "confidence": 0.8}])

        def mock_predict(model, features_df):
            return mock_prediction_df

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:735:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627883DC0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
___________________________________ test_ml_predict_not_called_with_empty_dataframe ___________________________________

    @pytest.mark.asyncio
    async def test_ml_predict_not_called_with_empty_dataframe():
        """Test that ml_predict is NOT called when features DataFrame is empty."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength

        mock_model = MagicMock()

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:775:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627B2B550>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_______________________________ test_ml_predict_called_with_metadata_features_fallback ________________________________

    @pytest.mark.asyncio
    async def test_ml_predict_called_with_metadata_features_fallback():
        """Test that ml_predict is called when features are in signal metadata."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength

        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": 1, "confidence": 0.8}])

        def mock_predict(model, features_df):
            return mock_prediction_df

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:814:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627A22770>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________________ test_ml_predict_called_with_ohlcv_fallback ______________________________________

    @pytest.mark.asyncio
    async def test_ml_predict_called_with_ohlcv_fallback():
        """Test that ml_predict is called when ohlcv data is provided as fallback."""
        import pandas as pd
        from unittest.mock import patch, MagicMock
        from decimal import Decimal
        from core.contracts import TradingSignal, SignalType, SignalStrength

        mock_model = MagicMock()
        mock_prediction_df = pd.DataFrame([{"prediction": 1, "confidence": 0.8}])

        def mock_predict(model, features_df):
            return mock_prediction_df

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:856:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x000002662789CFD0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
__________________________________________ test_get_symbol_lock_concurrency ___________________________________________

    @pytest.mark.asyncio
    async def test_get_symbol_lock_concurrency():
        """Test that symbol locks are distinct per symbol and global lock for None symbol."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:889:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266279D9C90>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_________________________________ test_concurrent_signal_processing_different_symbols _________________________________

    @pytest.mark.asyncio
    async def test_concurrent_signal_processing_different_symbols():
        """Test concurrent processing of signals for different symbols."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:925:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627990EB0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_______________________________ test_concurrent_signal_processing_same_symbol_conflicts _______________________________

    @pytest.mark.asyncio
    async def test_concurrent_signal_processing_same_symbol_conflicts():
        """Test concurrent processing of conflicting signals for the same symbol."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:962:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266279B3D60>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_retry_async_call_with_custom_backoff ______________________________________

    @pytest.mark.asyncio
    async def test_retry_async_call_with_custom_backoff():
        """Test _retry_async_call with custom backoff parameters."""
        from unittest.mock import patch, AsyncMock

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1402:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266273ACCA0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
________________________________________ test_retry_async_call_max_backoff_cap ________________________________________

    @pytest.mark.asyncio
    async def test_retry_async_call_max_backoff_cap():
        """Test that retry backoff is capped at max_backoff."""
        import time
        from unittest.mock import patch

        class PersistentFailureManager:
            def __init__(self):
                self.call_count = 0

            async def evaluate_signal(self, signal, market_data=None):
                self.call_count += 1
                raise Exception("Persistent failure")

        rm = PersistentFailureManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1077:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627AB4190>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________ test_journal_recovery_with_corrupted_entries _____________________________________

    @pytest.mark.asyncio
    async def test_journal_recovery_with_corrupted_entries():
        """Test journal recovery handles corrupted/malformed entries gracefully."""
        import tempfile
        import os
        from pathlib import Path

        rm = DummyRiskManager()
        with tempfile.TemporaryDirectory() as temp_dir:
            journal_path = Path(temp_dir) / "test_journal.jsonl"

            # Create journal with some corrupted entries
            with open(journal_path, "w") as f:
                f.write('{"action": "store", "id": "test_1", "signal": {"strategy_id": "test"}}\n')  # Valid
                f.write('{"action": "store", "id": "test_2"}\n')  # Missing signal
                f.write('invalid json line\n')  # Invalid JSON
                f.write('{"action": "store", "id": "test_3", "signal": {}}\n')  # Empty signal
                f.write('{"action": "cancel", "id": "test_1"}\n')  # Valid cancel

>           router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1125:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266275C16F0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________________ test_journal_recovery_signal_reconstruction _____________________________________

    @pytest.mark.asyncio
    async def test_journal_recovery_signal_reconstruction():
        """Test journal recovery properly reconstructs TradingSignal objects."""
        import tempfile
        import json
        from pathlib import Path

        rm = DummyRiskManager()
        with tempfile.TemporaryDirectory() as temp_dir:
            journal_path = Path(temp_dir) / "test_journal.jsonl"

            # Create a comprehensive signal entry
            signal_data = {
                "strategy_id": "test_strategy",
                "symbol": "BTC/USDT",
                "signal_type": "ENTRY_LONG",
                "signal_strength": "STRONG",
                "order_type": "market",
                "amount": "1.5",
                "price": "50000.0",
                "current_price": "50100.0",
                "timestamp": 1234567890,
                "stop_loss": "49000.0",
                "take_profit": "52000.0",
                "metadata": {"test_key": "test_value"}
            }

            with open(journal_path, "w") as f:
                json.dump({
                    "action": "store",
                    "id": "test_signal_1234567890_BTC/USDT",
                    "timestamp": 1234567890,
                    "signal": signal_data
                }, f)
                f.write('\n')

>           router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1174:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266272D0130>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
__________________________________ test_journal_recovery_enum_conversion_edge_cases ___________________________________

    @pytest.mark.asyncio
    async def test_journal_recovery_enum_conversion_edge_cases():
        """Test journal recovery handles enum conversion edge cases."""
        import tempfile
        import json
        from pathlib import Path

        rm = DummyRiskManager()
        with tempfile.TemporaryDirectory() as temp_dir:
            journal_path = Path(temp_dir) / "test_journal.jsonl"

            # Test various enum representations
            test_cases = [
                {"signal_type": 1, "signal_strength": 3},  # Integer values
                {"signal_type": "ENTRY_SHORT", "signal_strength": "WEAK"},  # String names
                {"signal_type": "invalid", "signal_strength": 99},  # Invalid values
            ]

            with open(journal_path, "w") as f:
                for i, case in enumerate(test_cases):
                    signal_data = {
                        "strategy_id": f"test_{i}",
                        "symbol": f"BTC{i}/USDT",
                        "order_type": "market",
                        "amount": "1.0",
                        "timestamp": 1234567890 + i,
                        **case
                    }
                    json.dump({
                        "action": "store",
                        "id": f"test_{i}_1234567890_BTC{i}/USDT",
                        "signal": signal_data
                    }, f)
                    f.write('\n')

>           router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1229:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627321000>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
__________________________________________ test_extract_features_edge_cases ___________________________________________

    @pytest.mark.asyncio
    async def test_extract_features_edge_cases():
        """Test _extract_features_for_ml handles various edge cases."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1254:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627947CD0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________________ test_extract_features_fallback_to_metadata ______________________________________

    @pytest.mark.asyncio
    async def test_extract_features_fallback_to_metadata():
        """Test _extract_features_for_ml falls back to signal metadata."""
        import pandas as pd

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1301:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627882770>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_extract_features_multiple_fallbacks _______________________________________

    @pytest.mark.asyncio
    async def test_extract_features_multiple_fallbacks():
        """Test _extract_features_for_ml tries multiple fallback locations."""
        import pandas as pd

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1326:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266279DFDF0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
________________________________ test_retry_async_call_with_different_exception_types _________________________________

    @pytest.mark.asyncio
    async def test_retry_async_call_with_different_exception_types():
        """Test _retry_async_call handles different exception types appropriately."""
        from unittest.mock import patch

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1361:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627B6F370>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_________________________________________ test_retry_async_call_zero_retries __________________________________________

    @pytest.mark.asyncio
    async def test_retry_async_call_zero_retries():
        """Test _retry_async_call with zero retries."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1384:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266277DC940>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________ test_process_signal_ml_confirmation_disabled _____________________________________

    @pytest.mark.asyncio
    async def test_process_signal_ml_confirmation_disabled():
        """Test process_signal when ML is disabled."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1437:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266277EF370>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
__________________________________________ test_process_signal_ml_model_none __________________________________________

    @pytest.mark.asyncio
    async def test_process_signal_ml_model_none():
        """Test process_signal when ML model is None."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1460:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627CECB50>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_process_signal_non_entry_signal_type ______________________________________

    @pytest.mark.asyncio
    async def test_process_signal_non_entry_signal_type():
        """Test process_signal with non-entry signal types (should skip ML)."""
        import pandas as pd
        from unittest.mock import patch

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1487:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266279A1270>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
__________________________________ test_process_signal_ml_prediction_error_handling ___________________________________

    @pytest.mark.asyncio
    async def test_process_signal_ml_prediction_error_handling():
        """Test process_signal handles ML prediction errors gracefully."""
        import pandas as pd
        from unittest.mock import patch

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1522:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266272ED330>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________ test_process_signal_ml_low_confidence_ignored ____________________________________

    @pytest.mark.asyncio
    async def test_process_signal_ml_low_confidence_ignored():
        """Test process_signal ignores ML predictions with low confidence."""
        import pandas as pd
        from unittest.mock import patch, MagicMock

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1558:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627332980>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
__________________________________________ test_process_signal_blocking_mode __________________________________________

    @pytest.mark.asyncio
    async def test_process_signal_blocking_mode():
        """Test process_signal when router is in blocking mode."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1598:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627878190>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_process_signal_risk_manager_failure _______________________________________

    @pytest.mark.asyncio
    async def test_process_signal_risk_manager_failure():
        """Test process_signal handles risk manager failures."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1621:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266272D38E0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_______________________________________ test_process_signal_validation_failure ________________________________________

    @pytest.mark.asyncio
    async def test_process_signal_validation_failure():
        """Test process_signal handles validation failures."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1647:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x000002662788D660>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________________ test_record_router_error_threshold_trigger ______________________________________

    @pytest.mark.asyncio
    async def test_record_router_error_threshold_trigger():
        """Test _record_router_error triggers blocking when threshold exceeded."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm, safe_mode_threshold=3)

tests\test_signal_router.py:1669:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627BCA7A0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_record_router_error_context_logging _______________________________________

    @pytest.mark.asyncio
    async def test_record_router_error_context_logging():
        """Test _record_router_error logs context information."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1683:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266275C2620>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________ test_update_signal_status_nonexistent_signal _____________________________________

    @pytest.mark.asyncio
    async def test_update_signal_status_nonexistent_signal():
        """Test update_signal_status with nonexistent signal ID."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1697:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627382A40>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________________ test_get_active_signals_empty ____________________________________________

    @pytest.mark.asyncio
    async def test_get_active_signals_empty():
        """Test get_active_signals when no signals are active."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1722:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627323E20>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________________ test_get_signal_history_limit ____________________________________________

    @pytest.mark.asyncio
    async def test_get_signal_history_limit():
        """Test get_signal_history respects limit parameter."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1732:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627B68160>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________________ test_close_journal_no_journal ____________________________________________

    @pytest.mark.asyncio
    async def test_close_journal_no_journal():
        """Test close_journal when no journal is configured."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1760:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266279DD300>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
___________________________________________ test_close_journal_with_journal ___________________________________________

    @pytest.mark.asyncio
    async def test_close_journal_with_journal():
        """Test close_journal with active journal."""
        import tempfile
        from pathlib import Path

        rm = DummyRiskManager()
        with tempfile.TemporaryDirectory() as temp_dir:
            journal_path = Path(temp_dir) / "test_journal.jsonl"

>           router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1776:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627901F90>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
__________________________________________ test_get_symbol_lock_none_symbol ___________________________________________

    @pytest.mark.asyncio
    async def test_get_symbol_lock_none_symbol():
        """Test _get_symbol_lock with None symbol returns global lock."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1790:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266274DD8A0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
________________________________________ test_get_symbol_lock_creates_new_lock ________________________________________

    @pytest.mark.asyncio
    async def test_get_symbol_lock_creates_new_lock():
        """Test _get_symbol_lock creates new lock for new symbol."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1800:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627D5C4C0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________ test_process_signal_with_invalid_symbol_lock _____________________________________

    @pytest.mark.asyncio
    async def test_process_signal_with_invalid_symbol_lock():
        """Test process_signal handles invalid symbol gracefully."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1811:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x000002662795C8B0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_______________________________________ test_resolve_conflicts_with_empty_list ________________________________________

    @pytest.mark.asyncio
    async def test_resolve_conflicts_with_empty_list():
        """Test _resolve_conflicts with empty conflicting signals list."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1829:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627AB6740>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
________________________________________ test_resolve_conflicts_strength_based ________________________________________

    @pytest.mark.asyncio
    async def test_resolve_conflicts_strength_based():
        """Test conflict resolution based on strength."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1847:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x000002662781FB80>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_________________________________________ test_resolve_conflicts_newer_first __________________________________________

    @pytest.mark.asyncio
    async def test_resolve_conflicts_newer_first():
        """Test conflict resolution based on timestamp (newer first)."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1880:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266279370A0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_______________________________________ test_store_signal_with_journal_disabled _______________________________________

    @pytest.mark.asyncio
    async def test_store_signal_with_journal_disabled():
        """Test _store_signal when journal is disabled."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1920:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627A9D7E0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
______________________________________ test_cancel_signal_with_journal_disabled _______________________________________

    @pytest.mark.asyncio
    async def test_cancel_signal_with_journal_disabled():
        """Test _cancel_signal when journal is disabled."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1940:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266279C5BA0>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_________________________________________ test_recover_from_journal_disabled __________________________________________

    @pytest.mark.asyncio
    async def test_recover_from_journal_disabled():
        """Test recover_from_journal when disabled."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1962:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x00000266272D1060>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
____________________________________ test_ml_extract_features_with_list_of_scalars ____________________________________

    @pytest.mark.asyncio
    async def test_ml_extract_features_with_list_of_scalars():
        """Test _extract_features_for_ml with list of scalars."""
        import pandas as pd

        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1975:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x000002662756E650>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_____________________________________ test_ml_extract_features_with_invalid_data ______________________________________

    @pytest.mark.asyncio
    async def test_ml_extract_features_with_invalid_data():
        """Test _extract_features_for_ml with invalid data types."""
        rm = DummyRiskManager()
>       router = SignalRouter(risk_manager=rm)

tests\test_signal_router.py:1999:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\signal_router\router.py:228: in __init__
    self.ensemble_manager = EnsembleManager()
core\ensemble_manager.py:92: in __init__
    self._load_from_config()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <core.ensemble_manager.EnsembleManager object at 0x0000026627944A30>

    def _load_from_config(self) -> None:
        """Load strategy configurations from config."""
        strategies_config = self.config.get("strategies", [])
        for strategy_config in strategies_config:
>           strategy_id = strategy_config.get("id")
E           AttributeError: 'str' object has no attribute 'get'

core\ensemble_manager.py:101: AttributeError
_______________________________________ TestStrategyGenome.test_genome_creation _______________________________________

self = <test_strategy_generator.TestStrategyGenome object at 0x00000266224AC9A0>

    def test_genome_creation(self):
        """Test basic genome creation."""
        genome = StrategyGenome()

        assert len(genome.genes) == 0
>       assert genome.fitness == 0.0
E       assert -inf == 0.0
E        +  where -inf = StrategyGenome(genes=[], fitness=-inf, age=0, generation=0, species_id=None, metadata={}).fitness

tests\test_strategy_generator.py:43: AssertionError
____________________________________ TestGeneticOperations.test_mutation_operation ____________________________________

self = <test_strategy_generator.TestGeneticOperations object at 0x00000266224AEBF0>

    def test_mutation_operation(self):
        """Test genome mutation."""
        genome = StrategyGenome()

        # Add initial gene
        gene = StrategyGene(
            component_type=StrategyComponent.INDICATOR,
            indicator_type=IndicatorType.RSI,
            parameters={'period': 14, 'overbought': 70, 'oversold': 30}
        )
        genome.genes.append(gene)

        original_period = gene.parameters['period']

        # Apply mutation
        mutated = genome.mutate(mutation_rate=1.0)  # 100% mutation rate

        # Should have same number of genes
>       assert len(mutated.genes) == len(genome.genes)
E       AttributeError: 'NoneType' object has no attribute 'genes'

tests\test_strategy_generator.py:150: AttributeError
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:optimization.strategy_generator:Genome missing required components (indicator or signal logic)
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  optimization.strategy_generator:strategy_generator.py:157 Genome missing required components (indicator or signal logic)
___________________________________ TestGeneticOperations.test_selection_operation ____________________________________

self = <test_strategy_generator.TestGeneticOperations object at 0x00000266224AF010>

    def test_selection_operation(self):
        """Test population selection."""
        # Create population with different fitness values
        population = []

        for i in range(10):
            genome = StrategyGenome()
            genome.fitness = i * 0.1  # 0.0, 0.1, 0.2, ..., 0.9
            population.append(genome)

        # Select best individuals
>       selected = StrategyGenome.select_best(population, num_to_select=5)
E       AttributeError: type object 'StrategyGenome' has no attribute 'select_best'

tests\test_strategy_generator.py:197: AttributeError
___________________________________ TestGeneticOperations.test_tournament_selection ___________________________________

self = <test_strategy_generator.TestGeneticOperations object at 0x00000266224AF220>

    def test_tournament_selection(self):
        """Test tournament selection."""
        population = []

        for i in range(20):
            genome = StrategyGenome()
            genome.fitness = np.random.random()
            population.append(genome)

        # Perform tournament selection
>       winner = StrategyGenome.tournament_selection(population, tournament_size=5)
E       AttributeError: type object 'StrategyGenome' has no attribute 'tournament_selection'

tests\test_strategy_generator.py:216: AttributeError
________________________________ TestStrategyGenerator.test_population_initialization _________________________________

self = <test_strategy_generator.TestStrategyGenerator object at 0x00000266224AF8E0>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpg1c2nmaa'

    @pytest.mark.asyncio
    async def test_population_initialization(self, test_config, temp_dir):
        """Test population initialization."""
        config = test_config.get("strategy_generator", {})
        config["population_size"] = 10  # Smaller for testing
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

>       assert len(generator.population) == 10
E       assert 0 == 10
E        +  where 0 = len([])
E        +    where [] = <optimization.strategy_generator.StrategyGenerator object at 0x0000026627929540>.population

tests\test_strategy_generator.py:247: AssertionError
____________________________________ TestStrategyGenerator.test_evolution_process _____________________________________

self = <test_strategy_generator.TestStrategyGenerator object at 0x00000266224AFC10>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmphhoo40rg'
generate_strategy_population = <function generate_strategy_population.<locals>._generate_population at 0x0000026627E64160>

    @pytest.mark.asyncio
    async def test_evolution_process(self, test_config, temp_dir, generate_strategy_population):
        """Test evolution process."""
        config = test_config.get("strategy_generator", {})
        config["population_size"] = 5  # Very small for testing
        config["generations"] = 2
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        # Mock fitness evaluation
        async def mock_evaluate_fitness(genome):
            # Simple fitness based on number of genes
            return len(genome.genes) * 0.1

        generator.evaluate_fitness = mock_evaluate_fitness

        # Run evolution
>       await generator.evolve()
E       AttributeError: 'StrategyGenerator' object has no attribute 'evolve'

tests\test_strategy_generator.py:272: AttributeError
___________________________________ TestStrategyGenerator.test_strategy_generation ____________________________________

self = <test_strategy_generator.TestStrategyGenerator object at 0x00000266224AFF10>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp86e99l1j'

    @pytest.mark.asyncio
    async def test_strategy_generation(self, test_config, temp_dir):
        """Test strategy generation from genome."""
        config = test_config.get("strategy_generator", {})
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        # Get first genome from population
>       genome = generator.population[0]
E       IndexError: list index out of range

tests\test_strategy_generator.py:290: IndexError
___________________________________ TestStrategyGenerator.test_population_save_load ___________________________________

self = <test_strategy_generator.TestStrategyGenerator object at 0x00000266224D8250>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp3udnxr9m'

    @pytest.mark.asyncio
    async def test_population_save_load(self, test_config, temp_dir):
        """Test population saving and loading."""
        config = test_config.get("strategy_generator", {})
        config["population_size"] = 5
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        # Save population
>       await generator.save_population()
E       TypeError: StrategyGenerator.save_population() missing 1 required positional argument: 'path'

tests\test_strategy_generator.py:310: TypeError
___________________________________ TestFitnessEvaluation.test_fitness_calculation ____________________________________

self = <test_strategy_generator.TestFitnessEvaluation object at 0x00000266224AEE30>
generate_strategy_population = <function generate_strategy_population.<locals>._generate_population at 0x0000026627E64DC0>
synthetic_market_data =                              open          high           low         close      volume
timestamp                     ...923835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]

    @pytest.mark.asyncio
    async def test_fitness_calculation(self, generate_strategy_population, synthetic_market_data):
        """Test fitness calculation for strategies."""
        population = generate_strategy_population(3)

        for genome in population:
            # Calculate fitness
>           fitness = await StrategyGenerator.calculate_fitness(genome, synthetic_market_data)
E           AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_fitness'. Did you mean: 'evaluate_fitness'?

tests\test_strategy_generator.py:390: AttributeError
___________________________________ TestFitnessEvaluation.test_backtest_integration ___________________________________

self = <test_strategy_generator.TestFitnessEvaluation object at 0x00000266224AC640>
generate_strategy_population = <function generate_strategy_population.<locals>._generate_population at 0x0000026627E679A0>
synthetic_market_data =                              open          high           low         close      volume
timestamp                     ...923835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]

    @pytest.mark.asyncio
    async def test_backtest_integration(self, generate_strategy_population, synthetic_market_data):
        """Test integration with backtesting system."""
        genome = generate_strategy_population(1)[0]

        # Mock backtester
        backtester = Mock(spec=Backtester)
        backtester.run_backtest = AsyncMock(return_value={
            'total_return': 0.15,
            'sharpe_ratio': 1.8,
            'max_drawdown': 0.08,
            'win_rate': 0.65,
            'total_trades': 50
        })

        # Calculate fitness using backtest results
>       fitness = await StrategyGenerator.calculate_fitness_with_backtest(
            genome, synthetic_market_data, backtester
        )
E       AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_fitness_with_backtest'

tests\test_strategy_generator.py:411: AttributeError
_________________________________ TestFitnessEvaluation.test_multi_objective_fitness __________________________________

self = <test_strategy_generator.TestFitnessEvaluation object at 0x00000266224D8490>

    def test_multi_objective_fitness(self):
        """Test multi-objective fitness calculation."""
        # Mock backtest results
        results = {
            'total_return': 0.20,      # 20% return
            'sharpe_ratio': 2.1,       # Good Sharpe
            'max_drawdown': 0.05,      # 5% drawdown
            'win_rate': 0.70,          # 70% win rate
            'total_trades': 100
        }

        # Calculate multi-objective fitness
>       fitness = StrategyGenerator.calculate_multi_objective_fitness(results)
E       AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_multi_objective_fitness'

tests\test_strategy_generator.py:433: AttributeError
__________________________________ TestFitnessEvaluation.test_risk_adjusted_fitness ___________________________________

self = <test_strategy_generator.TestFitnessEvaluation object at 0x00000266224D86A0>

    def test_risk_adjusted_fitness(self):
        """Test risk-adjusted fitness calculation."""
        # Strategy with high returns but high risk
        high_risk_results = {
            'total_return': 0.30,
            'sharpe_ratio': 1.2,
            'max_drawdown': 0.15,
            'win_rate': 0.60
        }

        # Strategy with moderate returns but low risk
        low_risk_results = {
            'total_return': 0.15,
            'sharpe_ratio': 2.5,
            'max_drawdown': 0.03,
            'win_rate': 0.75
        }

>       high_risk_fitness = StrategyGenerator.calculate_risk_adjusted_fitness(high_risk_results)
E       AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_risk_adjusted_fitness'

tests\test_strategy_generator.py:463: AttributeError
_____________________________ TestDistributedEvaluation.test_parallel_fitness_evaluation ______________________________

self = <test_strategy_generator.TestDistributedEvaluation object at 0x00000266224D8E20>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpm30hy7ma'
generate_strategy_population = <function generate_strategy_population.<locals>._generate_population at 0x0000026627E65240>

    @pytest.mark.asyncio
    async def test_parallel_fitness_evaluation(self, test_config, temp_dir, generate_strategy_population):
        """Test parallel fitness evaluation."""
        config = test_config.get("strategy_generator", {})
        config["distributed_enabled"] = True
        config["max_workers"] = 2
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        # Create small population for testing
        population = generate_strategy_population(4)

        # Mock fitness function
        async def mock_fitness(genome):
            await asyncio.sleep(0.01)  # Simulate computation time
            return len(genome.genes) * 0.1

        # Evaluate fitness in parallel
>       fitness_scores = await generator.evaluate_population_fitness_parallel(population, mock_fitness)
E       AttributeError: 'StrategyGenerator' object has no attribute 'evaluate_population_fitness_parallel'

tests\test_strategy_generator.py:507: AttributeError
__________________________________ TestDistributedEvaluation.test_worker_management ___________________________________

self = <test_strategy_generator.TestDistributedEvaluation object at 0x00000266224D9120>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpbr2xeh0e'

    @pytest.mark.asyncio
    async def test_worker_management(self, test_config, temp_dir):
        """Test worker management in distributed evaluation."""
        config = test_config.get("strategy_generator", {})
        config["distributed_enabled"] = True
        config["max_workers"] = 3
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        # Should be able to handle worker management
        assert generator.distributed_evaluator is not None

        # Test worker status
>       worker_status = generator.distributed_evaluator.get_worker_status()
E       AttributeError: 'DistributedEvaluator' object has no attribute 'get_worker_status'

tests\test_strategy_generator.py:527: AttributeError
_____________________________________ TestPerformance.test_generation_performance _____________________________________

self = <test_strategy_generator.TestPerformance object at 0x00000266224D9630>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpmfhzo66v'
performance_timer = <conftest.performance_timer.<locals>.PerformanceTimer object at 0x0000026627F583A0>

    @pytest.mark.asyncio
    async def test_generation_performance(self, test_config, temp_dir, performance_timer):
        """Test strategy generation performance."""
        config = test_config.get("strategy_generator", {})
        config["population_size"] = 5
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

>       genome = generator.population[0]
E       IndexError: list index out of range

tests\test_strategy_generator.py:601: IndexError
_____________________________________ TestPerformance.test_evolution_performance ______________________________________

self = <test_strategy_generator.TestPerformance object at 0x00000266224D9960>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmp5ce72eot'
performance_timer = <conftest.performance_timer.<locals>.PerformanceTimer object at 0x00000266278862F0>

    @pytest.mark.asyncio
    async def test_evolution_performance(self, test_config, temp_dir, performance_timer):
        """Test evolution performance."""
        config = test_config.get("strategy_generator", {})
        config["population_size"] = 10
        config["generations"] = 3
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        # Mock fitness evaluation
        async def quick_fitness(genome):
            return len(genome.genes) * 0.1

        generator.evaluate_fitness = quick_fitness

        # Measure evolution time
        performance_timer.start()
>       await generator.evolve()
E       AttributeError: 'StrategyGenerator' object has no attribute 'evolve'

tests\test_strategy_generator.py:633: AttributeError
_________________________________ TestPerformance.test_memory_usage_during_evolution __________________________________

self = <test_strategy_generator.TestPerformance object at 0x00000266224D9C90>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpn5f9jfd9'
memory_monitor = <conftest.memory_monitor.<locals>.MemoryMonitor object at 0x0000026627587010>

    @pytest.mark.asyncio
    async def test_memory_usage_during_evolution(self, test_config, temp_dir, memory_monitor):
        """Test memory usage during evolution."""
        config = test_config.get("strategy_generator", {})
        config["population_size"] = 20
        config["generations"] = 2
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        memory_monitor.start()

        # Run evolution
        async def mock_fitness(genome):
            return len(genome.genes) * 0.1

        generator.evaluate_fitness = mock_fitness
>       await generator.evolve()
E       AttributeError: 'StrategyGenerator' object has no attribute 'evolve'

tests\test_strategy_generator.py:659: AttributeError
__________________________________ TestErrorHandling.test_empty_population_handling ___________________________________

self = <test_strategy_generator.TestErrorHandling object at 0x00000266224D96F0>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpg4hs9g8p'

    @pytest.mark.asyncio
    async def test_empty_population_handling(self, test_config, temp_dir):
        """Test handling of empty population."""
        config = test_config.get("strategy_generator", {})
        config["population_size"] = 0
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        # Should handle empty population
        assert len(generator.population) == 0

        # Evolution should handle empty population
>       await generator.evolve()
E       AttributeError: 'StrategyGenerator' object has no attribute 'evolve'

tests\test_strategy_generator.py:702: AttributeError
__________________________________ TestErrorHandling.test_fitness_evaluation_failure __________________________________

self = <test_strategy_generator.TestErrorHandling object at 0x00000266224D8C40>
generate_strategy_population = <function generate_strategy_population.<locals>._generate_population at 0x0000026627E648B0>
synthetic_market_data =                              open          high           low         close      volume
timestamp                     ...923835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]

    @pytest.mark.asyncio
    async def test_fitness_evaluation_failure(self, generate_strategy_population, synthetic_market_data):
        """Test handling of fitness evaluation failures."""
        genome = generate_strategy_population(1)[0]

        # Mock backtester that fails
        backtester = Mock(spec=Backtester)
        backtester.run_backtest = AsyncMock(side_effect=Exception("Backtest failed"))

        # Should handle failure gracefully
>       fitness = await StrategyGenerator.calculate_fitness_with_backtest(
            genome, synthetic_market_data, backtester
        )
E       AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_fitness_with_backtest'

tests\test_strategy_generator.py:715: AttributeError
_________________________________ TestHealthMonitoring.test_health_check_integration __________________________________

self = <test_strategy_generator.TestHealthMonitoring object at 0x00000266224D9FF0>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}
temp_dir = 'C:\\Users\\TU\\AppData\\Local\\Temp\\tmpgrh2qycg'

    @pytest.mark.asyncio
    async def test_health_check_integration(self, test_config, temp_dir):
        """Test integration with health monitoring system."""
        from core.diagnostics import get_diagnostics_manager

        config = test_config.get("strategy_generator", {})
        config["model_path"] = temp_dir

        generator = StrategyGenerator(config)
        await generator.initialize()

        diagnostics = get_diagnostics_manager()

        # Register health check
        async def check_strategy_generator():
            try:
                population_size = len(generator.population)
                current_generation = generator.current_generation
                best_fitness = max([g.fitness for g in generator.population]) if generator.population else 0

                status = HealthStatus.HEALTHY if population_size > 0 else HealthStatus.DEGRADED

                return {
                    'component': 'strategy_generator',
                    'status': status,
                    'latency_ms': 15.0,
                    'message': f'Generator healthy: pop={population_size}, gen={current_generation}, best_fit={best_fitness:.2f}',
                    'details': {
                        'population_size': population_size,
                        'current_generation': current_generation,
                        'best_fitness': best_fitness
                    }
                }
            except Exception as e:
                return {
                    'component': 'strategy_generator',
                    'status': HealthStatus.CRITICAL,
                    'message': f'Health check failed: {str(e)}',
                    'details': {'error': str(e)}
                }

        diagnostics.register_health_check('strategy_generator', check_strategy_generator)

        # Run health check
>       state = await diagnostics.run_health_check()

tests\test_strategy_generator.py:789:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\diagnostics.py:183: in run_health_check
    self.state.overall_status = self._calculate_overall_status(results)
core\diagnostics.py:202: in _calculate_overall_status
    if any(r.status == HealthStatus.CRITICAL for r in results):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x00000266279DA5F0>

>   if any(r.status == HealthStatus.CRITICAL for r in results):
E   AttributeError: 'dict' object has no attribute 'status'

core\diagnostics.py:202: AttributeError
_________________________________ TestRuleBasedSelector.test_select_strategy_trending _________________________________

self = <test_strategy_selector.TestRuleBasedSelector object at 0x00000266224F9CC0>
mock_rsi = <MagicMock name='RSIStrategy' id='2637772808128'>
mock_ema = <MagicMock name='EMACrossStrategy' id='2637776221328'>

    @patch('strategies.regime.strategy_selector.EMACrossStrategy')
    @patch('strategies.regime.strategy_selector.RSIStrategy')
    def test_select_strategy_trending(self, mock_rsi, mock_ema):
        """Test strategy selection for trending market."""
        # Mock the strategy classes
        mock_rsi.__name__ = 'RSIStrategy'
        mock_ema.__name__ = 'EMACrossStrategy'

        selector = RuleBasedSelector()

        # Create trending data
        data = pd.DataFrame({
            'open': list(range(10, 50)),
            'high': [i + 3 for i in range(10, 50)],
            'low': [i - 1 for i in range(10, 50)],
            'close': [i + 2 for i in range(10, 50)],
            'volume': [1000] * 40
        })

        available_strategies = [mock_ema, mock_rsi]
        selected = selector.select_strategy(data, available_strategies)

        # Should select EMA for trending markets
>       assert selected == mock_ema
E       AssertionError: assert <MagicMock na...637772808128'> == <MagicMock na...637776221328'>
E
E         Use -v to get more diff

tests\test_strategy_selector.py:163: AssertionError
_____________________________________ TestTimeframeManagerCore.test_remove_symbol _____________________________________

self = <test_timeframe_manager.TestTimeframeManagerCore object at 0x000002662256B6D0>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637770265680'>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}

    @pytest.mark.asyncio
    async def test_remove_symbol(self, mock_data_fetcher, test_config):
        """Test symbol removal."""
        tf_config = test_config.get("multi_timeframe", {})
        manager = TimeframeManager(mock_data_fetcher, tf_config)
        await manager.initialize()

        # Add symbol first
        manager.add_symbol("BTC/USDT", ["1h", "4h"])

        # Remove symbol
>       manager.remove_symbol("BTC/USDT")
E       AttributeError: 'TimeframeManager' object has no attribute 'remove_symbol'

tests\test_timeframe_manager.py:77: AttributeError
________________________________ TestTimeframeManagerCore.test_get_registered_symbols _________________________________

self = <test_timeframe_manager.TestTimeframeManagerCore object at 0x000002662256BA00>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637773082432'>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}

    @pytest.mark.asyncio
    async def test_get_registered_symbols(self, mock_data_fetcher, test_config):
        """Test getting registered symbols."""
        tf_config = test_config.get("multi_timeframe", {})
        manager = TimeframeManager(mock_data_fetcher, tf_config)
        await manager.initialize()

        # Add symbols
        manager.add_symbol("BTC/USDT", ["1h"])
        manager.add_symbol("ETH/USDT", ["1h"])

>       symbols = manager.get_registered_symbols()
E       AttributeError: 'TimeframeManager' object has no attribute 'get_registered_symbols'

tests\test_timeframe_manager.py:92: AttributeError
___________________________ TestDataSynchronization.test_fetch_multi_timeframe_data_success ___________________________

self = <test_timeframe_manager.TestDataSynchronization object at 0x000002662256BAC0>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637775387568'>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    @pytest.mark.asyncio
    async def test_fetch_multi_timeframe_data_success(self, mock_data_fetcher, multi_timeframe_data):
        """Test successful multi-timeframe data fetching."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        # Mock the data fetcher to return multi-timeframe data
        mock_data_fetcher.get_historical_data = AsyncMock(side_effect=[
            multi_timeframe_data['5m'],  # 5m data
            multi_timeframe_data['15m'], # 15m data
            multi_timeframe_data['1h'],  # 1h data
            multi_timeframe_data['4h']   # 4h data
        ])

        manager.add_symbol("BTC/USDT", ["5m", "15m", "1h", "4h"])

        result = await manager.fetch_multi_timeframe_data("BTC/USDT")

>       assert result is not None
E       assert None is not None

tests\test_timeframe_manager.py:120: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:core.timeframe_manager:Unknown timeframe '5m' for symbol BTC/USDT, skipping
ERROR:core.timeframe_manager:Failed to fetch 15m data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  core.timeframe_manager:timeframe_manager.py:152 Unknown timeframe '5m' for symbol BTC/USDT, skipping
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 15m data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
__________________________________ TestDataSynchronization.test_timestamp_alignment ___________________________________

self = <test_timeframe_manager.TestDataSynchronization object at 0x000002662256BE20>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637778143696'>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    @pytest.mark.asyncio
    async def test_timestamp_alignment(self, mock_data_fetcher, multi_timeframe_data):
        """Test timestamp alignment across timeframes."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        # Mock data fetcher
        mock_data_fetcher.get_historical_data = AsyncMock(side_effect=[
            multi_timeframe_data['1h'],
            multi_timeframe_data['4h']
        ])

        manager.add_symbol("BTC/USDT", ["1h", "4h"])

        result = await manager.fetch_multi_timeframe_data("BTC/USDT")

>       assert result is not None
E       assert None is not None

tests\test_timeframe_manager.py:145: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
_________________________________ TestDataSynchronization.test_missing_data_handling __________________________________

self = <test_timeframe_manager.TestDataSynchronization object at 0x0000026622590130>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637767657888'>

    @pytest.mark.asyncio
    async def test_missing_data_handling(self, mock_data_fetcher):
        """Test handling of missing data in timeframes."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        # Mock data fetcher to return None for one timeframe
        mock_data_fetcher.get_historical_data = AsyncMock(side_effect=[
            pd.DataFrame(),  # Empty 1h data
            pd.DataFrame()   # Empty 4h data
        ])

        manager.add_symbol("BTC/USDT", ["1h", "4h"])

        result = await manager.fetch_multi_timeframe_data("BTC/USDT")

        # Should handle gracefully
>       assert result is not None
E       assert None is not None

tests\test_timeframe_manager.py:176: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
______________________________________ TestDataSynchronization.test_data_caching ______________________________________

self = <test_timeframe_manager.TestDataSynchronization object at 0x0000026622590490>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637774535168'>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    @pytest.mark.asyncio
    async def test_data_caching(self, mock_data_fetcher, multi_timeframe_data):
        """Test data caching functionality."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        # Mock data fetcher
        mock_data_fetcher.get_historical_data = AsyncMock(return_value=multi_timeframe_data['1h'])

        manager.add_symbol("BTC/USDT", ["1h"])

        # First fetch
        result1 = await manager.fetch_multi_timeframe_data("BTC/USDT")
>       assert result1 is not None
E       assert None is not None

tests\test_timeframe_manager.py:192: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
______________________________________ TestSyncedData.test_synced_data_creation _______________________________________

self = <test_timeframe_manager.TestSyncedData object at 0x0000026622590CA0>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    def test_synced_data_creation(self, multi_timeframe_data):
        """Test SyncedData object creation."""
>       synced = SyncedData(
            symbol="BTC/USDT",
            data=multi_timeframe_data,
            timestamp=datetime.now()
        )
E       TypeError: SyncedData.__init__() missing 2 required positional arguments: 'last_updated' and 'confidence_score'

tests\test_timeframe_manager.py:229: TypeError
____________________________________ TestSyncedData.test_synced_data_get_timeframe ____________________________________

self = <test_timeframe_manager.TestSyncedData object at 0x0000026622590EE0>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    def test_synced_data_get_timeframe(self, multi_timeframe_data):
        """Test getting specific timeframe data."""
>       synced = SyncedData(
            symbol="BTC/USDT",
            data=multi_timeframe_data,
            timestamp=datetime.now()
        )
E       TypeError: SyncedData.__init__() missing 2 required positional arguments: 'last_updated' and 'confidence_score'

tests\test_timeframe_manager.py:241: TypeError
________________________________ TestSyncedData.test_synced_data_get_latest_timestamp _________________________________

self = <test_timeframe_manager.TestSyncedData object at 0x0000026622591120>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    def test_synced_data_get_latest_timestamp(self, multi_timeframe_data):
        """Test getting latest timestamp across timeframes."""
>       synced = SyncedData(
            symbol="BTC/USDT",
            data=multi_timeframe_data,
            timestamp=datetime.now()
        )
E       TypeError: SyncedData.__init__() missing 2 required positional arguments: 'last_updated' and 'confidence_score'

tests\test_timeframe_manager.py:257: TypeError
_____________________________________ TestSyncedData.test_synced_data_is_aligned ______________________________________

self = <test_timeframe_manager.TestSyncedData object at 0x0000026622591390>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    def test_synced_data_is_aligned(self, multi_timeframe_data):
        """Test timestamp alignment checking."""
>       synced = SyncedData(
            symbol="BTC/USDT",
            data=multi_timeframe_data,
            timestamp=datetime.now()
        )
E       TypeError: SyncedData.__init__() missing 2 required positional arguments: 'last_updated' and 'confidence_score'

tests\test_timeframe_manager.py:269: TypeError
____________________ TestIntegrationWithStrategies.test_strategy_multi_timeframe_signal_generation ____________________

self = <test_timeframe_manager.TestIntegrationWithStrategies object at 0x00000266225918D0>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637773767536'>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    @pytest.mark.asyncio
    async def test_strategy_multi_timeframe_signal_generation(self, mock_data_fetcher, multi_timeframe_data):
        """Test strategy signal generation with multi-timeframe data."""
        from strategies.base_strategy import BaseStrategy

        # Mock strategy that uses multi-timeframe data
        strategy = Mock(spec=BaseStrategy)
        strategy.generate_signals = AsyncMock(return_value=[
            {
                'symbol': 'BTC/USDT',
                'signal_type': 'BUY',
                'price': 50000.0,
                'timestamp': datetime.now(),
                'confidence': 0.8
            }
        ])

        # Setup timeframe manager
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        mock_data_fetcher.get_historical_data = AsyncMock(side_effect=[
            multi_timeframe_data['1h'],
            multi_timeframe_data['4h']
        ])

        manager.add_symbol("BTC/USDT", ["1h", "4h"])

        # Get multi-timeframe data
        mtf_data = await manager.fetch_multi_timeframe_data("BTC/USDT")

        # Generate signals with multi-timeframe data
        signals = await strategy.generate_signals({'BTC/USDT': mtf_data})

        # Verify strategy was called with multi-timeframe data
        strategy.generate_signals.assert_called_once()
        call_args = strategy.generate_signals.call_args[0][0]

        assert 'BTC/USDT' in call_args
>       assert isinstance(call_args['BTC/USDT'], SyncedData)
E       assert False
E        +  where False = isinstance(None, SyncedData)

tests\test_timeframe_manager.py:321: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
_____________________ TestIntegrationWithStrategies.test_backward_compatibility_single_timeframe ______________________

self = <test_timeframe_manager.TestIntegrationWithStrategies object at 0x0000026622591900>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637773112416'>
synthetic_market_data =                              open          high           low         close      volume
timestamp                     ...923835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]

    @pytest.mark.asyncio
    async def test_backward_compatibility_single_timeframe(self, mock_data_fetcher, synthetic_market_data):
        """Test backward compatibility with single timeframe operation."""
        from strategies.base_strategy import BaseStrategy

        strategy = Mock(spec=BaseStrategy)
        strategy.generate_signals = AsyncMock(return_value=[])

        # Setup timeframe manager without multi-timeframe config
        manager = TimeframeManager(mock_data_fetcher, {"enabled": False})
        await manager.initialize()

        mock_data_fetcher.get_historical_data = AsyncMock(return_value=synthetic_market_data)

        # Single timeframe operation should still work
        manager.add_symbol("BTC/USDT", ["1h"])

        # This should work without multi-timeframe data
        result = await manager.fetch_multi_timeframe_data("BTC/USDT")

        # Should return single timeframe data wrapped appropriately
>       assert result is not None
E       assert None is not None

tests\test_timeframe_manager.py:344: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
___________________________________ TestPerformance.test_data_fetching_performance ____________________________________

self = <test_timeframe_manager.TestPerformance object at 0x0000026622591990>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637774263424'>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}
performance_timer = <conftest.performance_timer.<locals>.PerformanceTimer object at 0x0000026627886A40>

    @pytest.mark.asyncio
    async def test_data_fetching_performance(self, mock_data_fetcher, multi_timeframe_data, performance_timer):
        """Test data fetching performance."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        mock_data_fetcher.get_historical_data = AsyncMock(return_value=multi_timeframe_data['1h'])

        manager.add_symbol("BTC/USDT", ["1h"])

        # Measure performance
        performance_timer.start()
        result = await manager.fetch_multi_timeframe_data("BTC/USDT")
        performance_timer.stop()

        # Should complete within reasonable time
        duration_ms = performance_timer.duration_ms()
        assert duration_ms < 1000  # Less than 1 second
>       assert result is not None
E       assert None is not None

tests\test_timeframe_manager.py:369: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
__________________________________ TestPerformance.test_memory_usage_multi_timeframe __________________________________

self = <test_timeframe_manager.TestPerformance object at 0x0000026622591D20>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637769505856'>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}
memory_monitor = <conftest.memory_monitor.<locals>.MemoryMonitor object at 0x00000266279A3700>

    @pytest.mark.asyncio
    async def test_memory_usage_multi_timeframe(self, mock_data_fetcher, multi_timeframe_data, memory_monitor):
        """Test memory usage with multi-timeframe data."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        mock_data_fetcher.get_historical_data = AsyncMock(side_effect=[
            multi_timeframe_data['5m'],
            multi_timeframe_data['15m'],
            multi_timeframe_data['1h'],
            multi_timeframe_data['4h']
        ])

        manager.add_symbol("BTC/USDT", ["5m", "15m", "1h", "4h"])

        memory_monitor.start()

        # Fetch multi-timeframe data
        result = await manager.fetch_multi_timeframe_data("BTC/USDT")

        memory_delta = memory_monitor.get_memory_delta()

        # Memory usage should be reasonable
        assert memory_delta < 100  # Less than 100MB increase
>       assert result is not None
E       assert None is not None

tests\test_timeframe_manager.py:395: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
WARNING:core.timeframe_manager:Unknown timeframe '5m' for symbol BTC/USDT, skipping
ERROR:core.timeframe_manager:Failed to fetch 15m data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
WARNING  core.timeframe_manager:timeframe_manager.py:152 Unknown timeframe '5m' for symbol BTC/USDT, skipping
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 15m data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
___________________________________ TestPerformance.test_concurrent_symbol_fetching ___________________________________

self = <test_timeframe_manager.TestPerformance object at 0x0000026622592050>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637775448544'>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    @pytest.mark.asyncio
    async def test_concurrent_symbol_fetching(self, mock_data_fetcher, multi_timeframe_data):
        """Test concurrent fetching for multiple symbols."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        # Mock concurrent data fetching
        async def mock_get_data(*args, **kwargs):
            await asyncio.sleep(0.01)  # Simulate network delay
            return multi_timeframe_data['1h']

        mock_data_fetcher.get_historical_data = AsyncMock(side_effect=mock_get_data)

        # Add multiple symbols
        symbols = ["BTC/USDT", "ETH/USDT", "ADA/USDT"]
        for symbol in symbols:
            manager.add_symbol(symbol, ["1h"])

        # Fetch data concurrently
        tasks = [
            manager.fetch_multi_timeframe_data(symbol)
            for symbol in symbols
        ]

        results = await asyncio.gather(*tasks)

        # All fetches should succeed
>       assert all(result is not None for result in results)
E       assert False
E        +  where False = all(<generator object TestPerformance.test_concurrent_symbol_fetching.<locals>.<genexpr> at 0x0000026627FB7A00>)

tests\test_timeframe_manager.py:424: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 1h data for ETH/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 1h data for ADA/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
ERROR:core.timeframe_manager:No data fetched for symbol ETH/USDT
ERROR:core.timeframe_manager:No data fetched for symbol ADA/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for ETH/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for ADA/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol ETH/USDT
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol ADA/USDT
_____________________________________ TestErrorHandling.test_partial_data_failure _____________________________________

self = <test_timeframe_manager.TestErrorHandling object at 0x0000026622592890>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637775054992'>
multi_timeframe_data = {'15m':                              open          high           low         close      volume
timestamp             ...23835
2024-02-11 15:00:00  59650.840169  59817.653832  59582.881290  59796.419993  552.241789

[1000 rows x 5 columns]}

    @pytest.mark.asyncio
    async def test_partial_data_failure(self, mock_data_fetcher, multi_timeframe_data):
        """Test handling of partial data fetching failures."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        # Mock partial failure - some timeframes succeed, others fail
        mock_data_fetcher.get_historical_data = AsyncMock(side_effect=[
            multi_timeframe_data['1h'],  # Success
            ConnectionError("Failed to fetch 4h data")  # Failure
        ])

        manager.add_symbol("BTC/USDT", ["1h", "4h"])

        result = await manager.fetch_multi_timeframe_data("BTC/USDT")

        # Should handle partial failure gracefully
>       assert result is not None
E       assert None is not None

tests\test_timeframe_manager.py:466: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 4h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
_____________________________________ TestErrorHandling.test_empty_data_handling ______________________________________

self = <test_timeframe_manager.TestErrorHandling object at 0x0000026622592B60>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637775997008'>

    @pytest.mark.asyncio
    async def test_empty_data_handling(self, mock_data_fetcher):
        """Test handling of empty data responses."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        # Mock empty data response
        empty_df = pd.DataFrame()
        mock_data_fetcher.get_historical_data = AsyncMock(return_value=empty_df)

        manager.add_symbol("BTC/USDT", ["1h"])

        result = await manager.fetch_multi_timeframe_data("BTC/USDT")

        # Should handle empty data gracefully
>       assert result is not None
E       assert None is not None

tests\test_timeframe_manager.py:485: AssertionError
------------------------------------------------ Captured stderr call -------------------------------------------------
ERROR:core.timeframe_manager:Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR:core.timeframe_manager:No data fetched for symbol BTC/USDT
-------------------------------------------------- Captured log call --------------------------------------------------
ERROR    core.timeframe_manager:timeframe_manager.py:259 Failed to fetch 1h data for BTC/USDT: 'coroutine' object has no attribute 'empty'
ERROR    core.timeframe_manager:timeframe_manager.py:204 No data fetched for symbol BTC/USDT
_______________________________________ TestErrorHandling.test_shutdown_cleanup _______________________________________

self = <test_timeframe_manager.TestErrorHandling object at 0x0000026622593160>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637777730480'>

    @pytest.mark.asyncio
    async def test_shutdown_cleanup(self, mock_data_fetcher):
        """Test proper cleanup on shutdown."""
        manager = TimeframeManager(mock_data_fetcher, {})
        await manager.initialize()

        # Add some data to cache
        manager.add_symbol("BTC/USDT", ["1h"])
        manager.cache["BTC/USDT"] = "test_data"

        # Shutdown
        await manager.shutdown()

        # Cache should be cleared
>       assert len(manager.cache) == 0
E       AssertionError: assert 1 == 0
E        +  where 1 = len({'BTC/USDT': 'test_data'})
E        +    where {'BTC/USDT': 'test_data'} = <core.timeframe_manager.TimeframeManager object at 0x00000266279CCC10>.cache

tests\test_timeframe_manager.py:514: AssertionError
_________________________________ TestHealthMonitoring.test_health_check_integration __________________________________

self = <test_timeframe_manager.TestHealthMonitoring object at 0x0000026622592EF0>
mock_data_fetcher = <Mock spec='DataFetcher' id='2637773030832'>
test_config = {'backtesting': {'end_date': '2024-01-31', 'start_date': '2024-01-01', 'timeframe': '1h'}, 'environment': {'mode': 'ba...'testnet': True}, 'monitoring': {'performance_tracking': True, 'terminal_display': False, 'update_interval': 1.0}, ...}

    @pytest.mark.asyncio
    async def test_health_check_integration(self, mock_data_fetcher, test_config):
        """Test integration with health monitoring system."""
        from core.diagnostics import get_diagnostics_manager

        tf_config = test_config.get("multi_timeframe", {})
        manager = TimeframeManager(mock_data_fetcher, tf_config)
        await manager.initialize()

        diagnostics = get_diagnostics_manager()

        # Register health check
        async def check_timeframe_manager():
            try:
                # Simple health check
                symbol_count = len(manager.get_registered_symbols())
                cache_size = len(manager.cache)

                return {
                    'component': 'timeframe_manager',
                    'status': HealthStatus.HEALTHY,
                    'latency_ms': 10.0,
                    'message': f'Healthy: {symbol_count} symbols, {cache_size} cached',
                    'details': {
                        'registered_symbols': symbol_count,
                        'cache_entries': cache_size
                    }
                }
            except Exception as e:
                return {
                    'component': 'timeframe_manager',
                    'status': HealthStatus.CRITICAL,
                    'message': f'Health check failed: {str(e)}',
                    'details': {'error': str(e)}
                }

        diagnostics.register_health_check('timeframe_manager', check_timeframe_manager)

        # Run health check
>       state = await diagnostics.run_health_check()

tests\test_timeframe_manager.py:560:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
core\diagnostics.py:183: in run_health_check
    self.state.overall_status = self._calculate_overall_status(results)
core\diagnostics.py:202: in _calculate_overall_status
    if any(r.status == HealthStatus.CRITICAL for r in results):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

.0 = <list_iterator object at 0x000002662795D180>

>   if any(r.status == HealthStatus.CRITICAL for r in results):
E   AttributeError: 'dict' object has no attribute 'status'

core\diagnostics.py:202: AttributeError
================================================== warnings summary ===================================================
strategies\regime\regime_forecaster.py:63
  C:\Users\TU\Desktop\new project\N1V1\strategies\regime\regime_forecaster.py:63: UserWarning: PyTorch not available, transformer models disabled
    warnings.warn("PyTorch not available, transformer models disabled")

tests\test_discord_integration.py:298
  C:\Users\TU\Desktop\new project\N1V1\tests\test_discord_integration.py:298: PytestUnknownMarkWarning: Unknown pytest.mark.discord_live - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.discord_live

tests\test_integration_all_features.py:827
  C:\Users\TU\Desktop\new project\N1V1\tests\test_integration_all_features.py:827: PytestUnknownMarkWarning: Unknown pytest.mark.benchmark - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.benchmark

demo/test_simple_metrics.py::test_metrics_calculation
  C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:197: PytestReturnNotNoneWarning: Expected None, but demo/test_simple_metrics.py::test_metrics_calculation returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

demo/test_simple_metrics.py::test_metrics_result_structure
  C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:197: PytestReturnNotNoneWarning: Expected None, but demo/test_simple_metrics.py::test_metrics_result_structure returned False, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/test_circular_import_fix.py::test_import_chain
  C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:197: PytestReturnNotNoneWarning: Expected None, but tests/test_circular_import_fix.py::test_import_chain returned False, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/test_circular_import_fix.py::test_lazy_import_functionality
  C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:197: PytestReturnNotNoneWarning: Expected None, but tests/test_circular_import_fix.py::test_lazy_import_functionality returned False, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/test_circular_import_fix.py::test_module_interactions
  C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\_pytest\python.py:197: PytestReturnNotNoneWarning: Expected None, but tests/test_circular_import_fix.py::test_module_interactions returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/test_trainer.py::TestModelTraining::test_train_model_insufficient_data
  C:\Users\TU\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\model_selection\_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=============================================== short test summary info ===============================================
FAILED tests/core/test_anomaly_detection.py::TestAnomalySeverityLevels::test_warning_severity_for_moderate_spike - AssertionError: assert <AlertSeverity.INFO: 'info'> == <AlertSeverity.WARNING: 'warning'>
FAILED tests/core/test_diagnostics.py::TestBuiltInHealthChecks::test_check_api_connectivity_success - AssertionError: assert <HealthStatus.CRITICAL: 'critical'> == <HealthStatus.HEALTHY: 'healthy'>
FAILED tests/core/test_diagnostics.py::TestBuiltInHealthChecks::test_check_api_connectivity_timeout - AssertionError: assert 'API timeout' in 'API connection failed: Cannot connect to host api.example.com:443 ssl:defa...
FAILED tests/core/test_diagnostics.py::TestBuiltInHealthChecks::test_check_api_connectivity_failure - AssertionError: assert 'Cannot conne...rinfo failed]' == 'Connection failed'
FAILED tests/execution/test_validator.py::TestExecutionValidator::test_validate_valid_signal - assert False is True
FAILED tests/execution/test_validator.py::TestExecutionValidator::test_validate_order_size_invalid_lot_size - assert True is False
FAILED tests/execution/test_validator.py::TestExecutionValidator::test_validate_balance_sufficient - assert False is True
FAILED tests/execution/test_validator.py::TestExecutionValidator::test_validate_exchange_constraints_trading_hours - NameError: name 'context' is not defined
FAILED tests/execution/test_validator.py::TestValidationEdgeCases::test_balance_validation_without_price - assert False is True
FAILED tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_route_signal_success - TypeError: TradingSignal.__init__() got an unexpected keyword argument 'side'
FAILED tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_route_signal_unknown_strategy - TypeError: TradingSignal.__init__() got an unexpected keyword argument 'side'
FAILED tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_route_signal_risk_limit_exceeded - TypeError: TradingSignal.__init__() got an unexpected keyword argument 'side'
FAILED tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_multiple_strategies_equal_weight - AssertionError: assert 1.0 == 0.5
FAILED tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_weight_constraints - AssertionError: assert 0.01 >= 0.1
FAILED tests/portfolio/test_strategy_ensemble.py::TestEnsembleSignal::test_ensemble_signal_creation - TypeError: TradingSignal.__init__() got an unexpected keyword argument 'side'
FAILED tests/test_anomaly_detector.py::TestPriceZScoreDetector::test_detect_normal_price_movement - AssertionError: assert False is False
FAILED tests/test_anomaly_detector.py::TestPriceZScoreDetector::test_detect_price_anomaly - AssertionError: assert True is True
FAILED tests/test_anomaly_detector.py::TestVolumeZScoreDetector::test_detect_normal_volume - AssertionError: assert False is False
FAILED tests/test_anomaly_detector.py::TestVolumeZScoreDetector::test_detect_volume_spike - AssertionError: assert True is True
FAILED tests/test_anomaly_detector.py::TestPriceGapDetector::test_detect_normal_price_change - AssertionError: assert False is False
FAILED tests/test_anomaly_detector.py::TestPriceGapDetector::test_detect_price_gap - AssertionError: assert True is True
FAILED tests/test_anomaly_detector.py::TestPriceGapDetector::test_detect_small_gap - AssertionError: assert False is False
FAILED tests/test_anomaly_detector.py::TestAnomalyDetector::test_check_signal_anomaly_scale_down - assert False is True
FAILED tests/test_anomaly_detector.py::TestEdgeCases::test_extreme_values - AssertionError: assert True is True
FAILED tests/test_anomaly_detector.py::TestEdgeCases::test_max_history_limit - AssertionError: assert 10 == 5
FAILED tests/test_backtester.py::test_record_trade_equity_appends_record - assert 0 == 1
FAILED tests/test_backtester.py::test_record_trade_equity_fallbacks_to_total_pnl_when_order_manager_unavailable - IndexError: list index out of range
FAILED tests/test_backtester.py::test_record_trade_equity_handles_missing_pnl - IndexError: list index out of range
FAILED tests/test_backtester.py::test_export_equity_csv_written_and_contains_rows - assert 0 == 2
FAILED tests/test_bot_engine.py::TestBotEngine::test_trading_cycle - NameError: name 'time' is not defined
FAILED tests/test_bot_engine.py::TestBotEngine::test_record_trade_equity - assert 0 == 1
FAILED tests/test_bot_engine.py::TestBotEngine::test_trading_cycle_with_portfolio_mode - NameError: name 'time' is not defined
FAILED tests/test_bot_engine.py::TestBotEngine::test_trading_cycle_with_safe_mode_active - NameError: name 'time' is not defined
FAILED tests/test_bot_engine.py::TestBotEngine::test_record_trade_equity_backtest_mode - IndexError: list index out of range
FAILED tests/test_circuit_breaker.py::TestCircuitBreakerConfiguration::test_configuration_validation - NameError: name 'CircuitBreakerConfig' is not defined
FAILED tests/test_circuit_breaker.py::TestCircuitBreakerConfiguration::test_runtime_configuration_updates - NameError: name 'CircuitBreakerConfig' is not defined
FAILED tests/test_cli.py::TestCLI::test_status_flag_output_format - subprocess.TimeoutExpired: Command '['C:\\Users\\TU\\AppData\\Local\\Programs\\Python\\Python310\\python.exe', 'mai...
FAILED tests/test_config_loader.py::TestConfigLoader::test_lines_319_330_enforce_live_secrets_exchange - utils.security.SecurityException: Live mode requires exchange credentials: provide exchange.api_key and exchange.ap...
FAILED tests/test_config_loader.py::TestConfigLoader::test_line_333_enforce_live_secrets_exchange_with_creds - utils.security.SecurityException: Live mode requires exchange credentials: provide exchange.api_key and exchange.ap...
FAILED tests/test_config_loader.py::TestConfigLoader::test_lines_338_345_enforce_live_secrets_discord - utils.security.SecurityException: Live mode requires exchange credentials: provide exchange.api_key and exchange.ap...
FAILED tests/test_config_loader.py::TestConfigLoader::test_enforce_live_secrets_discord_with_webhook - utils.security.SecurityException: Live mode requires exchange credentials: provide exchange.api_key and exchange.ap...
FAILED tests/test_config_loader.py::TestConfigLoader::test_enforce_live_secrets_discord_with_bot_creds - utils.security.SecurityException: Live mode requires exchange credentials: provide exchange.api_key and exchange.ap...
FAILED tests/test_cross_asset_validation.py::TestDataPersistence::test_save_results_creates_files - AssertionError: assert False
FAILED tests/test_cross_asset_validation.py::TestDataPersistence::test_csv_summary_format - assert True is True
FAILED tests/test_cross_feature_integration.py::TestMonitoringPerformanceIntegration::test_performance_metrics_monitoring_system - assert 0 > 0
FAILED tests/test_cross_feature_integration.py::TestMonitoringPerformanceIntegration::test_monitoring_performance_profiling_operations - AssertionError: System health too low: 50.0
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_basic_notification - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_with_embed - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_trade_alert - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_signal_alert - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_error_alert - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_performance_report - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_bot_integration_basic_notification - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_bot_integration_trade_alert - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_rate_limit_handling - assert 0 > 0
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_large_payload - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_special_characters - assert False is True
FAILED tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_empty_content - assert False is True
FAILED tests/test_docstring_standardization.py::TestDocstringStandardizer::test_docstring_standardization - assert 'process_data' in '"""Simple function."""\n\nArgs:\n    data: Description of data\n    config: Description o...
FAILED tests/test_docstring_standardization.py::TestDocstringStandardizerIntegration::test_analyze_codebase_documentation - AttributeError: 'TestDocstringStandardizerIntegration' object has no attribute 'standardizer'
FAILED tests/test_docstring_standardization.py::TestDocstringStandardizerIntegration::test_quality_score_calculation - AttributeError: 'TestDocstringStandardizerIntegration' object has no attribute 'standardizer'
FAILED tests/test_docstring_standardization.py::TestDocstringStandardizerIntegration::test_recommendations_generation - AttributeError: 'TestDocstringStandardizerIntegration' object has no attribute 'standardizer'
FAILED tests/test_integration_all_features.py::TestMultiTimeframeRegimeIntegration::test_regime_forecasting_with_multi_timeframe_data - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestMultiTimeframeRegimeIntegration::test_regime_transition_detection_with_mtf - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestStrategyGeneratorSelfHealingIntegration::test_strategy_generation_with_health_monitoring - AttributeError: 'StrategyGenerator' object has no attribute 'evolve'
FAILED tests/test_integration_all_features.py::TestStrategyGeneratorSelfHealingIntegration::test_strategy_deployment_with_failure_recovery - assert 0 > 0
FAILED tests/test_integration_all_features.py::TestFullSystemIntegration::test_end_to_end_trading_workflow - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestFullSystemIntegration::test_cross_feature_performance_optimization - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestFullSystemIntegration::test_resource_contention_handling - RuntimeError: Event loop is closed
FAILED tests/test_integration_all_features.py::TestFailureScenariosAcrossFeatures::test_cascading_failure_prevention - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestFailureScenariosAcrossFeatures::test_recovery_coordination_across_features - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestRealisticMarketScenarios::test_high_volatility_scenario - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestRealisticMarketScenarios::test_regime_transition_scenario - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestProductionReadiness::test_24_hour_stability_test - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_integration_all_features.py::TestProductionReadiness::test_resource_usage_under_load - assert 2055 == 2000
FAILED tests/test_integration_all_features.py::TestProductionReadiness::test_security_integration - assert 2056 == 1
FAILED tests/test_main.py::TestMainFunction::test_main_fastapi_mode_unavailable - AssertionError: Expected 'exit' to be called once. Called 2 times.
FAILED tests/test_ml_signal_router.py::test_ml_confirmation_rejects_weak_signal_on_opposite_prediction - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_ml_signal_router.py::test_ml_confirmation_accepts_signal_on_matching_prediction - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_ml_signal_router.py::test_ml_confirmation_skips_when_no_features - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_optimization.py::TestWalkForwardOptimizer::test_initialization - AttributeError: 'WalkForwardOptimizer' object has no attribute 'train_window_days'
FAILED tests/test_optimization.py::TestWalkForwardOptimizer::test_window_generation - AttributeError: 'WalkForwardOptimizer' object has no attribute '_generate_windows'
FAILED tests/test_optimization.py::TestWalkForwardOptimizer::test_parameter_combinations - AttributeError: 'WalkForwardOptimizer' object has no attribute '_generate_param_combinations'
FAILED tests/test_optimization.py::TestOptimizerFactory::test_create_from_config - AttributeError: 'WalkForwardOptimizer' object has no attribute 'train_window_days'
FAILED tests/test_optimization.py::TestCrossPairValidation::test_cross_pair_validation_basic - AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'
FAILED tests/test_optimization.py::TestCrossPairValidation::test_cross_pair_validation_empty_validation_pairs - AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'
FAILED tests/test_optimization.py::TestCrossPairValidation::test_cross_pair_validation_missing_train_pair - AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'
FAILED tests/test_optimization.py::TestCrossPairValidation::test_cross_pair_validation_insufficient_data - AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'
FAILED tests/test_optimization.py::TestCrossPairValidation::test_save_cross_validation_results - AttributeError: 'WalkForwardOptimizer' object has no attribute '_save_cross_validation_results'
FAILED tests/test_optimization.py::TestCrossPairValidation::test_cross_pair_validation_with_config - AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'
FAILED tests/test_optimization.py::TestCrossPairValidation::test_cross_pair_validation_skip_if_empty_validation_pairs - AttributeError: 'WalkForwardOptimizer' object has no attribute 'cross_pair_validation'
FAILED tests/test_performance_optimization.py::TestProfilingAccuracy::test_function_timing_accuracy - AssertionError: assert 'test_function' in deque([])
FAILED tests/test_performance_optimization.py::TestProfilingAccuracy::test_memory_usage_tracking - AttributeError: 'PerformanceProfiler' object has no attribute '_get_memory_usage'
FAILED tests/test_performance_optimization.py::TestProfilingAccuracy::test_io_operations_monitoring - IndexError: list index out of range
FAILED tests/test_performance_optimization.py::TestProfilingAccuracy::test_garbage_collection_impact - IndexError: list index out of range
FAILED tests/test_performance_optimization.py::TestProfilingAccuracy::test_cpu_usage_monitoring - IndexError: list index out of range
FAILED tests/test_performance_optimization.py::TestProfilingAccuracy::test_concurrent_profiling_accuracy - TypeError: 'coroutine' object is not callable
FAILED tests/test_performance_optimization.py::TestOptimizationValidation::test_optimization_result_consistency - assert 2.4985118386735223 < 0.001
FAILED tests/test_performance_optimization.py::TestPerformanceBenchmarks::test_vectorization_speedup - assert 4.729372449219227e-10 < 1e-10
FAILED tests/test_performance_optimization.py::TestPerformanceBenchmarks::test_baseline_establishment - AssertionError: Baseline variability too high: CV = 0.79
FAILED tests/test_regime_forecaster.py::TestRegimeForecasterInitialization::test_initialization_with_config - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'enabled'
FAILED tests/test_regime_forecaster.py::TestRegimeForecasterInitialization::test_async_initialization - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'enabled'
FAILED tests/test_regime_forecaster.py::TestRegimeForecasterInitialization::test_initialization_without_config - AttributeError: 'RegimeForecaster' object has no attribute 'forecast_horizon'
FAILED tests/test_regime_forecaster.py::TestFeatureEngineering::test_feature_engineering_basic - AttributeError: 'RegimeForecaster' object has no attribute '_extract_features'
FAILED tests/test_regime_forecaster.py::TestFeatureEngineering::test_feature_engineering_different_regimes - AttributeError: 'RegimeForecaster' object has no attribute '_extract_features'
FAILED tests/test_regime_forecaster.py::TestFeatureEngineering::test_feature_engineering_edge_cases - AttributeError: 'RegimeForecaster' object has no attribute '_extract_features'
FAILED tests/test_regime_forecaster.py::TestFeatureEngineering::test_technical_indicators_calculation - AttributeError: 'RegimeForecaster' object has no attribute '_calculate_rsi'
FAILED tests/test_regime_forecaster.py::TestModelTraining::test_model_initialization - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'enabled'
FAILED tests/test_regime_forecaster.py::TestModelTraining::test_training_data_preparation - AttributeError: 'RegimeForecaster' object has no attribute '_prepare_training_data'
FAILED tests/test_regime_forecaster.py::TestModelTraining::test_model_training - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestModelTraining::test_model_persistence - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestPrediction::test_regime_prediction - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestPrediction::test_prediction_confidence_calibration - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestPrediction::test_prediction_edge_cases - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestPrediction::test_forecast_horizon - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestIntegrationWithStrategySelector::test_strategy_selector_integration - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestIntegrationWithStrategySelector::test_forecast_driven_strategy_selection - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestPerformance::test_prediction_performance - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestPerformance::test_memory_usage - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestPerformance::test_concurrent_predictions - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestModelUpdates::test_model_update - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestModelUpdates::test_model_versioning - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestErrorHandling::test_prediction_with_corrupted_data - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestErrorHandling::test_model_loading_failure - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestErrorHandling::test_empty_training_data - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestErrorHandling::test_insufficient_data_for_training - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'model_path'
FAILED tests/test_regime_forecaster.py::TestHealthMonitoring::test_health_check_integration - TypeError: ForecastingConfig.__init__() got an unexpected keyword argument 'enabled'
FAILED tests/test_safe_mode.py::test_botengine_respects_order_manager_safe_mode - NameError: name 'time' is not defined
FAILED tests/test_self_healing_engine.py::TestFailureDetector::test_anomaly_detection_latency - AssertionError: assert <FailureSeverity.HIGH: 'high'> == <FailureSeverity.MEDIUM: 'medium'>
FAILED tests/test_self_healing_engine.py::TestRecoveryOrchestrator::test_recovery_initiation - AssertionError: assert '_recover_bot_engine' == 'test_recovery_connectivity'
FAILED tests/test_self_healing_engine.py::TestRecoveryOrchestrator::test_priority_calculation - AttributeError: 'HealingOrchestrator' object has no attribute '_calculate_priority'
FAILED tests/test_self_healing_engine.py::TestRecoveryOrchestrator::test_timeout_calculation - assert 90 == 60
FAILED tests/test_self_healing_engine.py::TestWatchdogService::test_overdue_heartbeat_detection - assert 0 > 0
FAILED tests/test_self_healing_engine.py::TestMonitoringDashboard::test_system_health_calculation - AssertionError: assert 'HEALTHY' == 'DEGRADED'
FAILED tests/test_self_healing_engine.py::TestMonitoringDashboard::test_failure_statistics - assert 0 == 5
FAILED tests/test_self_healing_engine.py::TestSelfHealingEngine::test_component_status_retrieval - AttributeError: 'NoneType' object has no attribute 'get'
FAILED tests/test_self_healing_engine.py::TestIntegrationWithN1V1::test_diagnostics_integration - AttributeError: 'dict' object has no attribute 'status'
FAILED tests/test_self_healing_engine.py::TestIntegrationWithN1V1::test_event_bus_integration - ImportError: cannot import name 'get_default_enhanced_event_bus' from 'core.signal_router.events' (C:\Users\TU\Desk...
FAILED tests/test_self_healing_engine.py::TestPerformance::test_heartbeat_processing_performance - assert 11611.062 < 1000
FAILED tests/test_self_healing_engine.py::TestPerformance::test_concurrent_heartbeat_processing - assert 2167 == 10
FAILED tests/test_self_healing_engine.py::TestReliability::test_long_running_stability - assert 13.831456184387207 < 5
FAILED tests/test_signal_router.py::test_retry_async_call_success - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_retry_async_call_with_failure - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_retry_async_call_exhaust_retries - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_concurrent_signal_processing_no_conflicts - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_conflicting_signals_resolution - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_signal_router_init_defaults - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_confirmation_weak_signal_approved - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_confirmation_weak_signal_rejected - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_confirmation_strong_signal_bypassed - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_predict_called_with_dataframe_features - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_predict_called_with_dict_features - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_predict_called_with_series_features - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_predict_not_called_with_empty_dataframe - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_predict_called_with_metadata_features_fallback - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_predict_called_with_ohlcv_fallback - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_get_symbol_lock_concurrency - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_concurrent_signal_processing_different_symbols - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_concurrent_signal_processing_same_symbol_conflicts - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_retry_async_call_with_custom_backoff - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_retry_async_call_max_backoff_cap - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_journal_recovery_with_corrupted_entries - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_journal_recovery_signal_reconstruction - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_journal_recovery_enum_conversion_edge_cases - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_extract_features_edge_cases - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_extract_features_fallback_to_metadata - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_extract_features_multiple_fallbacks - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_retry_async_call_with_different_exception_types - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_retry_async_call_zero_retries - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_ml_confirmation_disabled - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_ml_model_none - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_non_entry_signal_type - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_ml_prediction_error_handling - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_ml_low_confidence_ignored - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_blocking_mode - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_risk_manager_failure - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_validation_failure - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_record_router_error_threshold_trigger - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_record_router_error_context_logging - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_update_signal_status_nonexistent_signal - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_get_active_signals_empty - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_get_signal_history_limit - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_close_journal_no_journal - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_close_journal_with_journal - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_get_symbol_lock_none_symbol - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_get_symbol_lock_creates_new_lock - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_process_signal_with_invalid_symbol_lock - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_resolve_conflicts_with_empty_list - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_resolve_conflicts_strength_based - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_resolve_conflicts_newer_first - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_store_signal_with_journal_disabled - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_cancel_signal_with_journal_disabled - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_recover_from_journal_disabled - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_extract_features_with_list_of_scalars - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_signal_router.py::test_ml_extract_features_with_invalid_data - AttributeError: 'str' object has no attribute 'get'
FAILED tests/test_strategy_generator.py::TestStrategyGenome::test_genome_creation - assert -inf == 0.0
FAILED tests/test_strategy_generator.py::TestGeneticOperations::test_mutation_operation - AttributeError: 'NoneType' object has no attribute 'genes'
FAILED tests/test_strategy_generator.py::TestGeneticOperations::test_selection_operation - AttributeError: type object 'StrategyGenome' has no attribute 'select_best'
FAILED tests/test_strategy_generator.py::TestGeneticOperations::test_tournament_selection - AttributeError: type object 'StrategyGenome' has no attribute 'tournament_selection'
FAILED tests/test_strategy_generator.py::TestStrategyGenerator::test_population_initialization - assert 0 == 10
FAILED tests/test_strategy_generator.py::TestStrategyGenerator::test_evolution_process - AttributeError: 'StrategyGenerator' object has no attribute 'evolve'
FAILED tests/test_strategy_generator.py::TestStrategyGenerator::test_strategy_generation - IndexError: list index out of range
FAILED tests/test_strategy_generator.py::TestStrategyGenerator::test_population_save_load - TypeError: StrategyGenerator.save_population() missing 1 required positional argument: 'path'
FAILED tests/test_strategy_generator.py::TestFitnessEvaluation::test_fitness_calculation - AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_fitness'. Did you mean: 'evaluate_fitne...
FAILED tests/test_strategy_generator.py::TestFitnessEvaluation::test_backtest_integration - AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_fitness_with_backtest'
FAILED tests/test_strategy_generator.py::TestFitnessEvaluation::test_multi_objective_fitness - AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_multi_objective_fitness'
FAILED tests/test_strategy_generator.py::TestFitnessEvaluation::test_risk_adjusted_fitness - AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_risk_adjusted_fitness'
FAILED tests/test_strategy_generator.py::TestDistributedEvaluation::test_parallel_fitness_evaluation - AttributeError: 'StrategyGenerator' object has no attribute 'evaluate_population_fitness_parallel'
FAILED tests/test_strategy_generator.py::TestDistributedEvaluation::test_worker_management - AttributeError: 'DistributedEvaluator' object has no attribute 'get_worker_status'
FAILED tests/test_strategy_generator.py::TestPerformance::test_generation_performance - IndexError: list index out of range
FAILED tests/test_strategy_generator.py::TestPerformance::test_evolution_performance - AttributeError: 'StrategyGenerator' object has no attribute 'evolve'
FAILED tests/test_strategy_generator.py::TestPerformance::test_memory_usage_during_evolution - AttributeError: 'StrategyGenerator' object has no attribute 'evolve'
FAILED tests/test_strategy_generator.py::TestErrorHandling::test_empty_population_handling - AttributeError: 'StrategyGenerator' object has no attribute 'evolve'
FAILED tests/test_strategy_generator.py::TestErrorHandling::test_fitness_evaluation_failure - AttributeError: type object 'StrategyGenerator' has no attribute 'calculate_fitness_with_backtest'
FAILED tests/test_strategy_generator.py::TestHealthMonitoring::test_health_check_integration - AttributeError: 'dict' object has no attribute 'status'
FAILED tests/test_strategy_selector.py::TestRuleBasedSelector::test_select_strategy_trending - AssertionError: assert <MagicMock na...637772808128'> == <MagicMock na...637776221328'>
FAILED tests/test_timeframe_manager.py::TestTimeframeManagerCore::test_remove_symbol - AttributeError: 'TimeframeManager' object has no attribute 'remove_symbol'
FAILED tests/test_timeframe_manager.py::TestTimeframeManagerCore::test_get_registered_symbols - AttributeError: 'TimeframeManager' object has no attribute 'get_registered_symbols'
FAILED tests/test_timeframe_manager.py::TestDataSynchronization::test_fetch_multi_timeframe_data_success - assert None is not None
FAILED tests/test_timeframe_manager.py::TestDataSynchronization::test_timestamp_alignment - assert None is not None
FAILED tests/test_timeframe_manager.py::TestDataSynchronization::test_missing_data_handling - assert None is not None
FAILED tests/test_timeframe_manager.py::TestDataSynchronization::test_data_caching - assert None is not None
FAILED tests/test_timeframe_manager.py::TestSyncedData::test_synced_data_creation - TypeError: SyncedData.__init__() missing 2 required positional arguments: 'last_updated' and 'confidence_score'
FAILED tests/test_timeframe_manager.py::TestSyncedData::test_synced_data_get_timeframe - TypeError: SyncedData.__init__() missing 2 required positional arguments: 'last_updated' and 'confidence_score'
FAILED tests/test_timeframe_manager.py::TestSyncedData::test_synced_data_get_latest_timestamp - TypeError: SyncedData.__init__() missing 2 required positional arguments: 'last_updated' and 'confidence_score'
FAILED tests/test_timeframe_manager.py::TestSyncedData::test_synced_data_is_aligned - TypeError: SyncedData.__init__() missing 2 required positional arguments: 'last_updated' and 'confidence_score'
FAILED tests/test_timeframe_manager.py::TestIntegrationWithStrategies::test_strategy_multi_timeframe_signal_generation - assert False
FAILED tests/test_timeframe_manager.py::TestIntegrationWithStrategies::test_backward_compatibility_single_timeframe - assert None is not None
FAILED tests/test_timeframe_manager.py::TestPerformance::test_data_fetching_performance - assert None is not None
FAILED tests/test_timeframe_manager.py::TestPerformance::test_memory_usage_multi_timeframe - assert None is not None
FAILED tests/test_timeframe_manager.py::TestPerformance::test_concurrent_symbol_fetching - assert False
FAILED tests/test_timeframe_manager.py::TestErrorHandling::test_partial_data_failure - assert None is not None
FAILED tests/test_timeframe_manager.py::TestErrorHandling::test_empty_data_handling - assert None is not None
FAILED tests/test_timeframe_manager.py::TestErrorHandling::test_shutdown_cleanup - AssertionError: assert 1 == 0
FAILED tests/test_timeframe_manager.py::TestHealthMonitoring::test_health_check_integration - AttributeError: 'dict' object has no attribute 'status'
ERROR tests/core/test_alerting.py::TestDiscordAlerting::test_send_critical_alert_success - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestDiscordAlerting::test_send_critical_alert_no_webhook - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestDiscordAlerting::test_send_critical_alert_http_error - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestDiscordAlerting::test_send_critical_alert_empty_results - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestEventBusIntegration::test_publish_anomaly_alert - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestEventBusIntegration::test_anomaly_alert_integration - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestAlertSeverityMapping::test_different_severity_alerts - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestAlertRateLimiting::test_multiple_critical_alerts - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestAlertRateLimiting::test_alert_with_no_critical_components - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestIntegrationWithHealthChecks::test_critical_health_check_triggers_alert - RuntimeError: Event loop is closed
ERROR tests/core/test_alerting.py::TestIntegrationWithHealthChecks::test_healthy_system_no_alert - RuntimeError: Event loop is closed
ERROR tests/core/test_anomaly_detection.py::TestDrawdownAnomalyDetection::test_drawdown_anomaly_detection_not_implemented - RuntimeError: Event loop is closed
ERROR tests/core/test_anomaly_detection.py::TestDrawdownAnomalyDetection::test_drawdown_anomaly_detection_with_exception - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestDiagnosticsManager::test_register_health_check - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestDiagnosticsManager::test_register_anomaly_detector - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestDiagnosticsManager::test_run_health_check_no_checks - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestDiagnosticsManager::test_run_health_check_with_healthy_component - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestDiagnosticsManager::test_run_health_check_with_critical_component - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestDiagnosticsManager::test_run_health_check_with_mixed_statuses - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestDiagnosticsManager::test_run_health_check_with_exception - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestDiagnosticsManager::test_start_stop_monitoring - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestBuiltInHealthChecks::test_check_api_connectivity_success - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestBuiltInHealthChecks::test_check_api_connectivity_timeout - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestBuiltInHealthChecks::test_check_api_connectivity_failure - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestAnomalyDetectors::test_detect_latency_anomalies_no_data - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestAnomalyDetectors::test_detect_latency_anomalies_normal - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestAnomalyDetectors::test_detect_latency_anomalies_spike - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestAnomalyDetectors::test_detect_latency_anomalies_insufficient_data - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestIntegration::test_full_health_check_workflow - RuntimeError: Event loop is closed
ERROR tests/core/test_diagnostics.py::TestIntegration::test_monitoring_loop - RuntimeError: Event loop is closed
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_initialization - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_policy_selection_small_order - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_policy_selection_large_order - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_policy_selection_high_spread - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_policy_selection_stable_liquidity - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_simple_order_execution - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_execution_with_validation_failure - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_execution_with_retry_success - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_execution_with_fallback - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_adaptive_pricing_applied - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_execution_result_structure - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_get_execution_status - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_get_active_executions - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_cancel_execution - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestExecutionSmartLayer::test_default_config - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestIntegrationScenarios::test_small_limit_order_flow - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestIntegrationScenarios::test_large_order_twap_flow - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_smart_layer.py::TestIntegrationScenarios::test_high_spread_dca_flow - NameError: name 'ExecutionValidator' is not defined
ERROR tests/execution/test_validator.py::TestExecutionValidator::test_validate_valid_signal - RuntimeError: Event loop is closed
ERROR tests/execution/test_validator.py::TestExecutionValidator::test_validate_disabled_validator - RuntimeError: Event loop is closed
ERROR tests/execution/test_validator.py::TestExecutionValidator::test_validate_balance_insufficient - RuntimeError: Event loop is closed
ERROR tests/execution/test_validator.py::TestExecutionValidator::test_validate_balance_sufficient - RuntimeError: Event loop is closed
ERROR tests/execution/test_validator.py::TestExecutionValidator::test_validation_error_logging - RuntimeError: Event loop is closed
ERROR tests/execution/test_validator.py::TestValidationEdgeCases::test_balance_validation_without_price - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_allocation_engine.py::TestGlobalFunctions::test_create_allocation_engine
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_add_strategy - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_add_duplicate_strategy - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_remove_strategy - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_remove_nonexistent_strategy - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_route_signal_success - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_route_signal_unknown_strategy - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_route_signal_risk_limit_exceeded - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_update_performance - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_get_portfolio_performance - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_start_stop_ensemble - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_multiple_strategies_equal_weight - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestStrategyEnsembleManager::test_weight_constraints - RuntimeError: Event loop is closed
ERROR tests/portfolio/test_strategy_ensemble.py::TestGlobalFunctions::test_create_ensemble_manager
ERROR tests/portfolio/test_strategy_ensemble.py::TestIntegrationScenarios::test_full_ensemble_workflow
ERROR tests/portfolio/test_strategy_ensemble.py::TestIntegrationScenarios::test_rebalancing_workflow
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerAdaptiveIntegration::test_position_sizing_with_adaptive_policy - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerAdaptiveIntegration::test_adaptive_policy_called_with_correct_data - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerAdaptiveIntegration::test_kill_switch_blocks_trading - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerAdaptiveIntegration::test_defensive_mode_reduces_position_size - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerAdaptiveIntegration::test_adaptive_atr_position_sizing_with_multiplier - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerAdaptiveIntegration::test_error_handling_in_adaptive_integration - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerAdaptiveIntegration::test_different_position_sizing_methods_with_adaptive - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerAdaptiveIntegration::test_adaptive_multiplier_application - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerPerformanceIntegration::test_trade_outcome_updates_adaptive_policy - RuntimeError: Event loop is closed
ERROR tests/risk/test_risk_manager_integration.py::TestRiskManagerPerformanceIntegration::test_consecutive_losses_affect_position_sizing - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_adaptive_position_size_calculation - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_adaptive_position_size_fallback - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_dynamic_stop_loss_atr - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_dynamic_stop_loss_percentage - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_adaptive_take_profit - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_trailing_stop_percentage - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_trailing_stop_atr - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_time_based_exit - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_time_based_exit_no_exit - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_regime_based_exit - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_position_tracking_update - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_enhanced_trade_logging - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_atr_calculation - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_trend_multiplier_calculation - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_timeframe_conversion - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_exit_type_statistics_update - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_risk_parameters_getter - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_risk_parameters_with_symbol - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_position_sizing_method_selection - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_error_handling_in_calculations - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskManagement::test_short_position_calculations - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskIntegration::test_complete_trade_workflow - RuntimeError: Event loop is closed
ERROR tests/test_adaptive_risk.py::TestAdaptiveRiskIntegration::test_risk_manager_initialization - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_buy_signal - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_sell_signal - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_with_explicit_side - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_with_fee_calculation - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_with_params - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_with_trailing_stop - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_with_invalid_trailing_stop - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_multiple_orders - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_different_order_types - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_fallback_signal_type - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_with_none_params - RuntimeError: Event loop is closed
ERROR tests/test_backtest_executor.py::TestBacktestOrderExecutor::test_execute_backtest_order_with_none_trailing_stop - RuntimeError: Event loop is closed
ERROR tests/test_backtester.py::test_record_trade_equity_appends_record - RuntimeError: Event loop is closed
ERROR tests/test_backtester.py::test_record_trade_equity_fallbacks_to_total_pnl_when_order_manager_unavailable - RuntimeError: Event loop is closed
ERROR tests/test_backtester.py::test_record_trade_equity_handles_missing_pnl - RuntimeError: Event loop is closed
ERROR tests/test_backtester.py::test_export_equity_csv_written_and_contains_rows - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_initialization - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_shutdown - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_emergency_shutdown - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_trading_cycle - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_record_trade_equity - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_check_global_safe_mode - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_run_main_loop - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_initialization_with_invalid_balance - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_initialization_portfolio_mode - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_initialization_with_discord_enabled - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_initialization_with_terminal_display - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_initialize_strategies_with_active_strategies - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_initialize_strategies_with_unknown_strategy - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_run_with_paused_state - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_run_with_exception - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_trading_cycle_with_portfolio_mode - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_trading_cycle_with_safe_mode_active - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_record_trade_equity_backtest_mode - RuntimeError: Event loop is closed
ERROR tests/test_bot_engine.py::TestBotEngine::test_check_global_safe_mode_activation - RuntimeError: Event loop is closed
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerCoreFunctionality::test_initialization - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerCoreFunctionality::test_equity_drawdown_trigger - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerCoreFunctionality::test_consecutive_losses_trigger - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerCoreFunctionality::test_volatility_spike_trigger - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerCoreFunctionality::test_multi_factor_trigger_logic - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerCoreFunctionality::test_trigger_threshold_boundary_conditions - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerCoreFunctionality::test_trigger_response_time - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerStateManagement::test_state_transitions - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerStateManagement::test_manual_state_override - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerStateManagement::test_state_persistence - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerStateManagement::test_concurrent_state_access - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerIntegration::test_order_cancellation_integration - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerIntegration::test_signal_blocking_integration - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerIntegration::test_portfolio_freeze_integration - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerIntegration::test_anomaly_detector_integration - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerPerformance::test_high_frequency_performance - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerPerformance::test_memory_usage_during_monitoring - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerPerformance::test_resource_exhaustion_behavior - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerEdgeCases::test_market_data_gaps - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerEdgeCases::test_boundary_conditions - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerEdgeCases::test_partial_failure_recovery - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerEdgeCases::test_dependent_service_unavailability - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_circuit_breaker.py::TestCircuitBreakerEdgeCases::test_extreme_market_conditions - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_227_232_apply_env_overrides - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_line_298_validate_config - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_319_330_enforce_live_secrets_exchange - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_338_345_enforce_live_secrets_discord - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_357_359_get_method - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_374_395_set_method - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_430_432_mask_sensitive - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_436_439_mask_sensitive_nested - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_452_463_generate_template - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_lines_473_539_generate_template_content - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_line_558_load_config_global_function - RuntimeError: Event loop is closed
ERROR tests/test_config_loader.py::TestConfigLoader::test_line_563_get_config_global_function - RuntimeError: Event loop is closed
ERROR tests/test_cross_feature_integration.py::TestCircuitBreakerMonitoringIntegration::test_circuit_breaker_events_in_metrics - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestCircuitBreakerMonitoringIntegration::test_monitoring_alerts_during_circuit_breaker - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestCircuitBreakerMonitoringIntegration::test_grafana_dashboards_circuit_breaker_status - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestCircuitBreakerMonitoringIntegration::test_monitoring_performance_during_circuit_breaker - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestPerformanceCircuitBreakerIntegration::test_performance_optimizations_circuit_breaker_timing - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestPerformanceCircuitBreakerIntegration::test_circuit_breaker_behavior_optimized_execution - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestPerformanceCircuitBreakerIntegration::test_performance_metrics_circuit_breaker_impact - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestPerformanceCircuitBreakerIntegration::test_resource_constrained_circuit_breaker - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestMonitoringPerformanceIntegration::test_performance_metrics_monitoring_system - RuntimeError: Event loop is closed
ERROR tests/test_cross_feature_integration.py::TestMonitoringPerformanceIntegration::test_monitoring_performance_profiling_operations - RuntimeError: Event loop is closed
ERROR tests/test_cross_feature_integration.py::TestMonitoringPerformanceIntegration::test_optimization_improvements_monitoring_dashboards - RuntimeError: Event loop is closed
ERROR tests/test_cross_feature_integration.py::TestMonitoringPerformanceIntegration::test_resource_usage_monitoring_profiling - RuntimeError: Event loop is closed
ERROR tests/test_cross_feature_integration.py::TestFullSystemIntegration::test_end_to_end_trading_scenario - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestFullSystemIntegration::test_stress_test_all_features - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestFullSystemIntegration::test_failure_recovery_integration - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_cross_feature_integration.py::TestFullSystemIntegration::test_performance_regression_detection - NameError: name 'CircuitBreakerConfig' is not defined
ERROR tests/test_data.py::test_data_fetcher_initialization - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_fetch_ohlcv_data - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_rate_limiting - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_data_validation - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_historical_loader - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_historical_data_validation - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_data_resampling - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_cache_operations - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_multiple_symbol_fetching - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_proxy_support - RuntimeError: Event loop is closed
ERROR tests/test_data.py::test_historical_data_pagination - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_initialize_success - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_initialize_failure - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_get_historical_data_success - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_get_historical_data_malformed_response - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_get_historical_data_exchange_error - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_get_historical_data_network_error - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_get_historical_data_with_caching - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_get_realtime_data_tickers_only - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_get_realtime_data_with_orderbooks - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_get_realtime_data_error_handling - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_fetch_ticker_success - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_fetch_orderbook_success - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_throttle_requests_basic - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_throttle_requests_rate_limiting - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_throttle_requests_invalid_rate_limit - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherAsyncMethods::test_throttle_requests_zero_rate_limit - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherMultipleData::test_get_multiple_historical_data_success - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherMultipleData::test_get_multiple_historical_data_partial_failure - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherCleanup::test_shutdown_success - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherCleanup::test_shutdown_exchange_error - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherCleanup::test_shutdown_with_session - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherCleanup::test_shutdown_session_already_closed - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherErrorScenarios::test_get_historical_data_unexpected_error - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherErrorScenarios::test_get_realtime_data_concurrent_errors - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherErrorScenarios::test_fetch_ticker_error - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherErrorScenarios::test_fetch_orderbook_error - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherEdgeCases::test_get_historical_data_empty_response - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherEdgeCases::test_get_historical_data_none_response - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherEdgeCases::test_throttle_requests_high_request_count - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherIntegration::test_full_workflow_with_caching - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherIntegration::test_multiple_symbols_workflow - RuntimeError: Event loop is closed
ERROR tests/test_data_fetcher.py::TestDataFetcherIntegration::test_realtime_data_workflow - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_basic_notification - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_with_embed - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_trade_alert - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_signal_alert - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_error_alert - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_performance_report - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_bot_integration_basic_notification - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_bot_integration_trade_alert - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_rate_limit_handling - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_error_recovery - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_large_payload - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_special_characters - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_empty_content - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_webhook_integration_disabled_alerts - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_live_discord_webhook_integration - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_bot_integration_initialization - RuntimeError: Event loop is closed
ERROR tests/test_discord_integration.py::TestDiscordIntegration::test_bot_integration_shutdown - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_webhook_success - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_bot_rest_success - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_with_embed - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_disabled_alerts - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_rate_limit_retry - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_server_error_retry - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_max_retries_exceeded - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_trade_alert - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_signal_alert - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_error_alert - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_performance_report - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_initialize_bot_mode - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_shutdown - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_shutdown_with_bot - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_bot_commands_status - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_bot_commands_pause - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_bot_commands_resume - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_bot_commands_trades - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_bot_commands_trades_no_history - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_verify_channel_correct - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_verify_channel_wrong - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_initialize_webhook_mode - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_shutdown_timeout_handling - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_shutdown_exception_handling - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_no_config - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_network_error_retry - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_timeout_retry - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_4xx_error_no_retry - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_rate_limit_no_retry_after - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_unexpected_error - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_trade_alert_disabled_alerts - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_trade_alert_loss - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_signal_alert_disabled_alerts - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_signal_alert_dict_input - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_error_alert_disabled_alerts - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_performance_report_disabled_alerts - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_exponential_backoff - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_jitter - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_init_environment_variables_priority - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_payload_structure - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_shutdown_already_closed_session - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_bot_command_wrong_channel_rejection - RuntimeError: Event loop is closed
ERROR tests/test_discord_notifier.py::TestDiscordNotifier::test_send_notification_mixed_mode_priority - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventBus::test_subscribe_and_publish - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventBus::test_multiple_subscribers - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventBus::test_unsubscribe - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEnhancedEventBus::test_publish_base_event - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEnhancedEventBus::test_publish_with_string_event_type - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEnhancedEventBus::test_event_history_management - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEnhancedEventBus::test_error_handling_in_publish - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEnhancedEventBus::test_clear_subscribers - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventDrivenLogging::test_trade_logger_event_handling - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventDrivenLogging::test_strategy_switch_event_handling - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventDrivenLogging::test_risk_limit_event_handling - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventDrivenLogging::test_diagnostic_alert_event_handling - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventDrivenLogging::test_knowledge_entry_event_handling - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventDrivenLogging::test_regime_change_event_handling - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventDrivenLogging::test_system_status_event_handling - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestEventDrivenLogging::test_unknown_event_handling - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestIntegration::test_end_to_end_event_flow - RuntimeError: Event loop is closed
ERROR tests/test_event_driven_architecture.py::TestIntegration::test_multiple_event_types - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestBaseExecutor::test_split_order - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestSmartOrderExecutor::test_execute_small_order - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestSmartOrderExecutor::test_execute_large_order - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestTWAPExecutor::test_execute_order - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestTWAPExecutor::test_get_execution_schedule - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_get_volume_profile - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_wait_for_volume_period_high_volume - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_wait_for_volume_period_low_volume - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_wait_for_volume_period_no_high_volume - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_execute_single_order_with_api - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_execute_single_order_without_api - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_place_order_via_api_buy_order - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_place_order_via_api_sell_order - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_place_order_via_api_no_exchange_api - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_cancel_order_with_api - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_cancel_order_without_api - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_cancel_order_api_failure - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_execute_order_invalid_signal - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestVWAPExecutor::test_execute_order_with_execution_failure - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestDCAExecutor::test_start_dca_session - RuntimeError: Event loop is closed
ERROR tests/test_execution.py::TestDCAExecutor::test_continue_dca_session - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderAsyncMethods::test_load_historical_data_success - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderAsyncMethods::test_load_historical_data_partial_failure - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderAsyncMethods::test_load_symbol_data_from_cache - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderAsyncMethods::test_load_symbol_data_force_refresh - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderResampling::test_resample_data_success - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderResampling::test_resample_data_empty - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderResampling::test_resample_data_multiple_symbols - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderUtilities::test_shutdown - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderFetchCompleteHistory::test_fetch_complete_history_success - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderFetchCompleteHistory::test_fetch_complete_history_no_data - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderFetchCompleteHistory::test_fetch_complete_history_with_deduplication - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderFetchCompleteHistory::test_fetch_complete_history_with_retries - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderFetchCompleteHistory::test_fetch_complete_history_max_retries_exceeded - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderIntegration::test_full_data_loading_workflow - RuntimeError: Event loop is closed
ERROR tests/test_historical_loader.py::TestHistoricalDataLoaderIntegration::test_data_validation_workflow - RuntimeError: Event loop is closed
ERROR tests/test_integration.py::TestIntegration::test_signal_to_order_flow - RuntimeError: Event loop is closed
ERROR tests/test_integration.py::TestIntegration::test_signal_to_notification_flow - RuntimeError: Event loop is closed
ERROR tests/test_integration.py::TestIntegration::test_signal_router_to_order_manager_integration - RuntimeError: Event loop is closed
ERROR tests/test_integration.py::TestIntegration::test_portfolio_balance_updates - RuntimeError: Event loop is closed
ERROR tests/test_integration.py::TestIntegration::test_multiple_signals_concurrent_processing - RuntimeError: Event loop is closed
ERROR tests/test_integration.py::TestIntegration::test_error_handling_in_integration_flow - RuntimeError: Event loop is closed
ERROR tests/test_integration.py::TestIntegration::test_safe_mode_integration - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestMultiTimeframeRegimeIntegration::test_regime_forecasting_with_multi_timeframe_data - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestMultiTimeframeRegimeIntegration::test_regime_transition_detection_with_mtf - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestStrategyGeneratorSelfHealingIntegration::test_strategy_generation_with_health_monitoring - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestStrategyGeneratorSelfHealingIntegration::test_strategy_deployment_with_failure_recovery - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestFullSystemIntegration::test_end_to_end_trading_workflow - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestFullSystemIntegration::test_cross_feature_performance_optimization - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestFullSystemIntegration::test_resource_contention_handling - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestFailureScenariosAcrossFeatures::test_cascading_failure_prevention - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestFailureScenariosAcrossFeatures::test_emergency_mode_with_multiple_features - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestFailureScenariosAcrossFeatures::test_recovery_coordination_across_features - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestRealisticMarketScenarios::test_high_volatility_scenario - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestRealisticMarketScenarios::test_regime_transition_scenario - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestProductionReadiness::test_24_hour_stability_test - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestProductionReadiness::test_resource_usage_under_load - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestProductionReadiness::test_security_integration - RuntimeError: Event loop is closed
ERROR tests/test_integration_all_features.py::TestPerformanceBenchmarks::test_heartbeat_throughput_benchmark
ERROR tests/test_integration_all_features.py::TestPerformanceBenchmarks::test_memory_efficiency_benchmark
ERROR tests/test_journal_writer.py::test_journal_writer_initialization - RuntimeError: Event loop is closed
ERROR tests/test_journal_writer.py::test_journal_writer_append_with_running_loop - RuntimeError: Event loop is closed
ERROR tests/test_journal_writer.py::test_journal_writer_append_without_loop - RuntimeError: Event loop is closed
ERROR tests/test_journal_writer.py::test_journal_writer_worker_loop - RuntimeError: Event loop is closed
ERROR tests/test_journal_writer.py::test_journal_writer_stop - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_success - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_dict_signal - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_no_exchange - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_network_error - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_exchange_error - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_unexpected_error - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_create_order_positional_args - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_create_order_kwargs_fallback - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_create_order_both_fail - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_shutdown_with_exchange - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_shutdown_no_exchange - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_limit_order - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_sell_signal - RuntimeError: Event loop is closed
ERROR tests/test_live_executor.py::TestLiveOrderExecutor::test_execute_live_order_with_params - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_initialize_success - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_initialize_with_fastapi - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_initialize_config_failure - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_initialize_bot_engine_failure - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_run_without_initialization - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_run_success - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_run_keyboard_interrupt - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_run_general_exception - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_shutdown - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestCryptoTradingBot::test_shutdown_without_bot_engine - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestMainFunction::test_main_fastapi_mode - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestMainFunction::test_main_fastapi_mode_unavailable - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestMainFunction::test_main_status_mode - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestMainFunction::test_main_cli_mode - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestMainFunction::test_main_environment_variable_fastapi - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestIntegrationScenarios::test_full_initialization_flow - RuntimeError: Event loop is closed
ERROR tests/test_main.py::TestIntegrationScenarios::test_error_recovery_flow - RuntimeError: Event loop is closed
ERROR tests/test_ml_signal_router.py::test_ml_confirmation_rejects_weak_signal_on_opposite_prediction - RuntimeError: Event loop is closed
ERROR tests/test_ml_signal_router.py::test_ml_confirmation_accepts_signal_on_matching_prediction - RuntimeError: Event loop is closed
ERROR tests/test_ml_signal_router.py::test_ml_confirmation_skips_when_no_features - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestMetricsCollection::test_metric_recording - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestMetricsCollection::test_counter_increment - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestMetricsCollection::test_histogram_observation - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestDataAccuracy::test_timestamp_accuracy - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestDataAccuracy::test_metric_value_precision - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestDataAccuracy::test_metric_persistence - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestDataAccuracy::test_concurrent_metric_recording - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestTradingMetricsCollection::test_trading_metrics_collection - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestTradingMetricsCollection::test_risk_metrics_collection - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestTradingMetricsCollection::test_strategy_metrics_collection - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestTradingMetricsCollection::test_exchange_metrics_collection - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestMetricsEndpoint::test_endpoint_creation - AttributeError: 'MetricsCollector' object has no attribute 'get'
ERROR tests/test_monitoring_observability.py::TestMetricsEndpoint::test_metrics_endpoint_response - AttributeError: 'MetricsCollector' object has no attribute 'get'
ERROR tests/test_monitoring_observability.py::TestMetricsEndpoint::test_health_endpoint - AttributeError: 'MetricsCollector' object has no attribute 'get'
ERROR tests/test_monitoring_observability.py::TestMetricsEndpoint::test_invalid_endpoint - AttributeError: 'MetricsCollector' object has no attribute 'get'
ERROR tests/test_monitoring_observability.py::TestAlertingSystem::test_alert_rule_creation - NameError: name 'AlertRulesManager' is not defined
ERROR tests/test_monitoring_observability.py::TestAlertingSystem::test_alert_evaluation - NameError: name 'AlertRulesManager' is not defined
ERROR tests/test_monitoring_observability.py::TestAlertingSystem::test_alert_deduplication - NameError: name 'AlertRulesManager' is not defined
ERROR tests/test_monitoring_observability.py::TestAlertingSystem::test_notification_delivery - NameError: name 'AlertRulesManager' is not defined
ERROR tests/test_monitoring_observability.py::TestPerformanceImpact::test_collection_performance - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestPerformanceImpact::test_memory_overhead - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestPerformanceImpact::test_scalability_under_load - RuntimeError: Event loop is closed
ERROR tests/test_monitoring_observability.py::TestGrafanaIntegration::test_dashboard_creation - NameError: name 'DashboardManager' is not defined
ERROR tests/test_monitoring_observability.py::TestGrafanaIntegration::test_dashboard_rendering - NameError: name 'DashboardManager' is not defined
ERROR tests/test_monitoring_observability.py::TestGrafanaIntegration::test_query_performance - NameError: name 'DashboardManager' is not defined
ERROR tests/test_optimization.py::TestWalkForwardOptimizer::test_optimization_with_insufficient_data - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_execute_order_paper_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_execute_order_backtest_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_execute_order_safe_mode_active - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_execute_order_live_mode_with_retry - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_cancel_order_live_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_cancel_order_non_live_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_balance_live_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_balance_paper_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_balance_backtest_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_equity_with_portfolio_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_initialize_portfolio - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_active_order_count - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_open_position_count - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_shutdown - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_execute_order_safe_mode_trigger_counter - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_execute_order_unknown_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_cancel_order_with_exception - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_cancel_all_orders_with_exception - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_rate_limit - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_cached_ticker_cache_hit - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_cached_ticker_live_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_cached_ticker_non_live_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_balance_live_mode_with_exception - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_balance_backtest_portfolio_mode - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_equity_live_mode_with_positions - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_get_equity_with_invalid_position_data - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_initialize_portfolio_invalid_inputs - RuntimeError: Event loop is closed
ERROR tests/test_order_manager.py::TestOrderManager::test_initialize_portfolio_equal_allocation - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_process_order_filled_buy - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_process_order_filled_sell_with_pnl - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_process_order_open_order - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_process_order_dynamic_take_profit - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_process_order_dynamic_tp_missing_params - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_process_order_trailing_stop_from_params - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_estimate_trend_strength_stub - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_estimate_trend_strength_with_params - RuntimeError: Event loop is closed
ERROR tests/test_order_processor.py::TestOrderProcessor::test_process_order_partial_fill - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_buy_signal - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_sell_signal - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_insufficient_balance - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_with_explicit_side - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_with_fee_calculation - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_with_params - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_with_trailing_stop - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_portfolio_mode_buy - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_portfolio_mode_sell - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_portfolio_mode_new_symbol - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_multiple_orders - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_different_order_types - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_fallback_signal_type - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_with_none_params - RuntimeError: Event loop is closed
ERROR tests/test_paper_executor.py::TestPaperOrderExecutor::test_execute_paper_order_with_none_trailing_stop - RuntimeError: Event loop is closed
ERROR tests/test_performance_optimization.py::TestPerformanceMonitoringIntegration::test_monitoring_during_optimization - RuntimeError: Event loop is closed
ERROR tests/test_performance_optimization.py::TestPerformanceMonitoringIntegration::test_anomaly_detection_with_optimizations - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestRegimeForecasterInitialization::test_async_initialization - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestModelTraining::test_model_training - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestModelTraining::test_model_persistence - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestPrediction::test_regime_prediction - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestPrediction::test_prediction_confidence_calibration - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestPrediction::test_prediction_edge_cases - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestPrediction::test_forecast_horizon - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestIntegrationWithStrategySelector::test_strategy_selector_integration - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestIntegrationWithStrategySelector::test_forecast_driven_strategy_selection - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestPerformance::test_prediction_performance - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestPerformance::test_memory_usage - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestPerformance::test_concurrent_predictions - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestModelUpdates::test_model_update - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestModelUpdates::test_model_versioning - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestErrorHandling::test_prediction_with_corrupted_data - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestErrorHandling::test_model_loading_failure - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestErrorHandling::test_empty_training_data - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestErrorHandling::test_insufficient_data_for_training - RuntimeError: Event loop is closed
ERROR tests/test_regime_forecaster.py::TestHealthMonitoring::test_health_check_integration - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_network_error_retry_mechanism - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_exchange_error_handling - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_timeout_error_recovery - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_corrupted_signal_data_handling - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_balance_fetch_failure_recovery - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_equity_calculation_with_missing_ticker_data - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_discord_notification_failure_does_not_break_flow - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_discord_rate_limit_handling - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_portfolio_initialization_with_invalid_data - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_concurrent_order_cancellation_stress - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_memory_leak_prevention_in_long_running_sessions - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_graceful_shutdown_under_load - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_invalid_trading_mode_fallback - RuntimeError: Event loop is closed
ERROR tests/test_regression.py::TestRegression::test_extreme_market_conditions_simulation - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_success_first_try - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_success_after_retry - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_exhaust_retries - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_custom_parameters - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_specific_exceptions - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_backoff_calculation - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_max_backoff_cap - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_with_different_exception_types - RuntimeError: Event loop is closed
ERROR tests/test_reliability_manager.py::TestReliabilityManager::test_retry_async_zero_retries - RuntimeError: Event loop is closed
ERROR tests/test_retry.py::test_async_retry_call_retries_and_succeeds - RuntimeError: Event loop is closed
ERROR tests/test_retry.py::test_retry_async_decorator - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_initialization - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_signal_validation - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_fixed_fractional_position_sizing - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_volatility_position_sizing - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_take_profit_calculation - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_position_size_validation - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_daily_loss_limit - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_max_concurrent_trades - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_emergency_check - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_trade_outcome_updates - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_volatility_tracking - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_risk_parameters_access - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_risk_manager_initialization_lines_38_43 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_evaluate_signal_validation_lines_142_143 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_evaluate_signal_stop_loss_lines_155_157 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_evaluate_signal_position_size_line_169 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_evaluate_signal_position_capping_lines_173_176 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_calculate_position_size_method_selection_line_202 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_calculate_position_size_volatility_line_204 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_calculate_position_size_martingale_kelly_lines_207_209 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_fixed_fractional_position_size_line_254 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_volatility_based_position_size_lines_261_276 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_kelly_position_size_lines_289_295 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_calculate_take_profit_lines_316_329 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_check_portfolio_risk_lines_364_365 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_validate_position_size_lines_402_406 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_update_volatility_line_432 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_update_trade_outcome_lines_452_458_465_466 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_emergency_check_lines_489_490 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_get_risk_parameters_lines_499_500 - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_position_sizing_edge_cases - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_risk_calculation_overflow_protection - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_concurrent_signal_processing - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_adaptive_sizing_based_on_performance - RuntimeError: Event loop is closed
ERROR tests/test_risk.py::test_decimal_precision_handling - RuntimeError: Event loop is closed
ERROR tests/test_safe_mode.py::test_botengine_respects_order_manager_safe_mode - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestRecoveryOrchestrator::test_recovery_initiation - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestWatchdogService::test_heartbeat_processing - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestWatchdogService::test_overdue_heartbeat_detection - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestEmergencyProcedures::test_emergency_activation - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestEmergencyProcedures::test_emergency_deactivation - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestSelfHealingEngine::test_heartbeat_sending - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestIntegrationWithN1V1::test_bot_engine_integration - AttributeError: Mock object has no attribute 'monitoring_dashboard'. Did you mean: '_update_monitoring_dashboard'?
ERROR tests/test_self_healing_engine.py::TestIntegrationWithN1V1::test_bot_engine_integration - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestIntegrationWithN1V1::test_diagnostics_integration - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestIntegrationWithN1V1::test_event_bus_integration - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestPerformance::test_heartbeat_processing_performance - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestPerformance::test_concurrent_heartbeat_processing - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestPerformance::test_memory_usage - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestErrorHandling::test_invalid_component_registration - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestErrorHandling::test_heartbeat_for_nonexistent_component - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestErrorHandling::test_recovery_failure_handling - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestErrorHandling::test_emergency_mode_under_load - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestReliability::test_long_running_stability - RuntimeError: Event loop is closed
ERROR tests/test_self_healing_engine.py::TestReliability::test_failure_recovery_cycle - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_initialization - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_process_signal_valid - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_process_signal_invalid_missing_symbol - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_process_signal_invalid_zero_amount - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_validate_signal_valid - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_validate_signal_invalid - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_signal_conflicts_opposite_entry - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_signal_conflicts_entry_vs_exit - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_cancel_signal - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_update_signal_status - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_get_active_signals - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_get_signal_history - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_clear_signals - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_retry_async_call_success - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_retry_async_call_with_failure - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_retry_async_call_exhaust_retries - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_record_router_error - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_concurrent_signal_processing_no_conflicts - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_conflicting_signals_resolution - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_generate_signal_id - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_get_symbol_lock - AttributeError: 'str' object has no attribute 'get'
ERROR tests/test_signal_router.py::test_ml_confirmation_weak_signal_approved - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_confirmation_weak_signal_rejected - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_confirmation_strong_signal_bypassed - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_predict_called_with_dataframe_features - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_predict_called_with_dict_features - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_predict_called_with_series_features - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_predict_not_called_with_empty_dataframe - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_predict_called_with_metadata_features_fallback - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_predict_called_with_ohlcv_fallback - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_get_symbol_lock_concurrency - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_concurrent_signal_processing_different_symbols - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_concurrent_signal_processing_same_symbol_conflicts - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_retry_async_call_with_custom_backoff - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_retry_async_call_max_backoff_cap - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_journal_recovery_with_corrupted_entries - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_journal_recovery_signal_reconstruction - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_journal_recovery_enum_conversion_edge_cases - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_extract_features_edge_cases - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_extract_features_fallback_to_metadata - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_extract_features_multiple_fallbacks - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_retry_async_call_with_different_exception_types - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_retry_async_call_zero_retries - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_ml_confirmation_disabled - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_ml_model_none - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_non_entry_signal_type - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_ml_prediction_error_handling - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_ml_low_confidence_ignored - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_blocking_mode - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_risk_manager_failure - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_validation_failure - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_record_router_error_threshold_trigger - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_record_router_error_context_logging - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_update_signal_status_nonexistent_signal - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_get_active_signals_empty - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_get_signal_history_limit - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_close_journal_no_journal - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_close_journal_with_journal - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_get_symbol_lock_none_symbol - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_get_symbol_lock_creates_new_lock - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_process_signal_with_invalid_symbol_lock - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_resolve_conflicts_with_empty_list - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_resolve_conflicts_strength_based - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_resolve_conflicts_newer_first - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_store_signal_with_journal_disabled - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_cancel_signal_with_journal_disabled - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_recover_from_journal_disabled - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_extract_features_with_list_of_scalars - RuntimeError: Event loop is closed
ERROR tests/test_signal_router.py::test_ml_extract_features_with_invalid_data - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_initialization_with_config_object - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_initialization_with_dict_config - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_calculate_indicators - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_calculate_indicators_insufficient_data - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_generate_signals_oversold - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_generate_signals_overbought - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_generate_signals_neutral - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_generate_signals_dict_format - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_generate_signals_empty_data - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_generate_signals_multiple_symbols - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestRSIStrategy::test_generate_signals_volume_confirmation - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestEMACrossStrategy::test_initialization_with_config_object - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestEMACrossStrategy::test_calculate_indicators - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestEMACrossStrategy::test_calculate_indicators_insufficient_data - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestEMACrossStrategy::test_generate_signals_bullish_crossover - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestEMACrossStrategy::test_generate_signals_bearish_crossover - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestEMACrossStrategy::test_generate_signals_no_crossover - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestEMACrossStrategy::test_generate_signals_insufficient_data - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestEMACrossStrategy::test_generate_signals_dict_format - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestStrategyIntegration::test_strategy_run_workflow - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestStrategyIntegration::test_strategy_shutdown - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestStrategyIntegration::test_strategy_performance_metrics - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestStrategyIntegration::test_strategy_error_handling - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::TestStrategyIntegration::test_strategy_create_signal_helper - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_rsi_strategy_init_lines_42_56 - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_calculate_indicators_single_symbol_lines_94_97 - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_generate_signals_dict_format_line_122 - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_generate_signals_dataframe_format_line_125 - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_generate_signals_for_symbol_rsi_check_lines_146_150 - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_generate_signals_for_symbol_short_signal_lines_206_209 - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_rsi_strategy_edge_cases - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_rsi_strategy_calculate_indicators_multiple_symbols - RuntimeError: Event loop is closed
ERROR tests/test_strategies.py::test_rsi_strategy_signal_tracking - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_strategy_initialization - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_strategy_lifecycle - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_indicator_calculation - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_signal_generation - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_strategy_run_method - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_strategy_performance_tracking - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_trend_analysis_mixin - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_volatility_analysis_mixin - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_strategy_error_handling - RuntimeError: Event loop is closed
ERROR tests/test_strategy.py::test_strategy_create_signal - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestStrategyGenerator::test_population_initialization - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestStrategyGenerator::test_evolution_process - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestStrategyGenerator::test_strategy_generation - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestStrategyGenerator::test_population_save_load - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestFitnessEvaluation::test_fitness_calculation - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestFitnessEvaluation::test_backtest_integration - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestDistributedEvaluation::test_distributed_evaluation_setup - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestDistributedEvaluation::test_parallel_fitness_evaluation - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestDistributedEvaluation::test_worker_management - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestPerformance::test_generation_performance - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestPerformance::test_evolution_performance - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestPerformance::test_memory_usage_during_evolution - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestErrorHandling::test_empty_population_handling - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestErrorHandling::test_fitness_evaluation_failure - RuntimeError: Event loop is closed
ERROR tests/test_strategy_generator.py::TestHealthMonitoring::test_health_check_integration - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_create_task_basic - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_create_task_with_name - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_task_completion_callback_success - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_task_completion_callback_exception - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_cancel_all_tasks - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_shutdown_prevents_new_tasks - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_multiple_concurrent_tasks - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_task_manager_with_exception_in_callback - RuntimeError: Event loop is closed
ERROR tests/test_task_manager.py::TestTaskManager::test_get_tracked_tasks_snapshot - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestTimeframeManagerCore::test_initialization - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestTimeframeManagerCore::test_add_symbol_success - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestTimeframeManagerCore::test_remove_symbol - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestTimeframeManagerCore::test_get_registered_symbols - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestDataSynchronization::test_fetch_multi_timeframe_data_success - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestDataSynchronization::test_timestamp_alignment - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestDataSynchronization::test_missing_data_handling - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestDataSynchronization::test_data_caching - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestDataSynchronization::test_cache_expiration - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestIntegrationWithStrategies::test_strategy_multi_timeframe_signal_generation - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestIntegrationWithStrategies::test_backward_compatibility_single_timeframe - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestPerformance::test_data_fetching_performance - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestPerformance::test_memory_usage_multi_timeframe - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestPerformance::test_concurrent_symbol_fetching - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestErrorHandling::test_exchange_connection_failure - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestErrorHandling::test_partial_data_failure - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestErrorHandling::test_empty_data_handling - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestErrorHandling::test_invalid_symbol_handling - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestErrorHandling::test_shutdown_cleanup - RuntimeError: Event loop is closed
ERROR tests/test_timeframe_manager.py::TestHealthMonitoring::test_health_check_integration - RuntimeError: Event loop is closed
=================== 235 failed, 1578 passed, 9 skipped, 9 warnings, 744 errors in 856.04s (0:14:16) ===================

C:\Users\TU\Desktop\new project\N1V1>